{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpI9eESpkREM"
   },
   "source": [
    "# Ex1: Onehot coding DNA\n",
    "\n",
    "Write a function called **onehot_dna(dna_str)** that allows to encode a DNA segment where each base is encoded as a vector of all zeros except one in a specific position. The result of this function is an array numpy.  DNA is a long chain of repeating bases strung together. There are 4 bases: A, C, G, T. For example, \"AACCCAAATCGGGGG\" is a DNA segment.\n",
    "\n",
    "\n",
    "\n",
    "For example, **onehot_dna('AAT')** should return\n",
    "\n",
    "array([[1, 0, 0, 0],\n",
    "       [1, 0, 0, 0],\n",
    "       [0, 0, 0, 1]])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YbhaCZP1moii"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def onehot_dna(sequence):\n",
    "    hot_encoded = np.zeros((len(sequence), 4))\n",
    "    dic = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "    for i in range(len(sequence)):\n",
    "        col = dic[sequence[i]]\n",
    "        hot_encoded[i][col] = 1\n",
    "    return hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WM-BaQiGo82A",
    "outputId": "a7357e07-2c79-4d10-be96-aa0d8520b702"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_dna('AAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkSQo19z60n1"
   },
   "source": [
    "# Deep learning to classify Transcription Factor Biding\n",
    "\n",
    "\n",
    "In the next exercises, we will learn how to use Deep learning to predict whether a segment of DNA does include or does not include a sit where JUND binds. (JUND is a particular transcription factor).\n",
    "\n",
    "In this purpose, we will use data that is extracted from the chapter 6 of the book: 'Deep learning for the life science'. This book is written by B.Ramsundar, P.Eastman, P. Walters and V.Pande.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the Wikipedia page of the Jund transcription factor, explain what is the use of such protein "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it protect the cellule, it can be use to fight addiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data consist of DNA segments that have been split up from a full chromosome. Each segment is of 101 bases long and has been labeled to indicate whether it does or does not include a site where JUND binds to.\n",
    "\n",
    "\n",
    "This is a binary classification problem.\n",
    "The process of creating a PyTorch neural network binary classifier consists of several steps:\n",
    "\n",
    "1. Prepare the training and test data\n",
    "\n",
    "2. Implement a Dataset object to serve up the data\n",
    "\n",
    "3. Design and implement a neural network\n",
    "\n",
    "4. Write code to train the network\n",
    "\n",
    "5. Write code to evaluate the model (the trained network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS8a5oOmOjqO"
   },
   "source": [
    "# Ex 2:  Load Data\n",
    "\n",
    "The data is available here : https://drive.google.com/drive/folders/1-nrTvNvEZo6Px1pnT7IeotKZR7p365UJ?usp=sharing\n",
    "\n",
    "1. With the help of the joblib library, load the following files for training set:  **y_train.joblib**, **X_train.joblib**  and then store the results in variables **y_train, X_train** ,respectively.\n",
    "\n",
    "2. Do the same thing for the test set: load  **y_test.joblib**, **X_test.joblib**  and then store the results in variables **y_test, X_test**, respectively.\n",
    "\n",
    "3. What are the shape of **X_train** and **y_train** ? How many DNA segments are there in traning set ?\n",
    "\n",
    "4. Display a DNA segment from **X_train** (using matplotlib.pyplot.imshow ).\n",
    "\n",
    "5. Plot the histogram of **y_train** to see whether data is imbalanced or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NB5Y5StQezcZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from joblib import load\n",
    "\n",
    "dir = \"../data/tfbind\"\n",
    "\n",
    "y_train = load(os.path.join(dir, \"y_train.joblib\"))\n",
    "X_train = load(os.path.join(dir, \"X_train.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FMLs0OOTV4E5"
   },
   "outputs": [],
   "source": [
    "''' Tests X_train, y_train '''\n",
    "assert(X_train.shape == (4672, 101, 4))\n",
    "assert(y_train.shape ==(4672, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9g9rlIr2AAy6"
   },
   "outputs": [],
   "source": [
    "y_test = load(os.path.join(dir, \"y_test.joblib\"))\n",
    "X_test = load(os.path.join(dir, \"X_test.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RO7DeSen_W_h"
   },
   "outputs": [],
   "source": [
    "''' Tests X_test, y_test'''\n",
    "assert(X_test.shape == (584, 101, 4))\n",
    "assert(y_test.shape ==(584, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "TceRZcS3MENp",
    "outputId": "25a06914-8dbe-425c-f6eb-d0627b622bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (4672, 101, 4)\n",
      "y_train shape:  (4672, 1)\n",
      "There is 4672 DNA fragments in the training set\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKd5JREFUeJzt3X1wVPWh//FPCOwCwm4ETDa5BOThCgR5kFDDVqEgKQEi1RGnUijQysOFBmcgFkIqBYRew+AjVYSx1IaZQnnogFeJBEJo4AoBNCWX59wC4QYHNqBIFhASkpzfH07Oj9WgbMwD3/h+zZwZ9pzvnv2eI7pvT85uQizLsgQAAGCQJg09AQAAgGARMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACM07ShJ1BXKisrde7cObVu3VohISENPR0AAHAHLMvSlStXFBUVpSZNbn+dpdEGzLlz5xQdHd3Q0wAAADVw9uxZtW/f/rbbG23AtG7dWtJXJ8DlcjXwbAAAwJ3w+/2Kjo6238dvp9EGTNWPjVwuFwEDAIBhvuv2D27iBQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYJygAmbFihXq3bu3/fuFvF6vtm7dam8fPHiwQkJCApZp06YF7KOoqEiJiYlq2bKlwsPDNXv2bJWXlweMycnJUb9+/eR0OtW1a1elp6fX/AgBAECjE9Qvc2zfvr2WLFmif//3f5dlWVq9erWeeOIJHTx4UD179pQkTZkyRYsWLbKf07JlS/vPFRUVSkxMlMfj0d69e3X+/HlNmDBBzZo100svvSRJKiwsVGJioqZNm6Y1a9YoOztbkydPVmRkpBISEmrjmAEAgOFCLMuyvs8O2rRpo5dfflmTJk3S4MGD1bdvX73xxhvVjt26dasef/xxnTt3ThEREZKklStXKiUlRRcvXpTD4VBKSooyMjJ05MgR+3ljxozR5cuXlZmZecfz8vv9crvdKikp4bdRAwBgiDt9/w7qCsytKioqtHHjRl27dk1er9dev2bNGv31r3+Vx+PRqFGj9Pvf/96+CpObm6tevXrZ8SJJCQkJmj59uo4ePaqHHnpIubm5io+PD3ithIQEzZw581vnU1paqtLSUvux3++v6aF9p/vnZtTZvgHcmTNLEht6CgAaUNABc/jwYXm9Xt24cUOtWrXS5s2bFRMTI0kaO3asOnbsqKioKB06dEgpKSkqKCjQpk2bJEk+ny8gXiTZj30+37eO8fv9un79ulq0aFHtvNLS0vTiiy8GezgAAMBAQQdMt27dlJ+fr5KSEv3973/XxIkTtWvXLsXExGjq1Kn2uF69eikyMlJDhw7VqVOn1KVLl1qd+NelpqYqOTnZfuz3+xUdHV2nrwkAABpG0B+jdjgc6tq1q2JjY5WWlqY+ffpo2bJl1Y6Ni4uTJJ08eVKS5PF4VFxcHDCm6rHH4/nWMS6X67ZXXyTJ6XTan46qWgAAQOP0vb8HprKyMuDek1vl5+dLkiIjIyVJXq9Xhw8f1oULF+wxWVlZcrlc9o+hvF6vsrOzA/aTlZUVcJ8NAAD4YQvqR0ipqakaMWKEOnTooCtXrmjt2rXKycnRtm3bdOrUKa1du1YjR45U27ZtdejQIc2aNUuDBg1S7969JUnDhg1TTEyMxo8fr6VLl8rn82nevHlKSkqS0+mUJE2bNk1vvfWW5syZo2effVY7d+7Uhg0blJHBjbMAAOArQQXMhQsXNGHCBJ0/f15ut1u9e/fWtm3b9NOf/lRnz57Vjh079MYbb+jatWuKjo7W6NGjNW/ePPv5oaGh2rJli6ZPny6v16t77rlHEydODPjemE6dOikjI0OzZs3SsmXL1L59e61atYrvgAEAALbv/T0wd6u6/B4YPkYNNDw+Rg00Tnf6/s3vQgIAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYJygAmbFihXq3bu3XC6XXC6XvF6vtm7dam+/ceOGkpKS1LZtW7Vq1UqjR49WcXFxwD6KioqUmJioli1bKjw8XLNnz1Z5eXnAmJycHPXr109Op1Ndu3ZVenp6zY8QAAA0OkEFTPv27bVkyRLl5eXpk08+0WOPPaYnnnhCR48elSTNmjVLH3zwgTZu3Khdu3bp3Llzeuqpp+znV1RUKDExUWVlZdq7d69Wr16t9PR0zZ8/3x5TWFioxMREDRkyRPn5+Zo5c6YmT56sbdu21dIhAwAA04VYlmV9nx20adNGL7/8sp5++mndd999Wrt2rZ5++mlJ0okTJ9SjRw/l5uZqwIAB2rp1qx5//HGdO3dOERERkqSVK1cqJSVFFy9elMPhUEpKijIyMnTkyBH7NcaMGaPLly8rMzPzjufl9/vldrtVUlIil8v1fQ7xG+6fm1Gr+wMQvDNLEht6CgDqwJ2+f9f4HpiKigqtW7dO165dk9frVV5enm7evKn4+Hh7TPfu3dWhQwfl5uZKknJzc9WrVy87XiQpISFBfr/fvoqTm5sbsI+qMVX7uJ3S0lL5/f6ABQAANE5BB8zhw4fVqlUrOZ1OTZs2TZs3b1ZMTIx8Pp8cDofCwsICxkdERMjn80mSfD5fQLxUba/a9m1j/H6/rl+/ftt5paWlye1220t0dHSwhwYAAAwRdMB069ZN+fn52r9/v6ZPn66JEyfq2LFjdTG3oKSmpqqkpMRezp4929BTAgAAdaRpsE9wOBzq2rWrJCk2NlYff/yxli1bpmeeeUZlZWW6fPlywFWY4uJieTweSZLH49GBAwcC9lf1KaVbx3z9k0vFxcVyuVxq0aLFbefldDrldDqDPRwAAGCg7/09MJWVlSotLVVsbKyaNWum7Oxse1tBQYGKiork9XolSV6vV4cPH9aFCxfsMVlZWXK5XIqJibHH3LqPqjFV+wAAAAjqCkxqaqpGjBihDh066MqVK1q7dq1ycnK0bds2ud1uTZo0ScnJyWrTpo1cLpeee+45eb1eDRgwQJI0bNgwxcTEaPz48Vq6dKl8Pp/mzZunpKQk++rJtGnT9NZbb2nOnDl69tlntXPnTm3YsEEZGXzyBwAAfCWogLlw4YImTJig8+fPy+12q3fv3tq2bZt++tOfSpJef/11NWnSRKNHj1ZpaakSEhL09ttv288PDQ3Vli1bNH36dHm9Xt1zzz2aOHGiFi1aZI/p1KmTMjIyNGvWLC1btkzt27fXqlWrlJCQUEuHDAAATPe9vwfmbsX3wACNG98DAzROdf49MAAAAA2FgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHGCCpi0tDT96Ec/UuvWrRUeHq4nn3xSBQUFAWMGDx6skJCQgGXatGkBY4qKipSYmKiWLVsqPDxcs2fPVnl5ecCYnJwc9evXT06nU127dlV6enrNjhAAADQ6QQXMrl27lJSUpH379ikrK0s3b97UsGHDdO3atYBxU6ZM0fnz5+1l6dKl9raKigolJiaqrKxMe/fu1erVq5Wenq758+fbYwoLC5WYmKghQ4YoPz9fM2fO1OTJk7Vt27bvebgAAKAxaBrM4MzMzIDH6enpCg8PV15engYNGmSvb9mypTweT7X72L59u44dO6YdO3YoIiJCffv21eLFi5WSkqKFCxfK4XBo5cqV6tSpk1599VVJUo8ePfTRRx/p9ddfV0JCQrDHCAAAGpnvdQ9MSUmJJKlNmzYB69esWaN27drpwQcfVGpqqr788kt7W25urnr16qWIiAh7XUJCgvx+v44ePWqPiY+PD9hnQkKCcnNzbzuX0tJS+f3+gAUAADROQV2BuVVlZaVmzpypRx55RA8++KC9fuzYserYsaOioqJ06NAhpaSkqKCgQJs2bZIk+Xy+gHiRZD/2+XzfOsbv9+v69etq0aLFN+aTlpamF198saaHAwAADFLjgElKStKRI0f00UcfBayfOnWq/edevXopMjJSQ4cO1alTp9SlS5eaz/Q7pKamKjk52X7s9/sVHR1dZ68HAAAaTo1+hDRjxgxt2bJF//jHP9S+fftvHRsXFydJOnnypCTJ4/GouLg4YEzV46r7Zm43xuVyVXv1RZKcTqdcLlfAAgAAGqegAsayLM2YMUObN2/Wzp071alTp+98Tn5+viQpMjJSkuT1enX48GFduHDBHpOVlSWXy6WYmBh7THZ2dsB+srKy5PV6g5kuAABopIIKmKSkJP31r3/V2rVr1bp1a/l8Pvl8Pl2/fl2SdOrUKS1evFh5eXk6c+aM3n//fU2YMEGDBg1S7969JUnDhg1TTEyMxo8fr//5n//Rtm3bNG/ePCUlJcnpdEqSpk2bptOnT2vOnDk6ceKE3n77bW3YsEGzZs2q5cMHAAAmCipgVqxYoZKSEg0ePFiRkZH2sn79ekmSw+HQjh07NGzYMHXv3l3PP/+8Ro8erQ8++MDeR2hoqLZs2aLQ0FB5vV798pe/1IQJE7Ro0SJ7TKdOnZSRkaGsrCz16dNHr776qlatWsVHqAEAgCQpxLIsq6EnURf8fr/cbrdKSkpq/X6Y++dm1Or+AATvzJLEhp4CgDpwp+/f/C4kAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGCSpg0tLS9KMf/UitW7dWeHi4nnzySRUUFASMuXHjhpKSktS2bVu1atVKo0ePVnFxccCYoqIiJSYmqmXLlgoPD9fs2bNVXl4eMCYnJ0f9+vWT0+lU165dlZ6eXrMjBAAAjU5QAbNr1y4lJSVp3759ysrK0s2bNzVs2DBdu3bNHjNr1ix98MEH2rhxo3bt2qVz587pqaeesrdXVFQoMTFRZWVl2rt3r1avXq309HTNnz/fHlNYWKjExEQNGTJE+fn5mjlzpiZPnqxt27bVwiEDAADThViWZdX0yRcvXlR4eLh27dqlQYMGqaSkRPfdd5/Wrl2rp59+WpJ04sQJ9ejRQ7m5uRowYIC2bt2qxx9/XOfOnVNERIQkaeXKlUpJSdHFixflcDiUkpKijIwMHTlyxH6tMWPG6PLly8rMzLyjufn9frndbpWUlMjlctX0EKt1/9yMWt0fgOCdWZLY0FMAUAfu9P37e90DU1JSIklq06aNJCkvL083b95UfHy8PaZ79+7q0KGDcnNzJUm5ubnq1auXHS+SlJCQIL/fr6NHj9pjbt1H1ZiqfVSntLRUfr8/YAEAAI1TjQOmsrJSM2fO1COPPKIHH3xQkuTz+eRwOBQWFhYwNiIiQj6fzx5za7xUba/a9m1j/H6/rl+/Xu180tLS5Ha77SU6OrqmhwYAAO5yNQ6YpKQkHTlyROvWravN+dRYamqqSkpK7OXs2bMNPSUAAFBHmtbkSTNmzNCWLVu0e/dutW/f3l7v8XhUVlamy5cvB1yFKS4ulsfjscccOHAgYH9Vn1K6dczXP7lUXFwsl8ulFi1aVDsnp9Mpp9NZk8MBAACGCeoKjGVZmjFjhjZv3qydO3eqU6dOAdtjY2PVrFkzZWdn2+sKCgpUVFQkr9crSfJ6vTp8+LAuXLhgj8nKypLL5VJMTIw95tZ9VI2p2gcAAPhhC+oKTFJSktauXav/+q//UuvWre17Vtxut1q0aCG3261JkyYpOTlZbdq0kcvl0nPPPSev16sBAwZIkoYNG6aYmBiNHz9eS5culc/n07x585SUlGRfQZk2bZreeustzZkzR88++6x27typDRs2KCODT/8AAIAgr8CsWLFCJSUlGjx4sCIjI+1l/fr19pjXX39djz/+uEaPHq1BgwbJ4/Fo06ZN9vbQ0FBt2bJFoaGh8nq9+uUvf6kJEyZo0aJF9phOnTopIyNDWVlZ6tOnj1599VWtWrVKCQkJtXDIAADAdN/re2DuZnwPDNC48T0wQONUL98DAwAA0BAIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGCfogNm9e7dGjRqlqKgohYSE6L333gvY/qtf/UohISEBy/DhwwPGXLp0SePGjZPL5VJYWJgmTZqkq1evBow5dOiQBg4cqObNmys6OlpLly4N/ugAAECjFHTAXLt2TX369NHy5ctvO2b48OE6f/68vfztb38L2D5u3DgdPXpUWVlZ2rJli3bv3q2pU6fa2/1+v4YNG6aOHTsqLy9PL7/8shYuXKh33nkn2OkCAIBGqGmwTxgxYoRGjBjxrWOcTqc8Hk+1244fP67MzEx9/PHH6t+/vyTpzTff1MiRI/XKK68oKipKa9asUVlZmd599105HA717NlT+fn5eu211wJCBwAA/DDVyT0wOTk5Cg8PV7du3TR9+nR9/vnn9rbc3FyFhYXZ8SJJ8fHxatKkifbv32+PGTRokBwOhz0mISFBBQUF+uKLL+piygAAwCBBX4H5LsOHD9dTTz2lTp066dSpU/rd736nESNGKDc3V6GhofL5fAoPDw+cRNOmatOmjXw+nyTJ5/OpU6dOAWMiIiLsbffee+83Xre0tFSlpaX2Y7/fX9uHBgAA7hK1HjBjxoyx/9yrVy/17t1bXbp0UU5OjoYOHVrbL2dLS0vTiy++WGf7BwAAd486/xh1586d1a5dO508eVKS5PF4dOHChYAx5eXlunTpkn3fjMfjUXFxccCYqse3u7cmNTVVJSUl9nL27NnaPhQAAHCXqPOA+fTTT/X5558rMjJSkuT1enX58mXl5eXZY3bu3KnKykrFxcXZY3bv3q2bN2/aY7KystStW7dqf3wkfXXjsMvlClgAAEDjFHTAXL16Vfn5+crPz5ckFRYWKj8/X0VFRbp69apmz56tffv26cyZM8rOztYTTzyhrl27KiEhQZLUo0cPDR8+XFOmTNGBAwe0Z88ezZgxQ2PGjFFUVJQkaezYsXI4HJo0aZKOHj2q9evXa9myZUpOTq69IwcAAMYKOmA++eQTPfTQQ3rooYckScnJyXrooYc0f/58hYaG6tChQ/rZz36mBx54QJMmTVJsbKz++7//W06n097HmjVr1L17dw0dOlQjR47Uo48+GvAdL263W9u3b1dhYaFiY2P1/PPPa/78+XyEGgAASJJCLMuyGnoSdcHv98vtdqukpKTWf5x0/9yMWt0fgOCdWZLY0FMAUAfu9P2b34UEAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4TRt6AgBQE/fPzWjoKQA/aGeWJDbo63MFBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYJygA2b37t0aNWqUoqKiFBISovfeey9gu2VZmj9/viIjI9WiRQvFx8frX//6V8CYS5cuady4cXK5XAoLC9OkSZN09erVgDGHDh3SwIED1bx5c0VHR2vp0qXBHx0AAGiUgg6Ya9euqU+fPlq+fHm125cuXao//vGPWrlypfbv36977rlHCQkJunHjhj1m3LhxOnr0qLKysrRlyxbt3r1bU6dOtbf7/X4NGzZMHTt2VF5enl5++WUtXLhQ77zzTg0OEQAANDYhlmVZNX5ySIg2b96sJ598UtJXV1+ioqL0/PPP67e//a0kqaSkRBEREUpPT9eYMWN0/PhxxcTE6OOPP1b//v0lSZmZmRo5cqQ+/fRTRUVFacWKFXrhhRfk8/nkcDgkSXPnztV7772nEydO3NHc/H6/3G63SkpK5HK5anqI1bp/bkat7g8AANOcWZJYJ/u90/fvWr0HprCwUD6fT/Hx8fY6t9utuLg45ebmSpJyc3MVFhZmx4skxcfHq0mTJtq/f789ZtCgQXa8SFJCQoIKCgr0xRdfVPvapaWl8vv9AQsAAGicajVgfD6fJCkiIiJgfUREhL3N5/MpPDw8YHvTpk3Vpk2bgDHV7ePW1/i6tLQ0ud1ue4mOjv7+BwQAAO5KjeZTSKmpqSopKbGXs2fPNvSUAABAHanVgPF4PJKk4uLigPXFxcX2No/HowsXLgRsLy8v16VLlwLGVLePW1/j65xOp1wuV8ACAAAap1oNmE6dOsnj8Sg7O9te5/f7tX//fnm9XkmS1+vV5cuXlZeXZ4/ZuXOnKisrFRcXZ4/ZvXu3bt68aY/JyspSt27ddO+999bmlAEAgIGCDpirV68qPz9f+fn5kr66cTc/P19FRUUKCQnRzJkz9Yc//EHvv/++Dh8+rAkTJigqKsr+pFKPHj00fPhwTZkyRQcOHNCePXs0Y8YMjRkzRlFRUZKksWPHyuFwaNKkSTp69KjWr1+vZcuWKTk5udYOHAAAmKtpsE/45JNPNGTIEPtxVVRMnDhR6enpmjNnjq5du6apU6fq8uXLevTRR5WZmanmzZvbz1mzZo1mzJihoUOHqkmTJho9erT++Mc/2tvdbre2b9+upKQkxcbGql27dpo/f37Ad8UAAIAfru/1PTB3M74HBgCAutOovgcGAACgPhAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwTq0HzMKFCxUSEhKwdO/e3d5+48YNJSUlqW3btmrVqpVGjx6t4uLigH0UFRUpMTFRLVu2VHh4uGbPnq3y8vLanioAADBU07rYac+ePbVjx47//yJN///LzJo1SxkZGdq4caPcbrdmzJihp556Snv27JEkVVRUKDExUR6PR3v37tX58+c1YcIENWvWTC+99FJdTBcAABimTgKmadOm8ng831hfUlKiP//5z1q7dq0ee+wxSdJf/vIX9ejRQ/v27dOAAQO0fft2HTt2TDt27FBERIT69u2rxYsXKyUlRQsXLpTD4aiLKQMAAIPUyT0w//rXvxQVFaXOnTtr3LhxKioqkiTl5eXp5s2bio+Pt8d2795dHTp0UG5uriQpNzdXvXr1UkREhD0mISFBfr9fR48eve1rlpaWyu/3BywAAKBxqvWAiYuLU3p6ujIzM7VixQoVFhZq4MCBunLlinw+nxwOh8LCwgKeExERIZ/PJ0ny+XwB8VK1vWrb7aSlpcntdttLdHR07R4YAAC4a9T6j5BGjBhh/7l3796Ki4tTx44dtWHDBrVo0aK2X86Wmpqq5ORk+7Hf7ydiAABopOr8Y9RhYWF64IEHdPLkSXk8HpWVleny5csBY4qLi+17Zjwezzc+lVT1uLr7aqo4nU65XK6ABQAANE51HjBXr17VqVOnFBkZqdjYWDVr1kzZ2dn29oKCAhUVFcnr9UqSvF6vDh8+rAsXLthjsrKy5HK5FBMTU9fTBQAABqj1HyH99re/1ahRo9SxY0edO3dOCxYsUGhoqH7xi1/I7XZr0qRJSk5OVps2beRyufTcc8/J6/VqwIABkqRhw4YpJiZG48eP19KlS+Xz+TRv3jwlJSXJ6XTW9nQBAICBaj1gPv30U/3iF7/Q559/rvvuu0+PPvqo9u3bp/vuu0+S9Prrr6tJkyYaPXq0SktLlZCQoLffftt+fmhoqLZs2aLp06fL6/Xqnnvu0cSJE7Vo0aLanioAADBUiGVZVkNPoi74/X653W6VlJTU+v0w98/NqNX9AQBgmjNLEutkv3f6/s3vQgIAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYJy7OmCWL1+u+++/X82bN1dcXJwOHDjQ0FMCAAB3gbs2YNavX6/k5GQtWLBA//znP9WnTx8lJCTowoULDT01AADQwO7agHnttdc0ZcoU/frXv1ZMTIxWrlypli1b6t13323oqQEAgAbWtKEnUJ2ysjLl5eUpNTXVXtekSRPFx8crNze32ueUlpaqtLTUflxSUiJJ8vv9tT6/ytIva32fAACYpC7eX2/dr2VZ3zrurgyYzz77TBUVFYqIiAhYHxERoRMnTlT7nLS0NL344ovfWB8dHV0ncwQA4IfM/Ubd7v/KlStyu9233X5XBkxNpKamKjk52X5cWVmpS5cuqW3btgoJCam11/H7/YqOjtbZs2flcrlqbb/4Js51/eA81w/Oc/3gPNePujzPlmXpypUrioqK+tZxd2XAtGvXTqGhoSouLg5YX1xcLI/HU+1znE6nnE5nwLqwsLC6mqJcLhf/ctQTznX94DzXD85z/eA814+6Os/fduWlyl15E6/D4VBsbKyys7PtdZWVlcrOzpbX623AmQEAgLvBXXkFRpKSk5M1ceJE9e/fXw8//LDeeOMNXbt2Tb/+9a8bemoAAKCB3bUB88wzz+jixYuaP3++fD6f+vbtq8zMzG/c2FvfnE6nFixY8I0fV6H2ca7rB+e5fnCe6wfnuX7cDec5xPquzykBAADcZe7Ke2AAAAC+DQEDAACMQ8AAAADjEDAAAMA4BEw1li9frvvvv1/NmzdXXFycDhw48K3jN27cqO7du6t58+bq1auXPvzww3qaqfmCOdd/+tOfNHDgQN1777269957FR8f/53/bPCVYP9OV1m3bp1CQkL05JNP1u0EG4lgz/Ply5eVlJSkyMhIOZ1OPfDAA/z34w4Ee57feOMNdevWTS1atFB0dLRmzZqlGzdu1NNszbR7926NGjVKUVFRCgkJ0Xvvvfedz8nJyVG/fv3kdDrVtWtXpaen1+0kLQRYt26d5XA4rHfffdc6evSoNWXKFCssLMwqLi6udvyePXus0NBQa+nSpdaxY8esefPmWc2aNbMOHz5czzM3T7DneuzYsdby5cutgwcPWsePH7d+9atfWW632/r000/reeZmCfY8VyksLLT+7d/+zRo4cKD1xBNP1M9kDRbseS4tLbX69+9vjRw50vroo4+swsJCKycnx8rPz6/nmZsl2PO8Zs0ay+l0WmvWrLEKCwutbdu2WZGRkdasWbPqeeZm+fDDD60XXnjB2rRpkyXJ2rx587eOP336tNWyZUsrOTnZOnbsmPXmm29aoaGhVmZmZp3NkYD5mocffthKSkqyH1dUVFhRUVFWWlpateN//vOfW4mJiQHr4uLirP/4j/+o03k2BsGe668rLy+3Wrduba1evbquptgo1OQ8l5eXWz/+8Y+tVatWWRMnTiRg7kCw53nFihVW586drbKysvqaYqMQ7HlOSkqyHnvssYB1ycnJ1iOPPFKn82xM7iRg5syZY/Xs2TNg3TPPPGMlJCTU2bz4EdItysrKlJeXp/j4eHtdkyZNFB8fr9zc3Gqfk5ubGzBekhISEm47Hl+pybn+ui+//FI3b95UmzZt6mqaxqvpeV60aJHCw8M1adKk+pim8Wpynt9//315vV4lJSUpIiJCDz74oF566SVVVFTU17SNU5Pz/OMf/1h5eXn2j5lOnz6tDz/8UCNHjqyXOf9QNMR74V37TbwN4bPPPlNFRcU3vu03IiJCJ06cqPY5Pp+v2vE+n6/O5tkY1ORcf11KSoqioqK+8S8N/r+anOePPvpIf/7zn5Wfn18PM2wcanKeT58+rZ07d2rcuHH68MMPdfLkSf3mN7/RzZs3tWDBgvqYtnFqcp7Hjh2rzz77TI8++qgsy1J5ebmmTZum3/3ud/Ux5R+M270X+v1+Xb9+XS1atKj11+QKDIy0ZMkSrVu3Tps3b1bz5s0bejqNxpUrVzR+/Hj96U9/Urt27Rp6Oo1aZWWlwsPD9c477yg2NlbPPPOMXnjhBa1cubKhp9ao5OTk6KWXXtLbb7+tf/7zn9q0aZMyMjK0ePHihp4avieuwNyiXbt2Cg0NVXFxccD64uJieTyeap/j8XiCGo+v1ORcV3nllVe0ZMkS7dixQ717967LaRov2PN86tQpnTlzRqNGjbLXVVZWSpKaNm2qgoICdenSpW4nbaCa/H2OjIxUs2bNFBoaaq/r0aOHfD6fysrK5HA46nTOJqrJef7973+v8ePHa/LkyZKkXr166dq1a5o6dapeeOEFNWnC/8fXhtu9F7pcrjq5+iJxBSaAw+FQbGyssrOz7XWVlZXKzs6W1+ut9jlerzdgvCRlZWXddjy+UpNzLUlLly7V4sWLlZmZqf79+9fHVI0W7Hnu3r27Dh8+rPz8fHv52c9+piFDhig/P1/R0dH1OX1j1OTv8yOPPKKTJ0/agShJ//u//6vIyEji5TZqcp6//PLLb0RKVTRa/CrAWtMg74V1dnuwodatW2c5nU4rPT3dOnbsmDV16lQrLCzM8vl8lmVZ1vjx4625c+fa4/fs2WM1bdrUeuWVV6zjx49bCxYs4GPUdyjYc71kyRLL4XBYf//7363z58/by5UrVxrqEIwQ7Hn+Oj6FdGeCPc9FRUVW69atrRkzZlgFBQXWli1brPDwcOsPf/hDQx2CEYI9zwsWLLBat25t/e1vf7NOnz5tbd++3erSpYv185//vKEOwQhXrlyxDh48aB08eNCSZL322mvWwYMHrf/7v/+zLMuy5s6da40fP94eX/Ux6tmzZ1vHjx+3li9fzseoG8Kbb75pdejQwXI4HNbDDz9s7du3z972k5/8xJo4cWLA+A0bNlgPPPCA5XA4rJ49e1oZGRn1PGNzBXOuO3bsaEn6xrJgwYL6n7hhgv07fSsC5s4Fe5737t1rxcXFWU6n0+rcubP1n//5n1Z5eXk9z9o8wZznmzdvWgsXLrS6dOliNW/e3IqOjrZ+85vfWF988UX9T9wg//jHP6r9723VuZ04caL1k5/85BvP6du3r+VwOKzOnTtbf/nLX+p0jiGWxTU0AABgFu6BAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGOf/AXtFwTyjpSeQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"There is 4672 DNA fragments in the training set\")\n",
    "\n",
    "\n",
    "plt.imshow(X_train[0].T, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n",
    "# histo = np.histogram(y_train, bins=2)\n",
    "plt.hist(y_train, bins=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EajNPJrnAmde"
   },
   "source": [
    "# Ex 3: Convert numpy array to tensor pytorch\n",
    "\n",
    "As you see in the previous exercise, **X_train** consists of 4672 segments. Each segment is encoded by 0 and 1 (one-hot encoding).\n",
    "\n",
    "\n",
    "1. Convert numpy array **X_train**, **y_train** into pytorch tensor. Reshape **X_train** to (4672, 4, 101). Note that the type of **X_train** and **y_train** should be float.\n",
    "\n",
    "2. Do the same thing for **X_test** and **y_test**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E8eedW_yPUKu"
   },
   "outputs": [],
   "source": [
    "# from torch import Tensor\n",
    "import torch\n",
    "\n",
    "def get_torch(X, y):\n",
    "    Xs = torch.tensor(X.astype(np.float32))\n",
    "    Xs = Xs.permute(0, 2, 1)\n",
    "    ys = torch.tensor(y.astype(np.float32))\n",
    "    return Xs, ys\n",
    "X_train_ts, y_train_ts = get_torch(X_train, y_train)\n",
    "X_test_ts, y_test_ts = get_torch(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GrBcHzR-E-yR"
   },
   "outputs": [],
   "source": [
    "''' Tests X_train_ts, y_train_ts '''\n",
    "assert(type(X_train_ts) is torch.Tensor)\n",
    "assert(type(y_train_ts) is torch.Tensor)\n",
    "assert(X_train_ts.shape == (4672, 4, 101))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wn_0dOuVHWhs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "o_it3skAH0Yt"
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "assert(type(X_test_ts) is torch.Tensor)\n",
    "assert(type(y_test_ts) is torch.Tensor)\n",
    "assert(X_test_ts.shape == (584, 4, 101))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYJBjBdg547"
   },
   "source": [
    "# Ex4: Create Dataset\n",
    "In order to train a deep learning model with Pytorch, we need a pytorch dataset.\n",
    "The DNADataset class below allows for creating a pytorch Dataset from DNA segments and their labels.\n",
    "\n",
    "1. Using this class, create a dataset for training set. You should call it **train_dataset**\n",
    "\n",
    "2. Create **Dataloader** from **train_dataset**. You should call it **train_loader**.\n",
    "\n",
    "3. Do the same thing for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GkvyOQVggLz_"
   },
   "outputs": [],
   "source": [
    "class DNADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dna, labels):\n",
    "        self.labels = labels\n",
    "        self.dna = dna\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        frag_dna = self.dna[idx]\n",
    "\n",
    "        sample = {'DNA': frag_dna, 'Class': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_d7J-rFbiig"
   },
   "source": [
    "**train_loader** is a generator. To get data out of it, you need to loop through it or convert it to an iterator and call next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DNADataset(X_train_ts, y_train_ts)\n",
    "test_dataset = DNADataset(X_test_ts, y_test_ts)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJQK0ZaLMpKF",
    "outputId": "76fb9583-e609-4e26-e954-1c88af29b5bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a batch data  torch.Size([32, 4, 101])\n",
      "Shape of label torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Run this to test your data loader\n",
    "data = next(iter(train_loader))\n",
    "dna = data['DNA']\n",
    "label = data['Class']\n",
    "print(\"a batch data \", dna.shape)\n",
    "print(\"Shape of label\", label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDcbP17Rb2xt"
   },
   "source": [
    "# Design and implement a convolutional neural network\n",
    "\n",
    "Now, it's time to build your model. This is a binary classification problem. We can use a convolution neural network, just like an image classification problem. However, since the size of a DNA segment is (4, 101), we will use 1D convolution instead of 2D convolution.\n",
    "\n",
    "\n",
    "\n",
    "Firstly, we will test how does a 1D convolution work on our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlPP4E3UfRYe"
   },
   "source": [
    "# EX 5: 1D Convolution\n",
    "\n",
    "1. With the help of the torch.nn.Conv1d class, create a 1D convolutional layer. You need to choose values for the following parameters: **in_channels**, **out_channels**, **kernel_size**.\n",
    "\n",
    "\n",
    "2. Apply this layer to **dna_seg** below. What is the size of the output ?\n",
    "\n",
    "\n",
    "3. [Optional] Display the output by using matplotlib.pyplot.imshow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C9y0yRXhhj1F"
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "dna_seg = data['DNA']\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJ9uE489imxl",
    "outputId": "b57cbf60-51b3-4746-e815-f58e24772b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after first convolution:  torch.Size([32, 16, 101])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "conv1 = nn.Conv1d(4, 16, kernel_size=3, stride=1, padding=1)\n",
    "output = conv1(dna_seg)\n",
    "print(\"Output shape after first convolution: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7a2f651a74a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAB9CAYAAACMJEjVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIkxJREFUeJztnXl4VdW5xt8TMANDEoFLQkoCEWlxQECGEKEVNZapOKCIFjCAlSJgidyKIIM4YKi2iNapeivYOqC0gJaiXhpEpCUhREABQRCQKCYUaHJCQMBk3T+s53LWerE7EDj7Ie/vefI87PestdfwrW+fxTnf+VbAGGMghBBCCOFDoiLdASGEEEKIE6GNihBCCCF8izYqQgghhPAt2qgIIYQQwrdooyKEEEII36KNihBCCCF8izYqQgghhPAt2qgIIYQQwrdooyKEEEII36KNihBCCCF8y2nbqDz11FNo3bo1YmNjkZGRgTVr1pyupoQQQghxlhI4HWf9vPbaa7j11lvx7LPPIiMjA3PmzMGCBQuwdetWNG/e/DvrVldXY8+ePWjcuDECgUBtd00IIYQQpwFjDCoqKpCSkoKoqNr7HOS0bFQyMjLQtWtXPPnkkwC+2XykpqbizjvvxKRJk76z7ueff47U1NTa7pIQQgghzgDFxcVo2bJlrd2vfq3d6d8cPXoURUVFmDx5ckiLiopCVlYWVq9e7ZQ/cuQIjhw5Err+dt/0AIDY48r9nPWUfDgzY4+rXUiq3nSlJRxzy0x/39Ue8NiPJaQfP/kJqdvIlSbOd7VHRrra+y+42g+vCb/+8E23DBkqOrNx9XGlHUtc7bxrXO1m0u58MoZCMoa/kq6MIlpKSvg1s/2M811t0XZXa0Pu38xDmzVpl0180Wf/uVh3e60CKFzuaj8gTeYR7XqPa3gSGdesOFd7/7CrbSRN3GHNHfWRX7naxHtc7ZFLXG3Th672CelHC6J1J+OqJOP6gNS1fW44WfujWZvEru8Tu9r3B07gX993tUoyAZtJX/5MNMfW5EE6vMjVJpB77SLaNR5983oyT4vIPDEfvuQk/Z/V+4zUa0We1dNvdrV+pG9e7c+eQxd0djVmi3lWG5+R+7fq7mrBfFdbSvpx801WvWNA6iKgcePGpPTJU+sblX379qGqqgpJSUlhelJSErZs2eKUz83Nxf333+/osQCO95N49i0Q+WQphhRrQLR4e+TkcyV2L6/9oG2eQ8Roj+2Scg09tEH2QXSjQsdF+suWHxsXG6rXMbDx03ateafzVs/VmG3YPHlpsybtospbu0ftexEvpbYnGl2Hp+BLrC7rSyzR7LmjfSMbBq/zy+aStUHnjoyLmdCLz7G1T+t5tatX/yId9joGT7YmN2P98GoHr77J5smrD5+s/7N69HlAbsbm8lTs73VcdE1YbdAxeNwFENfk72lArYdtRPxXP5MnT0Z5eXnor7i4ONJdEkIIIYRPqPVPVJo1a4Z69eqhtLQ0TC8tLUVycrJTPiYmBjEx7h7051nWbo199v+UK+Wyz9jWE80Kgyn8vVvkVz8i9e7y1o/rfk7KbSPaP1xpsitRXiVaL+tz3VakzLljiZhFtEWuRD6tRxvyWfLlpBzjK6LlMluz/j1n1WO27+RKgzaQcuz7QXe5Om3WpF2Qj467sgmw+0dCtli1+BxXG8QK9iYaWcOPVbjav8pdjf2najz7HsoaB/WRv7kS9QfyFVx7sjjbX0/qku+l/rXK1cjwPfncbFLmez2JSOzq5f4A96/Kj11tPynHlsRjCa5m2/oz8sNNOtYnXK0j+xiXfBpBfbMrKXceKcd8mCxOT/5P6rXeR+rNdSX6vlFNNI/2H0G09R5tgcHhl61/SMqQEAf2LBlGxgr7q+uvWSdOnVr/RCU6OhqdO3dGXt7/fzteXV2NvLw8ZGZm1nZzQgghhDiLqfVPVABgwoQJyM7ORpcuXdCtWzfMmTMHlZWVGDGC7Q2FEEIIITinZaMyePBg/POf/8T06dNRUlKCjh074u2333YCbIUQQgghvovTslEBgHHjxmHcuHGn6/ZCCCGEqAOcto3KKfMvhP8cjgTZ4b9caRkJdry6G6lbEH55KSny6UpXa9PeWz9WTHO1XreSujtcKUiKJR9wtZ+Rch9ZAXXtWaQji7Bl80tI9NAmAFzNKpMxsPtRG7JAZCvYldZjCSxYNCFL/EGCiVmAred2WQAgo9K6LnCLJJJqJXNcLXk4KejRl5aSwNl+6a62faerVWx1tT6Wk1EfedDVgiRnSDL5qfuuUldrzWxNcmSc+4WrLSHj8uJz9BebLK8OsauX+wPcvxr2d7WFJCkRe4R5sTW1M7kX9SWWvOViojF72f5wonKsXRaF6aUuq8cCgskz/VMSwN1mOKnr0f5s6ljcMLWFPXe7SBkSYFtCfDN5CKlrj/UQAJKD5VSJ+M+ThRBCCCFOhDYqQgghhPAt2qgIIYQQwrdooyKEEEII33JaTk8+FYLBIBISErAO4ecStCFZPReTALXrWMBqItHsLRoLsCJBkuyUM9oPkukSNxCNZWH9J9EmEo0EPGGKdf0bUuYtopHAYfQg2i1EY2NgfXvUlSpJgGlDjzZcbGXApLYngcMLSMDmoLtJ3SOuZLdZk3bxkrc2HFs86RapnONqDUkw4QISsDjIqy+xNfwnorFAZxIVuNgKvKP3v41oVxAtkWgk4+4CUpeNn80xyKFxeI1ols/tJ5GOTb22yXzf9mnAs3+xANNKcshhQy+2JkGi+0mK3KbMl4jfsGca9U0yJwtI8Df1YXLolCf/Z4dVkezNIPfCnURjp2N6tT/78ax9IBhOYAv7fixjNDsAiJ0MyzLuWj84CFYCCf2A8vJyxMez08dODn2iIoQQQgjfoo2KEEIIIXyLNipCCCGE8C3aqAghhBDCt/g2mPbXAOKO01ls08gurlay1tXIaeL4h3Xdhxxz/hDJ1khOl6f9+DvpR48mpHJfVxr8MilHYNkJ77O2nk+T48XtsQM8bnAk6W8RyS7bmWx3G5J2f0LaGEy0y4hGYudQz5p3ZvvkGa62nGhsx85ORLfbrEm7+B9XGvq5q3W3rllS00NEG/hjV1v+v662i9Rla3gZGRfLfsmOpmd9zrXaoD5Cgi4H/8LV2pL7TycaixFlmT6ZXz9EtGuJZvtcd7L2h5J6HpPVOvcHuH8xHyYJdzGVaCye0rY1y7g7k2iziNaLZDRGtisx37ySBKYv/8zVmA/3Ivfz4v+s3vNEu534zUNkXSe6kmf7M58jcbPUFvnW3D1P5o09by8iz5LZ5FliJ4g+jG9ipBVMK4QQQog6gzYqQgghhPAt2qgIIYQQwrdooyKEEEII3+LbYNry64H446O3WLAfyWx4dSNX++VBV/u1nU21E+nMIqJleezHcI/3I1GiW0gWy3YkkGkvCXhqbmexZFkt7yUaiyYk0VmTSYBxLmvjA6LdRTQSYIqlRGvnSnawJ7M9mhKNHdfOxk9gAaae22VRa0mulD8n/Lo7y/xbRjT7SHcAWEe0HFdia7iYVGUBlcNYtloS7bqsQ/g185Gj81xtB7l9OzYnNxGNZA3O3+NqLLnuMOP+P25vwI1idXyuJ2mTBM0zu+4l2bCd+wPUvw6TjNNxZAwY7I7hhdfdYrath7GsqcQfWNbcXaRca5a9mfnmEKKxHxwwH2Z99uL/rB7JzEsXJ8lynk+e317tz/yws1dbLLeu2djnEo2s4YUkW+1Ay+eCx4CERQqmFUIIIUQdQhsVIYQQQvgWbVSEEEII4Vu0URFCCCGEb/FvMO0UIP74dLQskCmXaL8k2nCi3Whdf0rKkMyJNEUq68dLZA84m6STHEHqTiJahistv83VrrQznT5D7sXGQIKE8Q7R7OAsAPidK21p6Wrtfk/qssDet4lWQrSXrGtme8YrRGtPNDZPdps1abeMaBuIZs87C8JlR8SXfN/VJpLz5dm6Zms41ZUOk7SjcSyY+A6i2UGszEdeIT7yHrkXi+o9j2hs3Wx0JRqISo61X77S1RyfG0DaTCYasetyEojq3B8n8C+W+fpiom0lGnkO2baOY8HKLAqZBc2ziNB+RGO+SQK98QjRmA8zzYv/s3pbiMbsymD99Wr/K0hd9mxitrCDeP9GyrAfPjxCniWp5FlivY8GjwAJzyiYVgghhBB1CG1UhBBCCOFbtFERQgghhG/RRkUIIYQQvsW/wbS/AuLjjnuBnGu9q4OrkUSfiCNZbVFoBfpMD7plSBZWkABW1o/WLHiqlGgvEu1+oo0Z5mrZf3S1a6zrGy51y/yVRE+RwEn8wZWCv3G1eJaZdCDRAmQMGe4YDq9xi8WR7Im7rABAansWAHgh0ViQMAkwtNusUbv/TTSWYfJN6/p3JCgtg6zXhuRe7Dx4EphO1zDLHDqDaCzw8AekjcbW/ZmPsCyZ44jGggkZm4lGMtMik2jD57hado6r2T7H1he5FbVrNrGrfX+A+9dHRLuENJyR42rs2TTDuh5OopU/JqlZR5N7sYBNFkzL5i6WaCT7K/VhFuzqxf9ZvdfSXG3obldjWalZf73anwVE3+3RFvaz5B4yhqZkDOR5QAPkredLsAJIOF/BtEIIIYSoQ2ijIoQQQgjfoo2KEEIIIXxL/Uh34IRMAxA47nq6W6SKVIsjCXOeftDVsgPh3wU2JAmU6HeNJCCB9YOdHguSLIvFLRSOdbWuS91YjtnkNMsJDcKv99/ofjnclCXLmkE0Ei/AqrIkQtl2Qj0AL/Z3xxAk8SjxxIbsO3573pnt8aorLSQnxQ684eTarEm7m0jSvovIhO63TjiNfc793pr1I56cKLuQnDw7kLRJ1zD5Xn0oidGZSqq2Iyd7O20wHyEnh1N/YIFB5Gv7hatdjYXtjCen1r50Z46jzSYnsds+93ey9snh39SuJHeic3+A+xeRMKBRjqMFyRjiyXzatv7jCDcG4h+kzR7El6pIm/VI3Ar1TXJ69EJysjv1Ya9t2HVJvd0BN5YjjZ3sTBIU7ieJHb3a/1qinTfRoy2suaNjYDGcP3OlfiTubqm9bki+xtpAn6gIIYQQwrdooyKEEEII36KNihBCCCF8izYqQgghhPAt/k349ncg/vhTWVlSKXYy8Biy97qfRPjYibZI8BA9jbUx0Vg/WMQeO/F2MNFY8rU2ZFxfk3HZEVXH3CK4imQ8MiQz1hhSlwQm06RaLHqwOdsXs8xK5MjbO8hY7TXBbH8jqdeRNDmVHFF6x7v/uc2atHuA1F3Osk9ZUazjSCKnJ1NcbQXJZLaK3J4EJ9I1zJLWPUu0qSSJ1BckidQb1jXzkfuI9iuitbna1YqWuRo5FRlTiWM/ROaYJHekieZsn/shKRMgbTK7zvFwf+AE/kVOvP2SnHjbgqydfWTt2Lb+CWmTJQYLEF96mvgSS9DWkWijiEaCn6kP30jaZW3YdVm9eqTeb4nGEtTFn4L9K4j2GdGYLez3uTtJmdfIvK0g419E6uaEXwYrgIQOSvgmhBBCiDqENipCCCGE8C3aqAghhBDCt9R4o7Jy5UoMGDAAKSkpCAQCWLx4cdjrxhhMnz4dLVq0QFxcHLKysrBt27ba6q8QQggh6hA1zkxbWVmJDh06YOTIkRg40D3C85FHHsETTzyBF198Eenp6Zg2bRp69+6NzZs3IzaWRRmdgIsAHB+LQw7epcFNN5EgRpZ573+taxbE9wYJgBpKAqBYP9gWkJxGTE8VLRjgag//xdVYkNlW65oF/31Mol9JpkcaPMVOsrXbBIAf3e1qDz/qar1JEB87tZe1cci6ZrZnwaRsDH8mwWNe2qxJu+REYWoLuw22vorIvDF7kZOSPfvSmCaudiuLCL7IlT4gwbQF1jXxkV1/drXWfyL+8AnxB5IhGT2JVkR8+FZSjtm6jGj2OvmRx0BfNude7g9w/zpK/IuNga2dzh5sTbKroiMZ65/JWBlefZNkOaZBp8yHT9b/WT2S+vcwyegbZ8j7BltzXu3fbJCrrVrgaswWP7ZswfxhIpm3I6RcDtHs9y+23mqBGm9U+vbti759+9LXjDGYM2cOpk6dimuv/Sbx7x/+8AckJSVh8eLFuPnmm0+tt0IIIYSoU9RqjMrOnTtRUlKCrKyskJaQkICMjAysXk0O3QBw5MgRBIPBsD8hhBBCCKCWNyolJd/kv0hKCv88LCkpKfSaTW5uLhISEkJ/qaksiYgQQggh6iIR/9XP5MmTUV5eHvorLi6OdJeEEEII4RNqHKPyXSQnf5NltLS0FC1atAjppaWl6NixI60TExODmJgY94XABCBwnN4z1y2zjtywGcmy9xYJFrL2QyVvukWS8akr9gx46wc5wv4wCTSKG+FqwNOudC0JHmSZM/9oXT9GylxHvobrmelIh192i8WxRKrsQ7AvSWAfO6/810SbS2y4gdjQnmNm+wOkHsuIGmjgaqOIwYhdPbdLYt1wAbHFQcsWu0g9luXYjW3nY91CNLaG0cyVCkgw7VaS/vWn5Ha2RuaSJX6m/vD9lq52L0mv+zW5XSXRWAbbuUQ7h2i2zx0lwaQskHoX0bz4NMD9qwWJHbyTDIwEgGKuB1uzNdebjPUG4kv3E19icdlsvbJAZ68+fAtp10tdVo+s17j/Jvdi7xs/IO8bu0hVZv83SMrZX5NgWmYLe+5YAO8QorFIjYZEs+PcT1PkRq1+opKeno7k5GTk5eWFtGAwiIKCAmRmum+EQgghhBDfRY0/UTl48CC2b98eut65cyfWr1+PJk2aIC0tDTk5OXjooYfQtm3b0M+TU1JScN1119Vmv4UQQghRB6jxRmXt2rW44or//6h7woQJAIDs7GzMmzcPEydORGVlJUaNGoWysjL07NkTb7/9ds1yqAghhBBC4CQ2Kr169cJ3HbgcCATwwAMP4IEHHjiljgkhhBBCBMx37ToiQDAYREJCAsqXAvHHBe/kkwCoS0n9aJLVdAU5Ets+cf4xlrEvw5Xyf+OtH+8RjR0kMIacuM4ihx7+3NV6k6r26e8NWbZSkmEyf4arsaS5TYnGTn9v+CNXe3ilq917Pql8zJWWEhtmWdfM9iAJN/FDopEj55du/M9t1qhdktmSjXXTjPDr/yHVHmPzxtrs7kr5T7gaW8OFRLMTOgPA/SzDLEnWay915iNXszGQGMm9xB+as6BAsmA3kfG/Tqoys5KwRtfnSEzvpjtcjdl1qIf7A9y/Hif+xYKTR5K18/ftrmbb+n4WhDuDaE8RjQXOsvsx3yTJm8GC+okP0+BnL/7P6rEMweyHBOSBuIm8b3i1P4vznurVFnam53+QMmysLPibJNfduy/8ugLA+QDKy8sRHx/vVjhJIv7zZCGEEEKIE6GNihBCCCF8izYqQgghhPAt2qgIIYQQwrf4Nph2OIDo4/TfkeS1C8hR1CxxaMdbiPhF+OVhEoh2Dam2zGM/WNAliVejAYskhhOziMZiu7pagYcsG+5oUu9FMq5CMi6WrPNpEuxYSNpl8VlLiMbi5Dre4GoLrEAxavuZpN4UV2Pxdcke2qxJu0dJu7eTui82sgQS6foKWa8/JdGfC0gQ8iCPa7gf6VtDtrBZptuupA0rsJvdiiW/zCEaiU1F+xmkTaINsucXoEHH+MqVCle5mu1zr5G1P5i1SexaSOxq3x/g/nUxaSKO/EjgFTIGljTasTVx4NdI4CxL/JrchYjXuxLzzUHdSLk1rkZ9+CT9n9UrJPXYgS8klhrLTsH+/0Wqth7raswWg625KyTz9ji5/0vkWVJEniWdrTURrAIStiuYVgghhBB1CG1UhBBCCOFbtFERQgghhG/RRkUIIYQQvqXGKfTPFLcCCIs/ItnzLicBgM1J4Oynr7qafZr2iklumSksgtVjP869wNXKPna1Mc+52uxRrnYv6coviWZne4wjAVBXkqyhbFxlZFxPsyyGJEvi3J2uxnbFT7JMpCTl7i5iQztoj9meZcQsIMUGkaDDXSRwlgUKem13Fyl2JdFg2acXCbBbweZtnisVXOFqgzyu4YZkDff6m6vdRLoyppS0YV0zH2FpWM8n2ZVZEHp7kuWZ2pqsfzYuFvw9mWj2+g+StW/bFOB2nefh/oB3/zpGAmd/x9YOacOekzwyR0Fyq2SW5fsjonn1TRI5XUCCQpkPn7T/k3rsGbxsmqs1eZAUPAX7/4lod5HAWWYLO+r8XjJvy1iWW9KRSeRZ8jPrFx0kxrtW0CcqQgghhPAt2qgIIYQQwrdooyKEEEII36KNihBCCCF8i28z05YPA+KPS02b/3u3bHcW7MpSBZKAMufo8FtJmb6ulH+jt37sJcG5zUlGUAwk2o+JlkO0l4lmp46NJmU6uVL+RFfrzvr7R6K9RjQWYGpHMAPAm0R73ZWWkeCxq+15J7b/lNRrQwLgWITdMvuce9ZmTdol2R5BMkw6c8Ki/0hgI/oT7S5XyifBftSXbB8BABKwylLMVhKtodUG9REWYEvWJsqJlks0EpiOd4nGAjHZc4P02fG5n5EyJNMptSsJHKbpoJl/kWB9vOOxL+T56th6j8d+PEY0EmX5KZlf5pslZL0me/ThT4kPe/F/Vu88Ui3AMu6yNTLnP7cJgNufrXVmV2KLEus9jMXNBsizCvcQ7edE2xV+GTwGJLypzLRCCCGEqENooyKEEEII36KNihBCCCF8i+8Svn0bMhM8Gq5XkrJBcropjhKtijXkoR75XtVrPypIuVgWDXSMaIeJxuqy7Dr2/QKkDOkvHdfJtnmicux+rGFiC0/zTuoxOwRJcjN8fZJt1qRdlmiP3c/ui9d5Y+XIWE/Jl9jckRuyNqqsNqiPMF9l/sD6y+bXo609l/Oy/tng2b1q27+8rCXAs885c8LKeB0r0bz6JivXwKO9Ttb/aT2iBdh6ZbY5FfszzaMt7HHEkWoBr+83bN6stRn893Vth776Lpj2888/R2pqaqS7IYQQQoiToLi4GC1btqy1+/luo1JdXY09e/agcePGqKioQGpqKoqLi2s1gljUjGAwKDv4ANnBH8gO/kG28Aff2mH37t0IBAJISUlBVFTtRZb47qufqKio0E4sEPjme4v4+HgtQh8gO/gD2cEfyA7+QbbwBwkJCafFDgqmFUIIIYRv0UZFCCGEEL7F1xuVmJgY3HfffYiJYSlSxZlCdvAHsoM/kB38g2zhD063HXwXTCuEEEII8S2+/kRFCCGEEHUbbVSEEEII4Vu0URFCCCGEb9FGRQghhBC+xbcblaeeegqtW7dGbGwsMjIysGbNmkh36awmNzcXXbt2RePGjdG8eXNcd9112Lp1a1iZr776CmPHjkXTpk3RqFEj3HDDDSgtLY1Qj+sGs2bNQiAQQE5OTkiTHc4cX3zxBYYOHYqmTZsiLi4O7du3x9q1a0OvG2Mwffp0tGjRAnFxccjKysK2bdsi2OOzj6qqKkybNg3p6emIi4tDmzZt8OCDD4adJyM71D4rV67EgAEDkJKSgkAggMWLF4e97mXODxw4gCFDhiA+Ph6JiYm47bbbcPDgwZp3xviQ+fPnm+joaPPCCy+YTZs2mdtvv90kJiaa0tLSSHftrKV3795m7ty5ZuPGjWb9+vWmX79+Ji0tzRw8eDBUZvTo0SY1NdXk5eWZtWvXmu7du5vLLrssgr0+u1mzZo1p3bq1ueSSS8z48eNDuuxwZjhw4IBp1aqVGT58uCkoKDA7duww77zzjtm+fXuozKxZs0xCQoJZvHix2bBhg7nmmmtMenq6OXz4cAR7fnYxc+ZM07RpU7NkyRKzc+dOs2DBAtOoUSPz+OOPh8rIDrXP0qVLzZQpU8zChQsNALNo0aKw173MeZ8+fUyHDh1Mfn6+ef/99835559vbrnllhr3xZcblW7dupmxY8eGrquqqkxKSorJzc2NYK/qFnv37jUAzHvvvWeMMaasrMycc845ZsGCBaEyH3/8sQFgVq9eHalunrVUVFSYtm3bmmXLlpnLL788tFGRHc4c99xzj+nZs+cJX6+urjbJycnm0UcfDWllZWUmJibGvPrqq2eii3WC/v37m5EjR4ZpAwcONEOGDDHGyA5nAnuj4mXON2/ebACYwsLCUJm33nrLBAIB88UXX9Sofd999XP06FEUFRUhKysrpEVFRSErKwurV6+OYM/qFuXl5QCAJk2aAACKiopw7NixMLu0a9cOaWlpsstpYOzYsejfv3/YfAOyw5nkzTffRJcuXTBo0CA0b94cnTp1wvPPPx96fefOnSgpKQmzRUJCAjIyMmSLWuSyyy5DXl4ePvnkEwDAhg0bsGrVKvTt2xeA7BAJvMz56tWrkZiYiC5duoTKZGVlISoqCgUFBTVqz3eHEu7btw9VVVVISkoK05OSkrBly5YI9apuUV1djZycHPTo0QMXX3wxAKCkpATR0dFITEwMK5uUlISSkpII9PLsZf78+fjggw9QWFjovCY7nDl27NiBZ555BhMmTMC9996LwsJC/OIXv0B0dDSys7ND882eVbJF7TFp0iQEg0G0a9cO9erVQ1VVFWbOnIkhQ4YAgOwQAbzMeUlJCZo3bx72ev369dGkSZMa28V3GxURecaOHYuNGzdi1apVke5KnaO4uBjjx4/HsmXLEBsbG+nu1Gmqq6vRpUsXPPzwwwCATp06YePGjXj22WeRnZ0d4d7VHV5//XW8/PLLeOWVV3DRRRdh/fr1yMnJQUpKiuxQR/DdVz/NmjVDvXr1nF8xlJaWIjk5OUK9qjuMGzcOS5YswbvvvouWLVuG9OTkZBw9ehRlZWVh5WWX2qWoqAh79+7FpZdeivr166N+/fp477338MQTT6B+/fpISkqSHc4QLVq0wIUXXhimXXDBBdi9ezcAhOZbz6rTy913341Jkybh5ptvRvv27TFs2DDcddddyM3NBSA7RAIvc56cnIy9e/eGvf7111/jwIEDNbaL7zYq0dHR6Ny5M/Ly8kJadXU18vLykJmZGcGend0YYzBu3DgsWrQIy5cvR3p6etjrnTt3xjnnnBNml61bt2L37t2ySy1y1VVX4aOPPsL69etDf126dMGQIUNC/5Ydzgw9evRwfqL/ySefoFWrVgCA9PR0JCcnh9kiGAyioKBAtqhFDh06hKio8LeqevXqobq6GoDsEAm8zHlmZibKyspQVFQUKrN8+XJUV1cjIyOjZg2eUijwaWL+/PkmJibGzJs3z2zevNmMGjXKJCYmmpKSkkh37azljjvuMAkJCWbFihXmyy+/DP0dOnQoVGb06NEmLS3NLF++3Kxdu9ZkZmaazMzMCPa6bnD8r36MkR3OFGvWrDH169c3M2fONNu2bTMvv/yyadCggXnppZdCZWbNmmUSExPNG2+8YT788ENz7bXX6mextUx2drb53ve+F/p58sKFC02zZs3MxIkTQ2Vkh9qnoqLCrFu3zqxbt84AMLNnzzbr1q0zn332mTHG25z36dPHdOrUyRQUFJhVq1aZtm3bnj0/TzbGmN/+9rcmLS3NREdHm27dupn8/PxId+msBgD9mzt3bqjM4cOHzZgxY8y5555rGjRoYK6//nrz5ZdfRq7TdQR7oyI7nDn+8pe/mIsvvtjExMSYdu3ameeeey7s9erqajNt2jSTlJRkYmJizFVXXWW2bt0aod6enQSDQTN+/HiTlpZmYmNjzXnnnWemTJlijhw5EiojO9Q+7777Ln1PyM7ONsZ4m/P9+/ebW265xTRq1MjEx8ebESNGmIqKihr3JWDMcen9hBBCCCF8hO9iVIQQQgghvkUbFSGEEEL4Fm1UhBBCCOFbtFERQgghhG/RRkUIIYQQvkUbFSGEEEL4Fm1UhBBCCOFbtFERQgghhG/RRkUIIYQQvkUbFSGEEEL4Fm1UhBBCCOFbtFERQgghhG/5P35mdcLexGuEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0].detach().numpy(), cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H26rAy0wkbca"
   },
   "source": [
    "# EX 6: Build a model\n",
    "\n",
    "Create a 3 layer 1-dimensional network to classify the TF binding sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "L_bs8O3lhB7m"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepDNA(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size):\n",
    "    super(DeepDNA, self).__init__()\n",
    "    self.conv1 = nn.Conv1d(4, 16, kernel_size=8, stride=1, padding=\"same\")\n",
    "    self.conv2 = nn.Conv1d(16, 32, kernel_size=8, stride=1, padding=\"same\")\n",
    "    self.conv3 = nn.Conv1d(32, 64, kernel_size=8, stride=1, padding=\"same\")\n",
    "    self.fc1 = nn.Linear(768, 32) # 3 max poolings so 64 / 8\n",
    "    self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.flat = nn.Flatten()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = self.pool(x)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = self.pool(x)\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.flat(x)\n",
    "    x = self.fc1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.sigmoid(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVbFqM4laclu"
   },
   "source": [
    "# Ex 7 Test the model\n",
    "\n",
    "\n",
    "1. Create an instance of the DeepDNA class named **net**.\n",
    "\n",
    "2. Print out the variable **net** to see detailed information about the model.\n",
    "\n",
    "3. Pass **dna_seg** below to **net** in order to  test if your model **net** works well.\n",
    "\n",
    "4. What is the size of the output ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ladQCir7btvX",
    "outputId": "557662ec-4e25-47aa-dd92-ee56348aeab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output of the model  tensor([[0.5245],\n",
      "        [0.5298],\n",
      "        [0.5307],\n",
      "        [0.5316],\n",
      "        [0.5294]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#data = next(iter(train_loader))\n",
    "dna_seg = data['DNA']\n",
    "\n",
    "##### TO DO #######\n",
    "\n",
    "net = DeepDNA(101)\n",
    "out = net(dna_seg)\n",
    "\n",
    "print(\"the output of the model \", out[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFzp7DbtlTwG",
    "outputId": "24e73025-a469-4042-b77f-d3d19ba0e2ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepDNA(\n",
      "  (conv1): Conv1d(4, 16, kernel_size=(8,), stride=(1,), padding=same)\n",
      "  (conv2): Conv1d(16, 32, kernel_size=(8,), stride=(1,), padding=same)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(8,), stride=(1,), padding=same)\n",
      "  (fc1): Linear(in_features=768, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yypmG11hr_so"
   },
   "source": [
    "# Ex 8: Define loss function and optimizer\n",
    "\n",
    "\n",
    "1. Define an SGD optimizer for the model. You need to choose the learning rate for your model.\n",
    "\n",
    "2. Define a Binary Cross Entropy (BCE) Loss  function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "uZwQKSrxh1A_"
   },
   "outputs": [],
   "source": [
    "## TODO ###\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rEjVjhetI6A"
   },
   "source": [
    "# Ex 9: Training your model\n",
    "\n",
    "The following function allows to train the model for one epoch. This function returns total loss per epoch.\n",
    "Implement the training pass for this function.\n",
    "\n",
    "\n",
    "\n",
    "The general process with PyTorch for one learning step consits of several steps:\n",
    "\n",
    "1. Make a forward pass through the network\n",
    "2. Use the network output to calculate the loss\n",
    "3. Perform a backward pass through the network with loss.backward() to calculate the gradients\n",
    "4. Take a step with the optimizer to update the weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "oAil9eU9iNCf"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()\n",
    "    loss_epoch = 0\n",
    "    for batch_data in train_loader:\n",
    "        dna = batch_data['DNA']\n",
    "        labels = batch_data['Class']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(dna)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz0VV2Oenftq"
   },
   "source": [
    "# Ex 11: Accuracy Calculation\n",
    "\n",
    "Write a function named **compute_num_correct_pred(y_prob, y_label)** that allows to compute the number of correct predictions. **y_prob** and **y_label** should be pytorch tensors.\n",
    "\n",
    "For example,\n",
    "y_prob = [[0.3],[0.4], [0.8], [0.7]].\n",
    "\n",
    "y = [[0], [1], [1], [0]].\n",
    "\n",
    "This function should return 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "JZ3cRawWpFe7"
   },
   "outputs": [],
   "source": [
    "### TODO ####\n",
    "def compute_num_correct_pred(y_prob, y_label):\n",
    "    y_pred = y_prob > 0.5\n",
    "    correct = y_pred.eq(y_label).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N_g4DA-vex8"
   },
   "source": [
    "\n",
    "The function below allows to calculate the accuracy of the model on dataset loader. Execute this function to see if you implemented the compute_num_correct_pred function correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "FY9V2-NCvv25"
   },
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "  net.eval()\n",
    "\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      dna = data['DNA']\n",
    "      y = data['Class']\n",
    "\n",
    "      out = net(dna)\n",
    "      correct += compute_num_correct_pred(out, y)\n",
    "\n",
    "  return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0vTKeeQ3wGb"
   },
   "source": [
    "# Ex 12: Training the model\n",
    "\n",
    "The code below allows to train your model on 10 epoches. If all work well, you should see the training loss drop with each epoch.\n",
    "\n",
    "train the model for 2000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-vkWpKLixGE",
    "outputId": "7f41715a-8840-4529-cb4c-c6f76d84bf5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train_Loss: 88.21358498930931, Test Acc: 0.75\n",
      "Epoch 1, Train_Loss: 83.09558141231537, Test Acc: 0.75\n",
      "Epoch 2, Train_Loss: 82.9089840054512, Test Acc: 0.75\n",
      "Epoch 3, Train_Loss: 82.72694957256317, Test Acc: 0.75\n",
      "Epoch 4, Train_Loss: 83.0775475203991, Test Acc: 0.75\n",
      "Epoch 5, Train_Loss: 82.63341316580772, Test Acc: 0.75\n",
      "Epoch 6, Train_Loss: 82.90729480981827, Test Acc: 0.75\n",
      "Epoch 7, Train_Loss: 82.90007662773132, Test Acc: 0.75\n",
      "Epoch 8, Train_Loss: 82.7200208902359, Test Acc: 0.75\n",
      "Epoch 9, Train_Loss: 82.6635454595089, Test Acc: 0.75\n",
      "Epoch 10, Train_Loss: 82.21003538370132, Test Acc: 0.75\n",
      "Epoch 11, Train_Loss: 82.42908334732056, Test Acc: 0.75\n",
      "Epoch 12, Train_Loss: 82.21816313266754, Test Acc: 0.75\n",
      "Epoch 13, Train_Loss: 82.59602236747742, Test Acc: 0.75\n",
      "Epoch 14, Train_Loss: 82.47535094618797, Test Acc: 0.75\n",
      "Epoch 15, Train_Loss: 82.41025239229202, Test Acc: 0.75\n",
      "Epoch 16, Train_Loss: 82.53464484214783, Test Acc: 0.75\n",
      "Epoch 17, Train_Loss: 82.39589330554008, Test Acc: 0.75\n",
      "Epoch 18, Train_Loss: 82.21480911970139, Test Acc: 0.75\n",
      "Epoch 19, Train_Loss: 82.41098853945732, Test Acc: 0.75\n",
      "Epoch 20, Train_Loss: 82.24386328458786, Test Acc: 0.75\n",
      "Epoch 21, Train_Loss: 82.35790744423866, Test Acc: 0.75\n",
      "Epoch 22, Train_Loss: 82.0443423986435, Test Acc: 0.75\n",
      "Epoch 23, Train_Loss: 82.14968773722649, Test Acc: 0.75\n",
      "Epoch 24, Train_Loss: 82.35616335272789, Test Acc: 0.75\n",
      "Epoch 25, Train_Loss: 82.25879383087158, Test Acc: 0.75\n",
      "Epoch 26, Train_Loss: 81.9319899380207, Test Acc: 0.75\n",
      "Epoch 27, Train_Loss: 81.92973491549492, Test Acc: 0.75\n",
      "Epoch 28, Train_Loss: 81.7795215845108, Test Acc: 0.75\n",
      "Epoch 29, Train_Loss: 82.12799680233002, Test Acc: 0.75\n",
      "Epoch 30, Train_Loss: 81.90604108572006, Test Acc: 0.75\n",
      "Epoch 31, Train_Loss: 82.16666606068611, Test Acc: 0.75\n",
      "Epoch 32, Train_Loss: 81.99988579750061, Test Acc: 0.75\n",
      "Epoch 33, Train_Loss: 82.01467791199684, Test Acc: 0.75\n",
      "Epoch 34, Train_Loss: 81.89274850487709, Test Acc: 0.75\n",
      "Epoch 35, Train_Loss: 81.86392939090729, Test Acc: 0.75\n",
      "Epoch 36, Train_Loss: 82.00415107607841, Test Acc: 0.75\n",
      "Epoch 37, Train_Loss: 81.94771885871887, Test Acc: 0.75\n",
      "Epoch 38, Train_Loss: 81.8409877717495, Test Acc: 0.75\n",
      "Epoch 39, Train_Loss: 81.64816707372665, Test Acc: 0.75\n",
      "Epoch 40, Train_Loss: 81.63018274307251, Test Acc: 0.75\n",
      "Epoch 41, Train_Loss: 81.73787897825241, Test Acc: 0.75\n",
      "Epoch 42, Train_Loss: 81.72985762357712, Test Acc: 0.75\n",
      "Epoch 43, Train_Loss: 81.6766985654831, Test Acc: 0.75\n",
      "Epoch 44, Train_Loss: 81.39313870668411, Test Acc: 0.75\n",
      "Epoch 45, Train_Loss: 81.46340328454971, Test Acc: 0.75\n",
      "Epoch 46, Train_Loss: 81.15947237610817, Test Acc: 0.75\n",
      "Epoch 47, Train_Loss: 81.28315871953964, Test Acc: 0.75\n",
      "Epoch 48, Train_Loss: 81.31373429298401, Test Acc: 0.75\n",
      "Epoch 49, Train_Loss: 81.05393832921982, Test Acc: 0.75\n",
      "Epoch 50, Train_Loss: 81.05058807134628, Test Acc: 0.75\n",
      "Epoch 51, Train_Loss: 81.00779056549072, Test Acc: 0.75\n",
      "Epoch 52, Train_Loss: 80.94670301675797, Test Acc: 0.75\n",
      "Epoch 53, Train_Loss: 80.9626844227314, Test Acc: 0.75\n",
      "Epoch 54, Train_Loss: 80.79432570934296, Test Acc: 0.75\n",
      "Epoch 55, Train_Loss: 80.83620765805244, Test Acc: 0.75\n",
      "Epoch 56, Train_Loss: 80.65020200610161, Test Acc: 0.75\n",
      "Epoch 57, Train_Loss: 80.55632689595222, Test Acc: 0.75\n",
      "Epoch 58, Train_Loss: 80.27286529541016, Test Acc: 0.75\n",
      "Epoch 59, Train_Loss: 80.5040212571621, Test Acc: 0.75\n",
      "Epoch 60, Train_Loss: 80.37343034148216, Test Acc: 0.75\n",
      "Epoch 61, Train_Loss: 80.1124994456768, Test Acc: 0.75\n",
      "Epoch 62, Train_Loss: 79.99168661236763, Test Acc: 0.75\n",
      "Epoch 63, Train_Loss: 79.89820739626884, Test Acc: 0.75\n",
      "Epoch 64, Train_Loss: 79.73108932375908, Test Acc: 0.75\n",
      "Epoch 65, Train_Loss: 79.62519258260727, Test Acc: 0.75\n",
      "Epoch 66, Train_Loss: 79.4006353020668, Test Acc: 0.75\n",
      "Epoch 67, Train_Loss: 79.42185366153717, Test Acc: 0.75\n",
      "Epoch 68, Train_Loss: 79.43102851510048, Test Acc: 0.75\n",
      "Epoch 69, Train_Loss: 79.05803340673447, Test Acc: 0.75\n",
      "Epoch 70, Train_Loss: 79.04520499706268, Test Acc: 0.75\n",
      "Epoch 71, Train_Loss: 78.7723291516304, Test Acc: 0.75\n",
      "Epoch 72, Train_Loss: 78.35493445396423, Test Acc: 0.75\n",
      "Epoch 73, Train_Loss: 78.50240528583527, Test Acc: 0.75\n",
      "Epoch 74, Train_Loss: 78.41202476620674, Test Acc: 0.75\n",
      "Epoch 75, Train_Loss: 78.58917751908302, Test Acc: 0.75\n",
      "Epoch 76, Train_Loss: 78.660564661026, Test Acc: 0.75\n",
      "Epoch 77, Train_Loss: 78.04987534880638, Test Acc: 0.75\n",
      "Epoch 78, Train_Loss: 78.06715840101242, Test Acc: 0.75\n",
      "Epoch 79, Train_Loss: 78.05239841341972, Test Acc: 0.75\n",
      "Epoch 80, Train_Loss: 77.81002128124237, Test Acc: 0.7517123287671232\n",
      "Epoch 81, Train_Loss: 77.73009461164474, Test Acc: 0.7517123287671232\n",
      "Epoch 82, Train_Loss: 77.01950463652611, Test Acc: 0.75\n",
      "Epoch 83, Train_Loss: 77.03593236207962, Test Acc: 0.7517123287671232\n",
      "Epoch 84, Train_Loss: 77.4793663918972, Test Acc: 0.7517123287671232\n",
      "Epoch 85, Train_Loss: 77.54589226841927, Test Acc: 0.7517123287671232\n",
      "Epoch 86, Train_Loss: 77.24884206056595, Test Acc: 0.75\n",
      "Epoch 87, Train_Loss: 77.08956253528595, Test Acc: 0.7517123287671232\n",
      "Epoch 88, Train_Loss: 77.059127420187, Test Acc: 0.7551369863013698\n",
      "Epoch 89, Train_Loss: 76.7625981271267, Test Acc: 0.7534246575342466\n",
      "Epoch 90, Train_Loss: 76.52517080307007, Test Acc: 0.7517123287671232\n",
      "Epoch 91, Train_Loss: 76.6300781071186, Test Acc: 0.7534246575342466\n",
      "Epoch 92, Train_Loss: 76.41124767065048, Test Acc: 0.7534246575342466\n",
      "Epoch 93, Train_Loss: 75.77463552355766, Test Acc: 0.7517123287671232\n",
      "Epoch 94, Train_Loss: 75.75713902711868, Test Acc: 0.7517123287671232\n",
      "Epoch 95, Train_Loss: 76.34856015443802, Test Acc: 0.7517123287671232\n",
      "Epoch 96, Train_Loss: 75.40054368972778, Test Acc: 0.7517123287671232\n",
      "Epoch 97, Train_Loss: 75.58734259009361, Test Acc: 0.7517123287671232\n",
      "Epoch 98, Train_Loss: 75.8856572508812, Test Acc: 0.7551369863013698\n",
      "Epoch 99, Train_Loss: 75.66800847649574, Test Acc: 0.75\n",
      "Epoch 100, Train_Loss: 74.94019469618797, Test Acc: 0.7602739726027398\n",
      "Epoch 101, Train_Loss: 75.11039590835571, Test Acc: 0.7585616438356164\n",
      "Epoch 102, Train_Loss: 74.20326933264732, Test Acc: 0.75\n",
      "Epoch 103, Train_Loss: 74.77618178725243, Test Acc: 0.75\n",
      "Epoch 104, Train_Loss: 74.45729525387287, Test Acc: 0.7602739726027398\n",
      "Epoch 105, Train_Loss: 73.84786605834961, Test Acc: 0.7517123287671232\n",
      "Epoch 106, Train_Loss: 74.41868537664413, Test Acc: 0.7534246575342466\n",
      "Epoch 107, Train_Loss: 74.14629182219505, Test Acc: 0.7568493150684932\n",
      "Epoch 108, Train_Loss: 73.46642751991749, Test Acc: 0.7551369863013698\n",
      "Epoch 109, Train_Loss: 73.60134315490723, Test Acc: 0.7482876712328768\n",
      "Epoch 110, Train_Loss: 73.63337518274784, Test Acc: 0.7517123287671232\n",
      "Epoch 111, Train_Loss: 73.11926385760307, Test Acc: 0.7534246575342466\n",
      "Epoch 112, Train_Loss: 72.98835933208466, Test Acc: 0.7551369863013698\n",
      "Epoch 113, Train_Loss: 72.70272728800774, Test Acc: 0.75\n",
      "Epoch 114, Train_Loss: 71.52251267433167, Test Acc: 0.75\n",
      "Epoch 115, Train_Loss: 72.69728261232376, Test Acc: 0.7517123287671232\n",
      "Epoch 116, Train_Loss: 71.93950688838959, Test Acc: 0.7551369863013698\n",
      "Epoch 117, Train_Loss: 71.41291603446007, Test Acc: 0.7551369863013698\n",
      "Epoch 118, Train_Loss: 71.56133270263672, Test Acc: 0.7602739726027398\n",
      "Epoch 119, Train_Loss: 70.628871768713, Test Acc: 0.7054794520547946\n",
      "Epoch 120, Train_Loss: 70.97029680013657, Test Acc: 0.75\n",
      "Epoch 121, Train_Loss: 70.8488579839468, Test Acc: 0.7534246575342466\n",
      "Epoch 122, Train_Loss: 70.3554274737835, Test Acc: 0.7037671232876712\n",
      "Epoch 123, Train_Loss: 68.49337139725685, Test Acc: 0.7585616438356164\n",
      "Epoch 124, Train_Loss: 68.92136040329933, Test Acc: 0.7517123287671232\n",
      "Epoch 125, Train_Loss: 69.24038726091385, Test Acc: 0.75\n",
      "Epoch 126, Train_Loss: 69.59091025590897, Test Acc: 0.75\n",
      "Epoch 127, Train_Loss: 67.85122925043106, Test Acc: 0.7482876712328768\n",
      "Epoch 128, Train_Loss: 67.67757287621498, Test Acc: 0.7585616438356164\n",
      "Epoch 129, Train_Loss: 66.98020765185356, Test Acc: 0.7003424657534246\n",
      "Epoch 130, Train_Loss: 66.9044632166624, Test Acc: 0.726027397260274\n",
      "Epoch 131, Train_Loss: 66.44157999753952, Test Acc: 0.7568493150684932\n",
      "Epoch 132, Train_Loss: 66.13851821422577, Test Acc: 0.696917808219178\n",
      "Epoch 133, Train_Loss: 64.68414753675461, Test Acc: 0.7517123287671232\n",
      "Epoch 134, Train_Loss: 65.14360570907593, Test Acc: 0.7534246575342466\n",
      "Epoch 135, Train_Loss: 63.32592470943928, Test Acc: 0.7517123287671232\n",
      "Epoch 136, Train_Loss: 63.81180642545223, Test Acc: 0.75\n",
      "Epoch 137, Train_Loss: 63.20953360199928, Test Acc: 0.75\n",
      "Epoch 138, Train_Loss: 61.724377542734146, Test Acc: 0.7431506849315068\n",
      "Epoch 139, Train_Loss: 61.130049124360085, Test Acc: 0.7482876712328768\n",
      "Epoch 140, Train_Loss: 60.90760941803455, Test Acc: 0.6746575342465754\n",
      "Epoch 141, Train_Loss: 58.81252293288708, Test Acc: 0.7448630136986302\n",
      "Epoch 142, Train_Loss: 59.624594032764435, Test Acc: 0.7482876712328768\n",
      "Epoch 143, Train_Loss: 56.47367708384991, Test Acc: 0.75\n",
      "Epoch 144, Train_Loss: 57.14747527241707, Test Acc: 0.696917808219178\n",
      "Epoch 145, Train_Loss: 55.22092171013355, Test Acc: 0.7328767123287672\n",
      "Epoch 146, Train_Loss: 55.438920378685, Test Acc: 0.7363013698630136\n",
      "Epoch 147, Train_Loss: 53.322906360030174, Test Acc: 0.7448630136986302\n",
      "Epoch 148, Train_Loss: 51.744743257761, Test Acc: 0.702054794520548\n",
      "Epoch 149, Train_Loss: 49.97782336175442, Test Acc: 0.7123287671232876\n",
      "Epoch 150, Train_Loss: 48.939682334661484, Test Acc: 0.7465753424657534\n",
      "Epoch 151, Train_Loss: 48.263401597738266, Test Acc: 0.7277397260273972\n",
      "Epoch 152, Train_Loss: 45.74686777591705, Test Acc: 0.7174657534246576\n",
      "Epoch 153, Train_Loss: 44.79475709795952, Test Acc: 0.666095890410959\n",
      "Epoch 154, Train_Loss: 43.43804579228163, Test Acc: 0.726027397260274\n",
      "Epoch 155, Train_Loss: 39.81371892988682, Test Acc: 0.6746575342465754\n",
      "Epoch 156, Train_Loss: 40.15564661473036, Test Acc: 0.7208904109589042\n",
      "Epoch 157, Train_Loss: 38.01723165810108, Test Acc: 0.6301369863013698\n",
      "Epoch 158, Train_Loss: 35.409085258841515, Test Acc: 0.696917808219178\n",
      "Epoch 159, Train_Loss: 33.968251682817936, Test Acc: 0.708904109589041\n",
      "Epoch 160, Train_Loss: 34.418047562241554, Test Acc: 0.696917808219178\n",
      "Epoch 161, Train_Loss: 30.52549798041582, Test Acc: 0.7448630136986302\n",
      "Epoch 162, Train_Loss: 28.615887194871902, Test Acc: 0.6934931506849316\n",
      "Epoch 163, Train_Loss: 28.088096283376217, Test Acc: 0.7345890410958904\n",
      "Epoch 164, Train_Loss: 25.046341188251972, Test Acc: 0.7106164383561644\n",
      "Epoch 165, Train_Loss: 24.147897209972143, Test Acc: 0.7226027397260274\n",
      "Epoch 166, Train_Loss: 20.793539902195334, Test Acc: 0.672945205479452\n",
      "Epoch 167, Train_Loss: 19.9934598878026, Test Acc: 0.678082191780822\n",
      "Epoch 168, Train_Loss: 18.972015544772148, Test Acc: 0.702054794520548\n",
      "Epoch 169, Train_Loss: 16.692347168922424, Test Acc: 0.6695205479452054\n",
      "Epoch 170, Train_Loss: 14.796042181551456, Test Acc: 0.7123287671232876\n",
      "Epoch 171, Train_Loss: 15.590547006577253, Test Acc: 0.708904109589041\n",
      "Epoch 172, Train_Loss: 11.68856710754335, Test Acc: 0.708904109589041\n",
      "Epoch 173, Train_Loss: 11.180381461977959, Test Acc: 0.6832191780821918\n",
      "Epoch 174, Train_Loss: 9.653774376958609, Test Acc: 0.6917808219178082\n",
      "Epoch 175, Train_Loss: 10.504453621804714, Test Acc: 0.6763698630136986\n",
      "Epoch 176, Train_Loss: 8.082125156186521, Test Acc: 0.6712328767123288\n",
      "Epoch 177, Train_Loss: 6.473476346582174, Test Acc: 0.6643835616438356\n",
      "Epoch 178, Train_Loss: 5.4985610181465745, Test Acc: 0.6592465753424658\n",
      "Epoch 179, Train_Loss: 4.915577810257673, Test Acc: 0.6558219178082192\n",
      "Epoch 180, Train_Loss: 4.439234578981996, Test Acc: 0.6986301369863014\n",
      "Epoch 181, Train_Loss: 4.014127808623016, Test Acc: 0.6866438356164384\n",
      "Epoch 182, Train_Loss: 3.402053349185735, Test Acc: 0.6746575342465754\n",
      "Epoch 183, Train_Loss: 3.2803688226267695, Test Acc: 0.6506849315068494\n",
      "Epoch 184, Train_Loss: 3.1556555768474936, Test Acc: 0.6506849315068494\n",
      "Epoch 185, Train_Loss: 2.723103140015155, Test Acc: 0.6575342465753424\n",
      "Epoch 186, Train_Loss: 2.556593195768073, Test Acc: 0.6541095890410958\n",
      "Epoch 187, Train_Loss: 2.471661443123594, Test Acc: 0.6643835616438356\n",
      "Epoch 188, Train_Loss: 2.2512931178789586, Test Acc: 0.6558219178082192\n",
      "Epoch 189, Train_Loss: 7.647319298237562, Test Acc: 0.6541095890410958\n",
      "Epoch 190, Train_Loss: 2.59036075649783, Test Acc: 0.6592465753424658\n",
      "Epoch 191, Train_Loss: 2.048696438781917, Test Acc: 0.660958904109589\n",
      "Epoch 192, Train_Loss: 1.754729806445539, Test Acc: 0.6575342465753424\n",
      "Epoch 193, Train_Loss: 1.7042482547694817, Test Acc: 0.6678082191780822\n",
      "Epoch 194, Train_Loss: 1.642226030351594, Test Acc: 0.648972602739726\n",
      "Epoch 195, Train_Loss: 1.4058549490291625, Test Acc: 0.678082191780822\n",
      "Epoch 196, Train_Loss: 1.4187106026802212, Test Acc: 0.6523972602739726\n",
      "Epoch 197, Train_Loss: 1.2606679222080857, Test Acc: 0.6678082191780822\n",
      "Epoch 198, Train_Loss: 1.2597413852345198, Test Acc: 0.6592465753424658\n",
      "Epoch 199, Train_Loss: 1.2022339720278978, Test Acc: 0.666095890410959\n",
      "Epoch 200, Train_Loss: 1.1549004847183824, Test Acc: 0.6643835616438356\n",
      "Epoch 201, Train_Loss: 1.0554645827505738, Test Acc: 0.6592465753424658\n",
      "Epoch 202, Train_Loss: 0.971377651905641, Test Acc: 0.6541095890410958\n",
      "Epoch 203, Train_Loss: 0.9723495768848807, Test Acc: 0.6558219178082192\n",
      "Epoch 204, Train_Loss: 0.9907063258579001, Test Acc: 0.6575342465753424\n",
      "Epoch 205, Train_Loss: 0.9064616714604199, Test Acc: 0.6558219178082192\n",
      "Epoch 206, Train_Loss: 0.8732486664084718, Test Acc: 0.6626712328767124\n",
      "Epoch 207, Train_Loss: 0.8350657683331519, Test Acc: 0.6643835616438356\n",
      "Epoch 208, Train_Loss: 0.8214304083958268, Test Acc: 0.648972602739726\n",
      "Epoch 209, Train_Loss: 0.7631368373986334, Test Acc: 0.6712328767123288\n",
      "Epoch 210, Train_Loss: 0.7968392771435902, Test Acc: 0.6678082191780822\n",
      "Epoch 211, Train_Loss: 0.771675783675164, Test Acc: 0.6678082191780822\n",
      "Epoch 212, Train_Loss: 0.7148465785430744, Test Acc: 0.660958904109589\n",
      "Epoch 213, Train_Loss: 0.6958652042085305, Test Acc: 0.660958904109589\n",
      "Epoch 214, Train_Loss: 0.6635588596691377, Test Acc: 0.6746575342465754\n",
      "Epoch 215, Train_Loss: 0.6837582730222493, Test Acc: 0.6472602739726028\n",
      "Epoch 216, Train_Loss: 0.6505920790368691, Test Acc: 0.6558219178082192\n",
      "Epoch 217, Train_Loss: 0.6033336404361762, Test Acc: 0.660958904109589\n",
      "Epoch 218, Train_Loss: 0.5841359584592283, Test Acc: 0.6592465753424658\n",
      "Epoch 219, Train_Loss: 0.5581279922043905, Test Acc: 0.6523972602739726\n",
      "Epoch 220, Train_Loss: 0.5670071338536218, Test Acc: 0.6541095890410958\n",
      "Epoch 221, Train_Loss: 0.5352864583255723, Test Acc: 0.6678082191780822\n",
      "Epoch 222, Train_Loss: 0.5177767149289139, Test Acc: 0.666095890410959\n",
      "Epoch 223, Train_Loss: 0.5472779271076433, Test Acc: 0.6438356164383562\n",
      "Epoch 224, Train_Loss: 0.5109106216114014, Test Acc: 0.6643835616438356\n",
      "Epoch 225, Train_Loss: 0.483397223346401, Test Acc: 0.6558219178082192\n",
      "Epoch 226, Train_Loss: 0.46149418415734544, Test Acc: 0.660958904109589\n",
      "Epoch 227, Train_Loss: 0.4427302847034298, Test Acc: 0.6695205479452054\n",
      "Epoch 228, Train_Loss: 0.4660986117669381, Test Acc: 0.6558219178082192\n",
      "Epoch 229, Train_Loss: 0.45628042420139536, Test Acc: 0.6746575342465754\n",
      "Epoch 230, Train_Loss: 0.44368916552048177, Test Acc: 0.6626712328767124\n",
      "Epoch 231, Train_Loss: 0.4401057661161758, Test Acc: 0.666095890410959\n",
      "Epoch 232, Train_Loss: 0.39841748942853883, Test Acc: 0.6712328767123288\n",
      "Epoch 233, Train_Loss: 0.402502728975378, Test Acc: 0.6575342465753424\n",
      "Epoch 234, Train_Loss: 0.4073731194366701, Test Acc: 0.6643835616438356\n",
      "Epoch 235, Train_Loss: 0.4161412663233932, Test Acc: 0.660958904109589\n",
      "Epoch 236, Train_Loss: 0.41347997984848917, Test Acc: 0.666095890410959\n",
      "Epoch 237, Train_Loss: 0.401812476455234, Test Acc: 0.6558219178082192\n",
      "Epoch 238, Train_Loss: 0.3612703681574203, Test Acc: 0.6575342465753424\n",
      "Epoch 239, Train_Loss: 0.39704522414831445, Test Acc: 0.672945205479452\n",
      "Epoch 240, Train_Loss: 0.36242726306954864, Test Acc: 0.6592465753424658\n",
      "Epoch 241, Train_Loss: 0.33999452090938576, Test Acc: 0.6695205479452054\n",
      "Epoch 242, Train_Loss: 0.32797341130208224, Test Acc: 0.6678082191780822\n",
      "Epoch 243, Train_Loss: 0.3490341837750748, Test Acc: 0.6643835616438356\n",
      "Epoch 244, Train_Loss: 0.3336110209056642, Test Acc: 0.660958904109589\n",
      "Epoch 245, Train_Loss: 0.30403477110667154, Test Acc: 0.6626712328767124\n",
      "Epoch 246, Train_Loss: 0.3149309420841746, Test Acc: 0.672945205479452\n",
      "Epoch 247, Train_Loss: 0.3006779904535506, Test Acc: 0.6558219178082192\n",
      "Epoch 248, Train_Loss: 0.30640811950434, Test Acc: 0.6643835616438356\n",
      "Epoch 249, Train_Loss: 0.30043960572220385, Test Acc: 0.6541095890410958\n",
      "Epoch 250, Train_Loss: 0.2818408426246606, Test Acc: 0.6643835616438356\n",
      "Epoch 251, Train_Loss: 0.3104113256849814, Test Acc: 0.6643835616438356\n",
      "Epoch 252, Train_Loss: 0.28736765607027337, Test Acc: 0.6678082191780822\n",
      "Epoch 253, Train_Loss: 0.2700896603782894, Test Acc: 0.6626712328767124\n",
      "Epoch 254, Train_Loss: 0.26399688224773854, Test Acc: 0.6695205479452054\n",
      "Epoch 255, Train_Loss: 0.2608792668906972, Test Acc: 0.666095890410959\n",
      "Epoch 256, Train_Loss: 0.25289361164323054, Test Acc: 0.6472602739726028\n",
      "Epoch 257, Train_Loss: 0.25039656888111494, Test Acc: 0.666095890410959\n",
      "Epoch 258, Train_Loss: 0.24452018458396196, Test Acc: 0.6506849315068494\n",
      "Epoch 259, Train_Loss: 0.26988271245500073, Test Acc: 0.6506849315068494\n",
      "Epoch 260, Train_Loss: 0.23401479216408916, Test Acc: 0.6695205479452054\n",
      "Epoch 261, Train_Loss: 0.24675093381665647, Test Acc: 0.6763698630136986\n",
      "Epoch 262, Train_Loss: 0.26742981126881205, Test Acc: 0.6746575342465754\n",
      "Epoch 263, Train_Loss: 0.2378982501977589, Test Acc: 0.6506849315068494\n",
      "Epoch 264, Train_Loss: 0.2674304852844216, Test Acc: 0.6626712328767124\n",
      "Epoch 265, Train_Loss: 0.23757729100179859, Test Acc: 0.672945205479452\n",
      "Epoch 266, Train_Loss: 0.2364055901998654, Test Acc: 0.6695205479452054\n",
      "Epoch 267, Train_Loss: 0.23500189796322957, Test Acc: 0.6746575342465754\n",
      "Epoch 268, Train_Loss: 0.24583983371849172, Test Acc: 0.672945205479452\n",
      "Epoch 269, Train_Loss: 0.22315052495105192, Test Acc: 0.6695205479452054\n",
      "Epoch 270, Train_Loss: 0.21353368897689506, Test Acc: 0.6678082191780822\n",
      "Epoch 271, Train_Loss: 0.20791275372903328, Test Acc: 0.666095890410959\n",
      "Epoch 272, Train_Loss: 0.21689950389554724, Test Acc: 0.6678082191780822\n",
      "Epoch 273, Train_Loss: 0.20687383132462855, Test Acc: 0.6695205479452054\n",
      "Epoch 274, Train_Loss: 0.217167619310203, Test Acc: 0.6712328767123288\n",
      "Epoch 275, Train_Loss: 0.2161492577579338, Test Acc: 0.6712328767123288\n",
      "Epoch 276, Train_Loss: 0.19593578309286386, Test Acc: 0.6712328767123288\n",
      "Epoch 277, Train_Loss: 0.21497332697617821, Test Acc: 0.666095890410959\n",
      "Epoch 278, Train_Loss: 0.23235031109652482, Test Acc: 0.666095890410959\n",
      "Epoch 279, Train_Loss: 0.1979106985963881, Test Acc: 0.678082191780822\n",
      "Epoch 280, Train_Loss: 0.18397818418452516, Test Acc: 0.672945205479452\n",
      "Epoch 281, Train_Loss: 0.20615939027629793, Test Acc: 0.6575342465753424\n",
      "Epoch 282, Train_Loss: 0.20213613582018297, Test Acc: 0.6763698630136986\n",
      "Epoch 283, Train_Loss: 0.1841348838643171, Test Acc: 0.672945205479452\n",
      "Epoch 284, Train_Loss: 0.2007536379605881, Test Acc: 0.6712328767123288\n",
      "Epoch 285, Train_Loss: 0.18358784382871818, Test Acc: 0.6558219178082192\n",
      "Epoch 286, Train_Loss: 0.18626811137073673, Test Acc: 0.660958904109589\n",
      "Epoch 287, Train_Loss: 0.1641199703735765, Test Acc: 0.666095890410959\n",
      "Epoch 288, Train_Loss: 0.17841362630133517, Test Acc: 0.660958904109589\n",
      "Epoch 289, Train_Loss: 0.18978458366473205, Test Acc: 0.6712328767123288\n",
      "Epoch 290, Train_Loss: 0.17084366403287277, Test Acc: 0.6712328767123288\n",
      "Epoch 291, Train_Loss: 0.18023102295410354, Test Acc: 0.660958904109589\n",
      "Epoch 292, Train_Loss: 0.17727027647197247, Test Acc: 0.6712328767123288\n",
      "Epoch 293, Train_Loss: 0.15599155296513345, Test Acc: 0.6712328767123288\n",
      "Epoch 294, Train_Loss: 0.16863543526415015, Test Acc: 0.6695205479452054\n",
      "Epoch 295, Train_Loss: 0.17424302545259707, Test Acc: 0.6678082191780822\n",
      "Epoch 296, Train_Loss: 0.15710258932085708, Test Acc: 0.6523972602739726\n",
      "Epoch 297, Train_Loss: 0.15751023625489324, Test Acc: 0.6626712328767124\n",
      "Epoch 298, Train_Loss: 0.16121640492929146, Test Acc: 0.666095890410959\n",
      "Epoch 299, Train_Loss: 0.1773352458840236, Test Acc: 0.672945205479452\n",
      "Epoch 300, Train_Loss: 0.16744739812565967, Test Acc: 0.660958904109589\n",
      "Epoch 301, Train_Loss: 0.16520762789878063, Test Acc: 0.6678082191780822\n",
      "Epoch 302, Train_Loss: 0.14864199173462112, Test Acc: 0.6695205479452054\n",
      "Epoch 303, Train_Loss: 0.1474981829233002, Test Acc: 0.6695205479452054\n",
      "Epoch 304, Train_Loss: 0.1570752366023953, Test Acc: 0.672945205479452\n",
      "Epoch 305, Train_Loss: 0.166788135538809, Test Acc: 0.6695205479452054\n",
      "Epoch 306, Train_Loss: 0.14455997374898288, Test Acc: 0.6695205479452054\n",
      "Epoch 307, Train_Loss: 0.1693665450293338, Test Acc: 0.6695205479452054\n",
      "Epoch 308, Train_Loss: 0.13287059882713947, Test Acc: 0.6643835616438356\n",
      "Epoch 309, Train_Loss: 0.1453842616028851, Test Acc: 0.6695205479452054\n",
      "Epoch 310, Train_Loss: 0.14720438796211965, Test Acc: 0.6712328767123288\n",
      "Epoch 311, Train_Loss: 0.1388282324478496, Test Acc: 0.6643835616438356\n",
      "Epoch 312, Train_Loss: 0.13637205975828692, Test Acc: 0.6712328767123288\n",
      "Epoch 313, Train_Loss: 0.14469382433162536, Test Acc: 0.6541095890410958\n",
      "Epoch 314, Train_Loss: 0.1411501829716144, Test Acc: 0.6695205479452054\n",
      "Epoch 315, Train_Loss: 0.12864725792314857, Test Acc: 0.6523972602739726\n",
      "Epoch 316, Train_Loss: 0.13631862938927952, Test Acc: 0.6643835616438356\n",
      "Epoch 317, Train_Loss: 0.1354257748753298, Test Acc: 0.6678082191780822\n",
      "Epoch 318, Train_Loss: 0.12995108601171523, Test Acc: 0.6678082191780822\n",
      "Epoch 319, Train_Loss: 0.13210877793608233, Test Acc: 0.666095890410959\n",
      "Epoch 320, Train_Loss: 0.13553305946697947, Test Acc: 0.6643835616438356\n",
      "Epoch 321, Train_Loss: 0.14469090228521964, Test Acc: 0.6626712328767124\n",
      "Epoch 322, Train_Loss: 0.14233017832157202, Test Acc: 0.660958904109589\n",
      "Epoch 323, Train_Loss: 0.13380529753339943, Test Acc: 0.6712328767123288\n",
      "Epoch 324, Train_Loss: 0.12201301365712425, Test Acc: 0.6643835616438356\n",
      "Epoch 325, Train_Loss: 0.13023030871408992, Test Acc: 0.672945205479452\n",
      "Epoch 326, Train_Loss: 0.1287093546707183, Test Acc: 0.6712328767123288\n",
      "Epoch 327, Train_Loss: 0.1297648267209297, Test Acc: 0.6541095890410958\n",
      "Epoch 328, Train_Loss: 0.11963282110809814, Test Acc: 0.6678082191780822\n",
      "Epoch 329, Train_Loss: 0.11508723015140276, Test Acc: 0.6678082191780822\n",
      "Epoch 330, Train_Loss: 0.12492206745082512, Test Acc: 0.6695205479452054\n",
      "Epoch 331, Train_Loss: 0.1130959976289887, Test Acc: 0.6592465753424658\n",
      "Epoch 332, Train_Loss: 0.11237673104187706, Test Acc: 0.6695205479452054\n",
      "Epoch 333, Train_Loss: 0.12539074815867934, Test Acc: 0.6678082191780822\n",
      "Epoch 334, Train_Loss: 0.11766353274288122, Test Acc: 0.6523972602739726\n",
      "Epoch 335, Train_Loss: 0.12761146607954288, Test Acc: 0.6626712328767124\n",
      "Epoch 336, Train_Loss: 0.11570939228113275, Test Acc: 0.6678082191780822\n",
      "Epoch 337, Train_Loss: 0.1197730181447696, Test Acc: 0.660958904109589\n",
      "Epoch 338, Train_Loss: 0.11408434415352531, Test Acc: 0.6558219178082192\n",
      "Epoch 339, Train_Loss: 0.12482056453882251, Test Acc: 0.6695205479452054\n",
      "Epoch 340, Train_Loss: 0.11918819911079481, Test Acc: 0.6746575342465754\n",
      "Epoch 341, Train_Loss: 0.12123381612764206, Test Acc: 0.672945205479452\n",
      "Epoch 342, Train_Loss: 0.11077419010689482, Test Acc: 0.6643835616438356\n",
      "Epoch 343, Train_Loss: 0.10947284648136701, Test Acc: 0.660958904109589\n",
      "Epoch 344, Train_Loss: 0.10191874281008495, Test Acc: 0.6695205479452054\n",
      "Epoch 345, Train_Loss: 0.12266974190424662, Test Acc: 0.6541095890410958\n",
      "Epoch 346, Train_Loss: 0.11061351966054644, Test Acc: 0.6592465753424658\n",
      "Epoch 347, Train_Loss: 0.09983065490814624, Test Acc: 0.6541095890410958\n",
      "Epoch 348, Train_Loss: 0.10251958981098142, Test Acc: 0.672945205479452\n",
      "Epoch 349, Train_Loss: 0.11076728481566533, Test Acc: 0.660958904109589\n",
      "Epoch 350, Train_Loss: 0.11660337667854037, Test Acc: 0.6575342465753424\n",
      "Epoch 351, Train_Loss: 0.10124415844620671, Test Acc: 0.6592465753424658\n",
      "Epoch 352, Train_Loss: 0.10518437922291923, Test Acc: 0.6626712328767124\n",
      "Epoch 353, Train_Loss: 0.09803782534436323, Test Acc: 0.6695205479452054\n",
      "Epoch 354, Train_Loss: 0.1004385581400129, Test Acc: 0.6695205479452054\n",
      "Epoch 355, Train_Loss: 0.09514804866921622, Test Acc: 0.6626712328767124\n",
      "Epoch 356, Train_Loss: 0.1072454958994058, Test Acc: 0.6626712328767124\n",
      "Epoch 357, Train_Loss: 0.10138423238822725, Test Acc: 0.672945205479452\n",
      "Epoch 358, Train_Loss: 0.0937974207008665, Test Acc: 0.6643835616438356\n",
      "Epoch 359, Train_Loss: 0.1012582563853357, Test Acc: 0.6763698630136986\n",
      "Epoch 360, Train_Loss: 0.08224102678650524, Test Acc: 0.6626712328767124\n",
      "Epoch 361, Train_Loss: 0.09274108448880725, Test Acc: 0.666095890410959\n",
      "Epoch 362, Train_Loss: 0.08885264410128002, Test Acc: 0.6626712328767124\n",
      "Epoch 363, Train_Loss: 0.09641725068649976, Test Acc: 0.6626712328767124\n",
      "Epoch 364, Train_Loss: 0.10118132665229496, Test Acc: 0.6746575342465754\n",
      "Epoch 365, Train_Loss: 0.10434240456379484, Test Acc: 0.6558219178082192\n",
      "Epoch 366, Train_Loss: 0.08453838709829142, Test Acc: 0.672945205479452\n",
      "Epoch 367, Train_Loss: 0.09318094015907263, Test Acc: 0.6712328767123288\n",
      "Epoch 368, Train_Loss: 0.10126335181121249, Test Acc: 0.6643835616438356\n",
      "Epoch 369, Train_Loss: 0.10938345511385705, Test Acc: 0.6695205479452054\n",
      "Epoch 370, Train_Loss: 0.10852592578885378, Test Acc: 0.6575342465753424\n",
      "Epoch 371, Train_Loss: 0.09328513879154343, Test Acc: 0.6558219178082192\n",
      "Epoch 372, Train_Loss: 0.08578729325381573, Test Acc: 0.672945205479452\n",
      "Epoch 373, Train_Loss: 0.08183480887237238, Test Acc: 0.6643835616438356\n",
      "Epoch 374, Train_Loss: 0.07978139657643624, Test Acc: 0.6626712328767124\n",
      "Epoch 375, Train_Loss: 0.09651375891553471, Test Acc: 0.672945205479452\n",
      "Epoch 376, Train_Loss: 0.08495708451664541, Test Acc: 0.6712328767123288\n",
      "Epoch 377, Train_Loss: 0.08885123788786586, Test Acc: 0.6678082191780822\n",
      "Epoch 378, Train_Loss: 0.08852548155846307, Test Acc: 0.6575342465753424\n",
      "Epoch 379, Train_Loss: 0.09448432480712654, Test Acc: 0.672945205479452\n",
      "Epoch 380, Train_Loss: 0.09469648153753951, Test Acc: 0.6712328767123288\n",
      "Epoch 381, Train_Loss: 0.08449620748433517, Test Acc: 0.6712328767123288\n",
      "Epoch 382, Train_Loss: 0.08318741744733416, Test Acc: 0.6763698630136986\n",
      "Epoch 383, Train_Loss: 0.08247671016579261, Test Acc: 0.6678082191780822\n",
      "Epoch 384, Train_Loss: 0.08168491566175362, Test Acc: 0.666095890410959\n",
      "Epoch 385, Train_Loss: 0.0844761259832012, Test Acc: 0.672945205479452\n",
      "Epoch 386, Train_Loss: 0.07736880002630642, Test Acc: 0.6626712328767124\n",
      "Epoch 387, Train_Loss: 0.0792219506802212, Test Acc: 0.6695205479452054\n",
      "Epoch 388, Train_Loss: 0.08144953443843406, Test Acc: 0.6712328767123288\n",
      "Epoch 389, Train_Loss: 0.09598693141742842, Test Acc: 0.6558219178082192\n",
      "Epoch 390, Train_Loss: 0.07848340913915308, Test Acc: 0.6695205479452054\n",
      "Epoch 391, Train_Loss: 0.08996234755613841, Test Acc: 0.6695205479452054\n",
      "Epoch 392, Train_Loss: 0.0802661148372863, Test Acc: 0.6695205479452054\n",
      "Epoch 393, Train_Loss: 0.06752624255750561, Test Acc: 0.6695205479452054\n",
      "Epoch 394, Train_Loss: 0.07376775472948793, Test Acc: 0.6695205479452054\n",
      "Epoch 395, Train_Loss: 0.09000656977514154, Test Acc: 0.6626712328767124\n",
      "Epoch 396, Train_Loss: 0.08319380153625389, Test Acc: 0.6695205479452054\n",
      "Epoch 397, Train_Loss: 0.07096114823798416, Test Acc: 0.672945205479452\n",
      "Epoch 398, Train_Loss: 0.07582147057109978, Test Acc: 0.666095890410959\n",
      "Epoch 399, Train_Loss: 0.08015575152239762, Test Acc: 0.6695205479452054\n",
      "Epoch 400, Train_Loss: 0.07446218895347556, Test Acc: 0.660958904109589\n",
      "Epoch 401, Train_Loss: 0.0824835074308794, Test Acc: 0.6678082191780822\n",
      "Epoch 402, Train_Loss: 0.07233979828015435, Test Acc: 0.6643835616438356\n",
      "Epoch 403, Train_Loss: 0.07955509649036685, Test Acc: 0.6712328767123288\n",
      "Epoch 404, Train_Loss: 0.0757339590927586, Test Acc: 0.672945205479452\n",
      "Epoch 405, Train_Loss: 0.06928156037611188, Test Acc: 0.6643835616438356\n",
      "Epoch 406, Train_Loss: 0.09059114276533364, Test Acc: 0.6643835616438356\n",
      "Epoch 407, Train_Loss: 0.07743564252450597, Test Acc: 0.6643835616438356\n",
      "Epoch 408, Train_Loss: 0.07272826530970633, Test Acc: 0.666095890410959\n",
      "Epoch 409, Train_Loss: 0.0683312062828918, Test Acc: 0.6695205479452054\n",
      "Epoch 410, Train_Loss: 0.08005619989489787, Test Acc: 0.6678082191780822\n",
      "Epoch 411, Train_Loss: 0.07456438933877507, Test Acc: 0.6592465753424658\n",
      "Epoch 412, Train_Loss: 0.06517367589549394, Test Acc: 0.6541095890410958\n",
      "Epoch 413, Train_Loss: 0.07473332896006468, Test Acc: 0.6643835616438356\n",
      "Epoch 414, Train_Loss: 0.0727177302032942, Test Acc: 0.6712328767123288\n",
      "Epoch 415, Train_Loss: 0.06617573471157812, Test Acc: 0.672945205479452\n",
      "Epoch 416, Train_Loss: 0.06840879277297063, Test Acc: 0.6575342465753424\n",
      "Epoch 417, Train_Loss: 0.06578750360858976, Test Acc: 0.660958904109589\n",
      "Epoch 418, Train_Loss: 0.0704688496698509, Test Acc: 0.6643835616438356\n",
      "Epoch 419, Train_Loss: 0.06397071589890402, Test Acc: 0.6712328767123288\n",
      "Epoch 420, Train_Loss: 0.06284358297125436, Test Acc: 0.6695205479452054\n",
      "Epoch 421, Train_Loss: 0.06171590075973654, Test Acc: 0.666095890410959\n",
      "Epoch 422, Train_Loss: 0.0661582440843631, Test Acc: 0.6575342465753424\n",
      "Epoch 423, Train_Loss: 0.0661577502542059, Test Acc: 0.6746575342465754\n",
      "Epoch 424, Train_Loss: 0.06751460622399463, Test Acc: 0.6592465753424658\n",
      "Epoch 425, Train_Loss: 0.06129943659470882, Test Acc: 0.6626712328767124\n",
      "Epoch 426, Train_Loss: 0.07033735416916898, Test Acc: 0.6678082191780822\n",
      "Epoch 427, Train_Loss: 0.07577974818559596, Test Acc: 0.6626712328767124\n",
      "Epoch 428, Train_Loss: 0.06409506966156187, Test Acc: 0.6678082191780822\n",
      "Epoch 429, Train_Loss: 0.06048192528032814, Test Acc: 0.6695205479452054\n",
      "Epoch 430, Train_Loss: 0.05894580508902436, Test Acc: 0.6678082191780822\n",
      "Epoch 431, Train_Loss: 0.05840428733063163, Test Acc: 0.6678082191780822\n",
      "Epoch 432, Train_Loss: 0.0619034669798566, Test Acc: 0.666095890410959\n",
      "Epoch 433, Train_Loss: 0.06238576844407362, Test Acc: 0.6541095890410958\n",
      "Epoch 434, Train_Loss: 0.06424156164212036, Test Acc: 0.6678082191780822\n",
      "Epoch 435, Train_Loss: 0.06706057000701549, Test Acc: 0.6575342465753424\n",
      "Epoch 436, Train_Loss: 0.05917101021623239, Test Acc: 0.6678082191780822\n",
      "Epoch 437, Train_Loss: 0.06293538493810047, Test Acc: 0.6695205479452054\n",
      "Epoch 438, Train_Loss: 0.07396223579416983, Test Acc: 0.6643835616438356\n",
      "Epoch 439, Train_Loss: 0.06563703724532388, Test Acc: 0.672945205479452\n",
      "Epoch 440, Train_Loss: 0.06483318427854101, Test Acc: 0.6643835616438356\n",
      "Epoch 441, Train_Loss: 0.05704824130225461, Test Acc: 0.6678082191780822\n",
      "Epoch 442, Train_Loss: 0.06941169031779282, Test Acc: 0.6763698630136986\n",
      "Epoch 443, Train_Loss: 0.06042116137541598, Test Acc: 0.6678082191780822\n",
      "Epoch 444, Train_Loss: 0.05703744594211457, Test Acc: 0.6523972602739726\n",
      "Epoch 445, Train_Loss: 0.05369215930113569, Test Acc: 0.6592465753424658\n",
      "Epoch 446, Train_Loss: 0.05984279698532191, Test Acc: 0.672945205479452\n",
      "Epoch 447, Train_Loss: 0.07459457896766253, Test Acc: 0.6558219178082192\n",
      "Epoch 448, Train_Loss: 0.06171138023637468, Test Acc: 0.6695205479452054\n",
      "Epoch 449, Train_Loss: 0.058561893696605694, Test Acc: 0.6678082191780822\n",
      "Epoch 450, Train_Loss: 0.06605394984217128, Test Acc: 0.6541095890410958\n",
      "Epoch 451, Train_Loss: 0.06012652491335757, Test Acc: 0.6643835616438356\n",
      "Epoch 452, Train_Loss: 0.0510035355873697, Test Acc: 0.6695205479452054\n",
      "Epoch 453, Train_Loss: 0.06162709702948632, Test Acc: 0.6695205479452054\n",
      "Epoch 454, Train_Loss: 0.05375828152682516, Test Acc: 0.6592465753424658\n",
      "Epoch 455, Train_Loss: 0.05893974439459271, Test Acc: 0.6695205479452054\n",
      "Epoch 456, Train_Loss: 0.05572658262826735, Test Acc: 0.666095890410959\n",
      "Epoch 457, Train_Loss: 0.06620873883366585, Test Acc: 0.6523972602739726\n",
      "Epoch 458, Train_Loss: 0.059756099435617216, Test Acc: 0.6626712328767124\n",
      "Epoch 459, Train_Loss: 0.06491777212795569, Test Acc: 0.672945205479452\n",
      "Epoch 460, Train_Loss: 0.05706647898477968, Test Acc: 0.6643835616438356\n",
      "Epoch 461, Train_Loss: 0.06074142481520539, Test Acc: 0.6678082191780822\n",
      "Epoch 462, Train_Loss: 0.06018344602853176, Test Acc: 0.6678082191780822\n",
      "Epoch 463, Train_Loss: 0.06165454312576912, Test Acc: 0.666095890410959\n",
      "Epoch 464, Train_Loss: 0.05836569112216239, Test Acc: 0.666095890410959\n",
      "Epoch 465, Train_Loss: 0.0593993633883656, Test Acc: 0.6643835616438356\n",
      "Epoch 466, Train_Loss: 0.057430530428973725, Test Acc: 0.6678082191780822\n",
      "Epoch 467, Train_Loss: 0.06387429408277967, Test Acc: 0.666095890410959\n",
      "Epoch 468, Train_Loss: 0.050939742737682536, Test Acc: 0.6626712328767124\n",
      "Epoch 469, Train_Loss: 0.05349956697318703, Test Acc: 0.6643835616438356\n",
      "Epoch 470, Train_Loss: 0.04966752034124511, Test Acc: 0.6643835616438356\n",
      "Epoch 471, Train_Loss: 0.05731017018115381, Test Acc: 0.6575342465753424\n",
      "Epoch 472, Train_Loss: 0.05666346623911522, Test Acc: 0.6678082191780822\n",
      "Epoch 473, Train_Loss: 0.053049920294142794, Test Acc: 0.6558219178082192\n",
      "Epoch 474, Train_Loss: 0.04945850567310117, Test Acc: 0.6643835616438356\n",
      "Epoch 475, Train_Loss: 0.059039990683231736, Test Acc: 0.6678082191780822\n",
      "Epoch 476, Train_Loss: 0.05481569705079892, Test Acc: 0.666095890410959\n",
      "Epoch 477, Train_Loss: 0.05402679332473781, Test Acc: 0.666095890410959\n",
      "Epoch 478, Train_Loss: 0.05244912483613007, Test Acc: 0.6626712328767124\n",
      "Epoch 479, Train_Loss: 0.05434212610271061, Test Acc: 0.666095890410959\n",
      "Epoch 480, Train_Loss: 0.05701140647215652, Test Acc: 0.6626712328767124\n",
      "Epoch 481, Train_Loss: 0.056030088857369265, Test Acc: 0.666095890410959\n",
      "Epoch 482, Train_Loss: 0.056239770412503276, Test Acc: 0.6626712328767124\n",
      "Epoch 483, Train_Loss: 0.053664986859075725, Test Acc: 0.6575342465753424\n",
      "Epoch 484, Train_Loss: 0.05029760128854832, Test Acc: 0.6575342465753424\n",
      "Epoch 485, Train_Loss: 0.05098285827989457, Test Acc: 0.6678082191780822\n",
      "Epoch 486, Train_Loss: 0.04665079227561364, Test Acc: 0.666095890410959\n",
      "Epoch 487, Train_Loss: 0.05276147567201406, Test Acc: 0.648972602739726\n",
      "Epoch 488, Train_Loss: 0.05468586082861293, Test Acc: 0.6695205479452054\n",
      "Epoch 489, Train_Loss: 0.05092804757805425, Test Acc: 0.6626712328767124\n",
      "Epoch 490, Train_Loss: 0.04833549417890026, Test Acc: 0.666095890410959\n",
      "Epoch 491, Train_Loss: 0.04759244645538274, Test Acc: 0.666095890410959\n",
      "Epoch 492, Train_Loss: 0.05363695578125771, Test Acc: 0.672945205479452\n",
      "Epoch 493, Train_Loss: 0.053669171891669976, Test Acc: 0.666095890410959\n",
      "Epoch 494, Train_Loss: 0.049448744179244386, Test Acc: 0.666095890410959\n",
      "Epoch 495, Train_Loss: 0.05557230735576013, Test Acc: 0.6643835616438356\n",
      "Epoch 496, Train_Loss: 0.050074763250449905, Test Acc: 0.666095890410959\n",
      "Epoch 497, Train_Loss: 0.0484532848422532, Test Acc: 0.666095890410959\n",
      "Epoch 498, Train_Loss: 0.053021215837361524, Test Acc: 0.6626712328767124\n",
      "Epoch 499, Train_Loss: 0.051135898123902734, Test Acc: 0.666095890410959\n",
      "Epoch 500, Train_Loss: 0.048104166042321594, Test Acc: 0.6678082191780822\n",
      "Epoch 501, Train_Loss: 0.04445792973638163, Test Acc: 0.6643835616438356\n",
      "Epoch 502, Train_Loss: 0.045874717416154454, Test Acc: 0.6712328767123288\n",
      "Epoch 503, Train_Loss: 0.054572659526456846, Test Acc: 0.6643835616438356\n",
      "Epoch 504, Train_Loss: 0.04754875548678683, Test Acc: 0.6678082191780822\n",
      "Epoch 505, Train_Loss: 0.047340330042061396, Test Acc: 0.6695205479452054\n",
      "Epoch 506, Train_Loss: 0.044363874119881075, Test Acc: 0.666095890410959\n",
      "Epoch 507, Train_Loss: 0.052277322098234436, Test Acc: 0.6678082191780822\n",
      "Epoch 508, Train_Loss: 0.05154958383900521, Test Acc: 0.666095890410959\n",
      "Epoch 509, Train_Loss: 0.04520920427603414, Test Acc: 0.660958904109589\n",
      "Epoch 510, Train_Loss: 0.04723782379733166, Test Acc: 0.666095890410959\n",
      "Epoch 511, Train_Loss: 0.05498024851112859, Test Acc: 0.6575342465753424\n",
      "Epoch 512, Train_Loss: 0.04505230534778093, Test Acc: 0.6678082191780822\n",
      "Epoch 513, Train_Loss: 0.04555085511674406, Test Acc: 0.6695205479452054\n",
      "Epoch 514, Train_Loss: 0.041921102896594675, Test Acc: 0.6712328767123288\n",
      "Epoch 515, Train_Loss: 0.041155960998366936, Test Acc: 0.666095890410959\n",
      "Epoch 516, Train_Loss: 0.04421324483701028, Test Acc: 0.6643835616438356\n",
      "Epoch 517, Train_Loss: 0.04353824364443426, Test Acc: 0.6695205479452054\n",
      "Epoch 518, Train_Loss: 0.056162768094509374, Test Acc: 0.6643835616438356\n",
      "Epoch 519, Train_Loss: 0.045954599660035456, Test Acc: 0.666095890410959\n",
      "Epoch 520, Train_Loss: 0.043802106725706835, Test Acc: 0.6643835616438356\n",
      "Epoch 521, Train_Loss: 0.05072281671164092, Test Acc: 0.6643835616438356\n",
      "Epoch 522, Train_Loss: 0.04710178753703076, Test Acc: 0.6541095890410958\n",
      "Epoch 523, Train_Loss: 0.042705183412181213, Test Acc: 0.666095890410959\n",
      "Epoch 524, Train_Loss: 0.044698135661747074, Test Acc: 0.6678082191780822\n",
      "Epoch 525, Train_Loss: 0.042828710120375035, Test Acc: 0.666095890410959\n",
      "Epoch 526, Train_Loss: 0.04386850539231091, Test Acc: 0.666095890410959\n",
      "Epoch 527, Train_Loss: 0.041507419520712574, Test Acc: 0.666095890410959\n",
      "Epoch 528, Train_Loss: 0.041504812503262656, Test Acc: 0.666095890410959\n",
      "Epoch 529, Train_Loss: 0.04759796164580621, Test Acc: 0.6592465753424658\n",
      "Epoch 530, Train_Loss: 0.04603580036564381, Test Acc: 0.6626712328767124\n",
      "Epoch 531, Train_Loss: 0.05033816711147665, Test Acc: 0.666095890410959\n",
      "Epoch 532, Train_Loss: 0.04160388342643273, Test Acc: 0.6592465753424658\n",
      "Epoch 533, Train_Loss: 0.04749936452572001, Test Acc: 0.6558219178082192\n",
      "Epoch 534, Train_Loss: 0.05448604788944067, Test Acc: 0.6643835616438356\n",
      "Epoch 535, Train_Loss: 0.044941166506760055, Test Acc: 0.6643835616438356\n",
      "Epoch 536, Train_Loss: 0.043095614259073045, Test Acc: 0.6626712328767124\n",
      "Epoch 537, Train_Loss: 0.041137132022413425, Test Acc: 0.666095890410959\n",
      "Epoch 538, Train_Loss: 0.03954041607721592, Test Acc: 0.6592465753424658\n",
      "Epoch 539, Train_Loss: 0.04126090424324502, Test Acc: 0.6643835616438356\n",
      "Epoch 540, Train_Loss: 0.04465718470601132, Test Acc: 0.6558219178082192\n",
      "Epoch 541, Train_Loss: 0.043618070591037394, Test Acc: 0.666095890410959\n",
      "Epoch 542, Train_Loss: 0.04004705078295956, Test Acc: 0.6643835616438356\n",
      "Epoch 543, Train_Loss: 0.04639938541004085, Test Acc: 0.6592465753424658\n",
      "Epoch 544, Train_Loss: 0.04573664538474986, Test Acc: 0.666095890410959\n",
      "Epoch 545, Train_Loss: 0.040087386063532904, Test Acc: 0.666095890410959\n",
      "Epoch 546, Train_Loss: 0.04138456423061143, Test Acc: 0.666095890410959\n",
      "Epoch 547, Train_Loss: 0.0414731059645419, Test Acc: 0.6678082191780822\n",
      "Epoch 548, Train_Loss: 0.053249438176862895, Test Acc: 0.6643835616438356\n",
      "Epoch 549, Train_Loss: 0.04584794948823401, Test Acc: 0.6592465753424658\n",
      "Epoch 550, Train_Loss: 0.03864119371610286, Test Acc: 0.6643835616438356\n",
      "Epoch 551, Train_Loss: 0.042678316804085625, Test Acc: 0.666095890410959\n",
      "Epoch 552, Train_Loss: 0.04743212453649903, Test Acc: 0.666095890410959\n",
      "Epoch 553, Train_Loss: 0.05055313190314337, Test Acc: 0.6626712328767124\n",
      "Epoch 554, Train_Loss: 0.04210760080422915, Test Acc: 0.666095890410959\n",
      "Epoch 555, Train_Loss: 0.03865104706346756, Test Acc: 0.666095890410959\n",
      "Epoch 556, Train_Loss: 0.0403075076683308, Test Acc: 0.666095890410959\n",
      "Epoch 557, Train_Loss: 0.04195110561704496, Test Acc: 0.6643835616438356\n",
      "Epoch 558, Train_Loss: 0.036095735214985325, Test Acc: 0.666095890410959\n",
      "Epoch 559, Train_Loss: 0.03570041764396592, Test Acc: 0.6626712328767124\n",
      "Epoch 560, Train_Loss: 0.04951989624169073, Test Acc: 0.660958904109589\n",
      "Epoch 561, Train_Loss: 0.03793951829356956, Test Acc: 0.6626712328767124\n",
      "Epoch 562, Train_Loss: 0.03998273234537919, Test Acc: 0.6626712328767124\n",
      "Epoch 563, Train_Loss: 0.0406277721231163, Test Acc: 0.6626712328767124\n",
      "Epoch 564, Train_Loss: 0.0400548909092322, Test Acc: 0.6626712328767124\n",
      "Epoch 565, Train_Loss: 0.03739170708831807, Test Acc: 0.6626712328767124\n",
      "Epoch 566, Train_Loss: 0.039373019186314195, Test Acc: 0.6541095890410958\n",
      "Epoch 567, Train_Loss: 0.039078061910913675, Test Acc: 0.666095890410959\n",
      "Epoch 568, Train_Loss: 0.03972105406501214, Test Acc: 0.6626712328767124\n",
      "Epoch 569, Train_Loss: 0.03711350553203374, Test Acc: 0.666095890410959\n",
      "Epoch 570, Train_Loss: 0.04183396350344992, Test Acc: 0.660958904109589\n",
      "Epoch 571, Train_Loss: 0.04071831375222246, Test Acc: 0.6626712328767124\n",
      "Epoch 572, Train_Loss: 0.04076359654936823, Test Acc: 0.6626712328767124\n",
      "Epoch 573, Train_Loss: 0.03924044003360905, Test Acc: 0.6626712328767124\n",
      "Epoch 574, Train_Loss: 0.044604801889363443, Test Acc: 0.6626712328767124\n",
      "Epoch 575, Train_Loss: 0.035113524234475335, Test Acc: 0.6643835616438356\n",
      "Epoch 576, Train_Loss: 0.04569574499328155, Test Acc: 0.6626712328767124\n",
      "Epoch 577, Train_Loss: 0.040736144212132785, Test Acc: 0.660958904109589\n",
      "Epoch 578, Train_Loss: 0.03461590928782243, Test Acc: 0.6643835616438356\n",
      "Epoch 579, Train_Loss: 0.038039785395085346, Test Acc: 0.666095890410959\n",
      "Epoch 580, Train_Loss: 0.038899017996300245, Test Acc: 0.6592465753424658\n",
      "Epoch 581, Train_Loss: 0.0460584835454938, Test Acc: 0.660958904109589\n",
      "Epoch 582, Train_Loss: 0.036528740403809934, Test Acc: 0.6678082191780822\n",
      "Epoch 583, Train_Loss: 0.04195300841820426, Test Acc: 0.666095890410959\n",
      "Epoch 584, Train_Loss: 0.036310254923591856, Test Acc: 0.6643835616438356\n",
      "Epoch 585, Train_Loss: 0.040274629478517454, Test Acc: 0.6592465753424658\n",
      "Epoch 586, Train_Loss: 0.036705295538922655, Test Acc: 0.6626712328767124\n",
      "Epoch 587, Train_Loss: 0.035700529642781476, Test Acc: 0.6678082191780822\n",
      "Epoch 588, Train_Loss: 0.031106297024052765, Test Acc: 0.6643835616438356\n",
      "Epoch 589, Train_Loss: 0.041238642575990525, Test Acc: 0.6626712328767124\n",
      "Epoch 590, Train_Loss: 0.0346569117045874, Test Acc: 0.660958904109589\n",
      "Epoch 591, Train_Loss: 0.03842447050010378, Test Acc: 0.666095890410959\n",
      "Epoch 592, Train_Loss: 0.03447084088838892, Test Acc: 0.6626712328767124\n",
      "Epoch 593, Train_Loss: 0.037133257908863015, Test Acc: 0.6626712328767124\n",
      "Epoch 594, Train_Loss: 0.0317056888488878, Test Acc: 0.666095890410959\n",
      "Epoch 595, Train_Loss: 0.03693417397880694, Test Acc: 0.6643835616438356\n",
      "Epoch 596, Train_Loss: 0.0310725819254003, Test Acc: 0.6626712328767124\n",
      "Epoch 597, Train_Loss: 0.03598759736996726, Test Acc: 0.6626712328767124\n",
      "Epoch 598, Train_Loss: 0.03210669627515017, Test Acc: 0.6678082191780822\n",
      "Epoch 599, Train_Loss: 0.03329588388623961, Test Acc: 0.660958904109589\n",
      "Epoch 600, Train_Loss: 0.03113435080013005, Test Acc: 0.6643835616438356\n",
      "Epoch 601, Train_Loss: 0.03257332233170018, Test Acc: 0.660958904109589\n",
      "Epoch 602, Train_Loss: 0.04078254060004838, Test Acc: 0.6558219178082192\n",
      "Epoch 603, Train_Loss: 0.03255802607600344, Test Acc: 0.6643835616438356\n",
      "Epoch 604, Train_Loss: 0.04077122777016484, Test Acc: 0.660958904109589\n",
      "Epoch 605, Train_Loss: 0.0337883637566847, Test Acc: 0.6643835616438356\n",
      "Epoch 606, Train_Loss: 0.03642041875536961, Test Acc: 0.6643835616438356\n",
      "Epoch 607, Train_Loss: 0.038552166888621286, Test Acc: 0.6643835616438356\n",
      "Epoch 608, Train_Loss: 0.03717098736706248, Test Acc: 0.666095890410959\n",
      "Epoch 609, Train_Loss: 0.04070513915212359, Test Acc: 0.6643835616438356\n",
      "Epoch 610, Train_Loss: 0.03634462796617299, Test Acc: 0.6643835616438356\n",
      "Epoch 611, Train_Loss: 0.042782959795658826, Test Acc: 0.6592465753424658\n",
      "Epoch 612, Train_Loss: 0.031981115744201816, Test Acc: 0.6643835616438356\n",
      "Epoch 613, Train_Loss: 0.038263497550360626, Test Acc: 0.666095890410959\n",
      "Epoch 614, Train_Loss: 0.035202539973397506, Test Acc: 0.6626712328767124\n",
      "Epoch 615, Train_Loss: 0.030985119678007322, Test Acc: 0.6643835616438356\n",
      "Epoch 616, Train_Loss: 0.03563156852942484, Test Acc: 0.6643835616438356\n",
      "Epoch 617, Train_Loss: 0.0348852004208311, Test Acc: 0.6626712328767124\n",
      "Epoch 618, Train_Loss: 0.028864347432318027, Test Acc: 0.6695205479452054\n",
      "Epoch 619, Train_Loss: 0.030837995518595562, Test Acc: 0.666095890410959\n",
      "Epoch 620, Train_Loss: 0.03194715944300697, Test Acc: 0.666095890410959\n",
      "Epoch 621, Train_Loss: 0.03238872087058553, Test Acc: 0.6643835616438356\n",
      "Epoch 622, Train_Loss: 0.036540975197567604, Test Acc: 0.666095890410959\n",
      "Epoch 623, Train_Loss: 0.03595299203698232, Test Acc: 0.666095890410959\n",
      "Epoch 624, Train_Loss: 0.03194322425224527, Test Acc: 0.666095890410959\n",
      "Epoch 625, Train_Loss: 0.04408599664384383, Test Acc: 0.660958904109589\n",
      "Epoch 626, Train_Loss: 0.03287198852376605, Test Acc: 0.666095890410959\n",
      "Epoch 627, Train_Loss: 0.03740582698992512, Test Acc: 0.6592465753424658\n",
      "Epoch 628, Train_Loss: 0.03240004335202684, Test Acc: 0.666095890410959\n",
      "Epoch 629, Train_Loss: 0.029780915075207304, Test Acc: 0.666095890410959\n",
      "Epoch 630, Train_Loss: 0.03143767916117213, Test Acc: 0.666095890410959\n",
      "Epoch 631, Train_Loss: 0.030492692201733007, Test Acc: 0.6643835616438356\n",
      "Epoch 632, Train_Loss: 0.02917641845488106, Test Acc: 0.6643835616438356\n",
      "Epoch 633, Train_Loss: 0.03263434242580843, Test Acc: 0.666095890410959\n",
      "Epoch 634, Train_Loss: 0.027497904338815715, Test Acc: 0.666095890410959\n",
      "Epoch 635, Train_Loss: 0.0309264133020406, Test Acc: 0.666095890410959\n",
      "Epoch 636, Train_Loss: 0.02941426590041374, Test Acc: 0.6643835616438356\n",
      "Epoch 637, Train_Loss: 0.028182935144286603, Test Acc: 0.666095890410959\n",
      "Epoch 638, Train_Loss: 0.03118468752290937, Test Acc: 0.666095890410959\n",
      "Epoch 639, Train_Loss: 0.037554634756816085, Test Acc: 0.6626712328767124\n",
      "Epoch 640, Train_Loss: 0.02882878178388637, Test Acc: 0.666095890410959\n",
      "Epoch 641, Train_Loss: 0.03397498409321997, Test Acc: 0.6626712328767124\n",
      "Epoch 642, Train_Loss: 0.03192400796706352, Test Acc: 0.666095890410959\n",
      "Epoch 643, Train_Loss: 0.03516590703839029, Test Acc: 0.6643835616438356\n",
      "Epoch 644, Train_Loss: 0.03453317989442439, Test Acc: 0.666095890410959\n",
      "Epoch 645, Train_Loss: 0.028456213323806878, Test Acc: 0.6643835616438356\n",
      "Epoch 646, Train_Loss: 0.03274537758898077, Test Acc: 0.6643835616438356\n",
      "Epoch 647, Train_Loss: 0.024502958956873044, Test Acc: 0.6643835616438356\n",
      "Epoch 648, Train_Loss: 0.033315241043965216, Test Acc: 0.6643835616438356\n",
      "Epoch 649, Train_Loss: 0.028188075570142246, Test Acc: 0.6678082191780822\n",
      "Epoch 650, Train_Loss: 0.032163046682399, Test Acc: 0.6643835616438356\n",
      "Epoch 651, Train_Loss: 0.029908266540587647, Test Acc: 0.666095890410959\n",
      "Epoch 652, Train_Loss: 0.031471278931348934, Test Acc: 0.666095890410959\n",
      "Epoch 653, Train_Loss: 0.033601991688556154, Test Acc: 0.6643835616438356\n",
      "Epoch 654, Train_Loss: 0.030597583414419205, Test Acc: 0.6643835616438356\n",
      "Epoch 655, Train_Loss: 0.02974851004728407, Test Acc: 0.6643835616438356\n",
      "Epoch 656, Train_Loss: 0.03121187858414487, Test Acc: 0.666095890410959\n",
      "Epoch 657, Train_Loss: 0.035144099636454484, Test Acc: 0.6626712328767124\n",
      "Epoch 658, Train_Loss: 0.03345922503831389, Test Acc: 0.666095890410959\n",
      "Epoch 659, Train_Loss: 0.034329435029576416, Test Acc: 0.660958904109589\n",
      "Epoch 660, Train_Loss: 0.029092361186485505, Test Acc: 0.666095890410959\n",
      "Epoch 661, Train_Loss: 0.02856973081270553, Test Acc: 0.6643835616438356\n",
      "Epoch 662, Train_Loss: 0.029501824035833124, Test Acc: 0.666095890410959\n",
      "Epoch 663, Train_Loss: 0.029599537874673842, Test Acc: 0.6541095890410958\n",
      "Epoch 664, Train_Loss: 0.031420265024280525, Test Acc: 0.6643835616438356\n",
      "Epoch 665, Train_Loss: 0.030607664964918513, Test Acc: 0.666095890410959\n",
      "Epoch 666, Train_Loss: 0.030209708862457774, Test Acc: 0.6643835616438356\n",
      "Epoch 667, Train_Loss: 0.035681282461155206, Test Acc: 0.6643835616438356\n",
      "Epoch 668, Train_Loss: 0.02954057361057494, Test Acc: 0.6643835616438356\n",
      "Epoch 669, Train_Loss: 0.036117031901085284, Test Acc: 0.6643835616438356\n",
      "Epoch 670, Train_Loss: 0.040794898139211, Test Acc: 0.666095890410959\n",
      "Epoch 671, Train_Loss: 0.034889459171608905, Test Acc: 0.6643835616438356\n",
      "Epoch 672, Train_Loss: 0.02885507916107599, Test Acc: 0.6643835616438356\n",
      "Epoch 673, Train_Loss: 0.02859774465468945, Test Acc: 0.6643835616438356\n",
      "Epoch 674, Train_Loss: 0.027953267488555866, Test Acc: 0.6643835616438356\n",
      "Epoch 675, Train_Loss: 0.031164956923021236, Test Acc: 0.6592465753424658\n",
      "Epoch 676, Train_Loss: 0.030128261762001785, Test Acc: 0.666095890410959\n",
      "Epoch 677, Train_Loss: 0.02859770537725126, Test Acc: 0.6643835616438356\n",
      "Epoch 678, Train_Loss: 0.02838175614579086, Test Acc: 0.6643835616438356\n",
      "Epoch 679, Train_Loss: 0.026923067305688164, Test Acc: 0.6643835616438356\n",
      "Epoch 680, Train_Loss: 0.031888474404695444, Test Acc: 0.6643835616438356\n",
      "Epoch 681, Train_Loss: 0.030025751650100574, Test Acc: 0.660958904109589\n",
      "Epoch 682, Train_Loss: 0.03236935840050137, Test Acc: 0.6643835616438356\n",
      "Epoch 683, Train_Loss: 0.02982489178521064, Test Acc: 0.6643835616438356\n",
      "Epoch 684, Train_Loss: 0.031687419192167, Test Acc: 0.6626712328767124\n",
      "Epoch 685, Train_Loss: 0.03169157729098515, Test Acc: 0.666095890410959\n",
      "Epoch 686, Train_Loss: 0.02962530941294972, Test Acc: 0.6643835616438356\n",
      "Epoch 687, Train_Loss: 0.030159110630847863, Test Acc: 0.6678082191780822\n",
      "Epoch 688, Train_Loss: 0.025034046195287374, Test Acc: 0.6678082191780822\n",
      "Epoch 689, Train_Loss: 0.029848331614630297, Test Acc: 0.6643835616438356\n",
      "Epoch 690, Train_Loss: 0.02711600668681058, Test Acc: 0.6626712328767124\n",
      "Epoch 691, Train_Loss: 0.027302279060677392, Test Acc: 0.6626712328767124\n",
      "Epoch 692, Train_Loss: 0.027374837463867152, Test Acc: 0.6643835616438356\n",
      "Epoch 693, Train_Loss: 0.03171419250429608, Test Acc: 0.6626712328767124\n",
      "Epoch 694, Train_Loss: 0.024184946649256744, Test Acc: 0.6643835616438356\n",
      "Epoch 695, Train_Loss: 0.03175365230708849, Test Acc: 0.660958904109589\n",
      "Epoch 696, Train_Loss: 0.03007431610240019, Test Acc: 0.6643835616438356\n",
      "Epoch 697, Train_Loss: 0.030317755348733044, Test Acc: 0.6643835616438356\n",
      "Epoch 698, Train_Loss: 0.030026428488781676, Test Acc: 0.6643835616438356\n",
      "Epoch 699, Train_Loss: 0.026348118164605694, Test Acc: 0.6626712328767124\n",
      "Epoch 700, Train_Loss: 0.027208218183659483, Test Acc: 0.6643835616438356\n",
      "Epoch 701, Train_Loss: 0.027482157453050604, Test Acc: 0.6643835616438356\n",
      "Epoch 702, Train_Loss: 0.027645583970297594, Test Acc: 0.666095890410959\n",
      "Epoch 703, Train_Loss: 0.0309768144525151, Test Acc: 0.6643835616438356\n",
      "Epoch 704, Train_Loss: 0.03119892220274778, Test Acc: 0.666095890410959\n",
      "Epoch 705, Train_Loss: 0.02647241435806791, Test Acc: 0.6643835616438356\n",
      "Epoch 706, Train_Loss: 0.026315787413295766, Test Acc: 0.6643835616438356\n",
      "Epoch 707, Train_Loss: 0.028649918354858528, Test Acc: 0.6626712328767124\n",
      "Epoch 708, Train_Loss: 0.028343005058559356, Test Acc: 0.6643835616438356\n",
      "Epoch 709, Train_Loss: 0.025751930714250193, Test Acc: 0.6643835616438356\n",
      "Epoch 710, Train_Loss: 0.03346464210699196, Test Acc: 0.6643835616438356\n",
      "Epoch 711, Train_Loss: 0.025609697457184666, Test Acc: 0.6592465753424658\n",
      "Epoch 712, Train_Loss: 0.027481567954964703, Test Acc: 0.666095890410959\n",
      "Epoch 713, Train_Loss: 0.02719684721159865, Test Acc: 0.6643835616438356\n",
      "Epoch 714, Train_Loss: 0.026891034691743698, Test Acc: 0.6643835616438356\n",
      "Epoch 715, Train_Loss: 0.02869927018400631, Test Acc: 0.6643835616438356\n",
      "Epoch 716, Train_Loss: 0.02714094477778417, Test Acc: 0.6643835616438356\n",
      "Epoch 717, Train_Loss: 0.03425801432604203, Test Acc: 0.6506849315068494\n",
      "Epoch 718, Train_Loss: 0.031723846515888, Test Acc: 0.6626712328767124\n",
      "Epoch 719, Train_Loss: 0.03387396188827552, Test Acc: 0.6643835616438356\n",
      "Epoch 720, Train_Loss: 0.02779884283154388, Test Acc: 0.6643835616438356\n",
      "Epoch 721, Train_Loss: 0.02868790030515811, Test Acc: 0.666095890410959\n",
      "Epoch 722, Train_Loss: 0.029327358564842143, Test Acc: 0.6643835616438356\n",
      "Epoch 723, Train_Loss: 0.025170291506583453, Test Acc: 0.6643835616438356\n",
      "Epoch 724, Train_Loss: 0.025680512297640234, Test Acc: 0.6643835616438356\n",
      "Epoch 725, Train_Loss: 0.024177004477678565, Test Acc: 0.660958904109589\n",
      "Epoch 726, Train_Loss: 0.028692795323877363, Test Acc: 0.6643835616438356\n",
      "Epoch 727, Train_Loss: 0.030150358548780787, Test Acc: 0.6643835616438356\n",
      "Epoch 728, Train_Loss: 0.026755783021144453, Test Acc: 0.6626712328767124\n",
      "Epoch 729, Train_Loss: 0.024625588906928897, Test Acc: 0.6643835616438356\n",
      "Epoch 730, Train_Loss: 0.022956532271564356, Test Acc: 0.666095890410959\n",
      "Epoch 731, Train_Loss: 0.029275082941239816, Test Acc: 0.6643835616438356\n",
      "Epoch 732, Train_Loss: 0.027584389352341532, Test Acc: 0.660958904109589\n",
      "Epoch 733, Train_Loss: 0.02989889424861758, Test Acc: 0.6643835616438356\n",
      "Epoch 734, Train_Loss: 0.023172252425865736, Test Acc: 0.6678082191780822\n",
      "Epoch 735, Train_Loss: 0.023491651241783984, Test Acc: 0.666095890410959\n",
      "Epoch 736, Train_Loss: 0.021307482516931486, Test Acc: 0.666095890410959\n",
      "Epoch 737, Train_Loss: 0.02635564552656433, Test Acc: 0.6643835616438356\n",
      "Epoch 738, Train_Loss: 0.027034957457544806, Test Acc: 0.6643835616438356\n",
      "Epoch 739, Train_Loss: 0.029288623935826763, Test Acc: 0.6643835616438356\n",
      "Epoch 740, Train_Loss: 0.02757097433459421, Test Acc: 0.6626712328767124\n",
      "Epoch 741, Train_Loss: 0.025067363976631896, Test Acc: 0.6626712328767124\n",
      "Epoch 742, Train_Loss: 0.0259237277914508, Test Acc: 0.6643835616438356\n",
      "Epoch 743, Train_Loss: 0.02613323981859139, Test Acc: 0.666095890410959\n",
      "Epoch 744, Train_Loss: 0.026160035190059716, Test Acc: 0.6643835616438356\n",
      "Epoch 745, Train_Loss: 0.02852301525308576, Test Acc: 0.6643835616438356\n",
      "Epoch 746, Train_Loss: 0.028478825093770865, Test Acc: 0.6643835616438356\n",
      "Epoch 747, Train_Loss: 0.028332675343335723, Test Acc: 0.6643835616438356\n",
      "Epoch 748, Train_Loss: 0.03414257961594558, Test Acc: 0.6643835616438356\n",
      "Epoch 749, Train_Loss: 0.02256205021876667, Test Acc: 0.6643835616438356\n",
      "Epoch 750, Train_Loss: 0.026605828878018656, Test Acc: 0.6592465753424658\n",
      "Epoch 751, Train_Loss: 0.028321899309048604, Test Acc: 0.6643835616438356\n",
      "Epoch 752, Train_Loss: 0.02697262433503056, Test Acc: 0.6643835616438356\n",
      "Epoch 753, Train_Loss: 0.029324755433663086, Test Acc: 0.6575342465753424\n",
      "Epoch 754, Train_Loss: 0.024832038890963304, Test Acc: 0.6643835616438356\n",
      "Epoch 755, Train_Loss: 0.026530495270890242, Test Acc: 0.660958904109589\n",
      "Epoch 756, Train_Loss: 0.025167455278278794, Test Acc: 0.6643835616438356\n",
      "Epoch 757, Train_Loss: 0.024473750872857636, Test Acc: 0.6643835616438356\n",
      "Epoch 758, Train_Loss: 0.023299424723518314, Test Acc: 0.6643835616438356\n",
      "Epoch 759, Train_Loss: 0.023323648531004437, Test Acc: 0.6643835616438356\n",
      "Epoch 760, Train_Loss: 0.023459366363567824, Test Acc: 0.6643835616438356\n",
      "Epoch 761, Train_Loss: 0.02385035368570243, Test Acc: 0.6643835616438356\n",
      "Epoch 762, Train_Loss: 0.023969514940290537, Test Acc: 0.6643835616438356\n",
      "Epoch 763, Train_Loss: 0.025569080781679077, Test Acc: 0.6643835616438356\n",
      "Epoch 764, Train_Loss: 0.022203426828127704, Test Acc: 0.6643835616438356\n",
      "Epoch 765, Train_Loss: 0.02270281774326577, Test Acc: 0.6626712328767124\n",
      "Epoch 766, Train_Loss: 0.0260423116342281, Test Acc: 0.6643835616438356\n",
      "Epoch 767, Train_Loss: 0.024505894609319512, Test Acc: 0.666095890410959\n",
      "Epoch 768, Train_Loss: 0.02501877448958112, Test Acc: 0.6643835616438356\n",
      "Epoch 769, Train_Loss: 0.023171392495896725, Test Acc: 0.666095890410959\n",
      "Epoch 770, Train_Loss: 0.022304030352643167, Test Acc: 0.6643835616438356\n",
      "Epoch 771, Train_Loss: 0.02122215343206335, Test Acc: 0.6626712328767124\n",
      "Epoch 772, Train_Loss: 0.028001910577586386, Test Acc: 0.6592465753424658\n",
      "Epoch 773, Train_Loss: 0.024217462849264848, Test Acc: 0.6643835616438356\n",
      "Epoch 774, Train_Loss: 0.022506734825583408, Test Acc: 0.666095890410959\n",
      "Epoch 775, Train_Loss: 0.025608211697544903, Test Acc: 0.6643835616438356\n",
      "Epoch 776, Train_Loss: 0.02096886309027468, Test Acc: 0.6643835616438356\n",
      "Epoch 777, Train_Loss: 0.0244115880013851, Test Acc: 0.6643835616438356\n",
      "Epoch 778, Train_Loss: 0.022317152607683965, Test Acc: 0.6643835616438356\n",
      "Epoch 779, Train_Loss: 0.023956115892360685, Test Acc: 0.6643835616438356\n",
      "Epoch 780, Train_Loss: 0.02279541312418587, Test Acc: 0.6678082191780822\n",
      "Epoch 781, Train_Loss: 0.02031606783020834, Test Acc: 0.6643835616438356\n",
      "Epoch 782, Train_Loss: 0.024040243410127005, Test Acc: 0.6643835616438356\n",
      "Epoch 783, Train_Loss: 0.02364368469443434, Test Acc: 0.660958904109589\n",
      "Epoch 784, Train_Loss: 0.024115042006087606, Test Acc: 0.666095890410959\n",
      "Epoch 785, Train_Loss: 0.02507929135890663, Test Acc: 0.666095890410959\n",
      "Epoch 786, Train_Loss: 0.023986527993656637, Test Acc: 0.6626712328767124\n",
      "Epoch 787, Train_Loss: 0.02682119396104099, Test Acc: 0.660958904109589\n",
      "Epoch 788, Train_Loss: 0.028684317369879864, Test Acc: 0.6643835616438356\n",
      "Epoch 789, Train_Loss: 0.0228967496777841, Test Acc: 0.660958904109589\n",
      "Epoch 790, Train_Loss: 0.02720694538220414, Test Acc: 0.660958904109589\n",
      "Epoch 791, Train_Loss: 0.02692027134798991, Test Acc: 0.666095890410959\n",
      "Epoch 792, Train_Loss: 0.02548155535441765, Test Acc: 0.6643835616438356\n",
      "Epoch 793, Train_Loss: 0.0239516841556906, Test Acc: 0.666095890410959\n",
      "Epoch 794, Train_Loss: 0.02606045226457354, Test Acc: 0.6643835616438356\n",
      "Epoch 795, Train_Loss: 0.02369951052696706, Test Acc: 0.6626712328767124\n",
      "Epoch 796, Train_Loss: 0.023878042104342967, Test Acc: 0.6643835616438356\n",
      "Epoch 797, Train_Loss: 0.021199731916567544, Test Acc: 0.6643835616438356\n",
      "Epoch 798, Train_Loss: 0.02410383207279665, Test Acc: 0.6643835616438356\n",
      "Epoch 799, Train_Loss: 0.02438587357482902, Test Acc: 0.6643835616438356\n",
      "Epoch 800, Train_Loss: 0.022612122529608314, Test Acc: 0.6643835616438356\n",
      "Epoch 801, Train_Loss: 0.022609556954193977, Test Acc: 0.666095890410959\n",
      "Epoch 802, Train_Loss: 0.023626845072612923, Test Acc: 0.6643835616438356\n",
      "Epoch 803, Train_Loss: 0.026336085431466927, Test Acc: 0.6626712328767124\n",
      "Epoch 804, Train_Loss: 0.020936236049237777, Test Acc: 0.666095890410959\n",
      "Epoch 805, Train_Loss: 0.026854783955059247, Test Acc: 0.6643835616438356\n",
      "Epoch 806, Train_Loss: 0.01955296463893319, Test Acc: 0.666095890410959\n",
      "Epoch 807, Train_Loss: 0.025788452348933788, Test Acc: 0.6643835616438356\n",
      "Epoch 808, Train_Loss: 0.02173457900789799, Test Acc: 0.6643835616438356\n",
      "Epoch 809, Train_Loss: 0.0205508877770626, Test Acc: 0.6643835616438356\n",
      "Epoch 810, Train_Loss: 0.020921196608469472, Test Acc: 0.6626712328767124\n",
      "Epoch 811, Train_Loss: 0.022769857749153743, Test Acc: 0.6643835616438356\n",
      "Epoch 812, Train_Loss: 0.02211137667291041, Test Acc: 0.6643835616438356\n",
      "Epoch 813, Train_Loss: 0.02434745092978119, Test Acc: 0.6643835616438356\n",
      "Epoch 814, Train_Loss: 0.023020577282295562, Test Acc: 0.6643835616438356\n",
      "Epoch 815, Train_Loss: 0.019464209247416875, Test Acc: 0.666095890410959\n",
      "Epoch 816, Train_Loss: 0.024927130518335616, Test Acc: 0.6626712328767124\n",
      "Epoch 817, Train_Loss: 0.022060105325181212, Test Acc: 0.666095890410959\n",
      "Epoch 818, Train_Loss: 0.022362446170518524, Test Acc: 0.6592465753424658\n",
      "Epoch 819, Train_Loss: 0.020949070102687983, Test Acc: 0.6643835616438356\n",
      "Epoch 820, Train_Loss: 0.01857830551762163, Test Acc: 0.6643835616438356\n",
      "Epoch 821, Train_Loss: 0.02137570889226481, Test Acc: 0.6643835616438356\n",
      "Epoch 822, Train_Loss: 0.018809074066666653, Test Acc: 0.6643835616438356\n",
      "Epoch 823, Train_Loss: 0.018432159880831023, Test Acc: 0.6643835616438356\n",
      "Epoch 824, Train_Loss: 0.0237211382973328, Test Acc: 0.6643835616438356\n",
      "Epoch 825, Train_Loss: 0.019861813009811158, Test Acc: 0.6643835616438356\n",
      "Epoch 826, Train_Loss: 0.022603435178098152, Test Acc: 0.6626712328767124\n",
      "Epoch 827, Train_Loss: 0.019458767505966534, Test Acc: 0.666095890410959\n",
      "Epoch 828, Train_Loss: 0.02259738904376718, Test Acc: 0.6643835616438356\n",
      "Epoch 829, Train_Loss: 0.02173270993080223, Test Acc: 0.660958904109589\n",
      "Epoch 830, Train_Loss: 0.01966425592218002, Test Acc: 0.666095890410959\n",
      "Epoch 831, Train_Loss: 0.017659784761235642, Test Acc: 0.6643835616438356\n",
      "Epoch 832, Train_Loss: 0.019373544477275573, Test Acc: 0.6643835616438356\n",
      "Epoch 833, Train_Loss: 0.025496383092104224, Test Acc: 0.6643835616438356\n",
      "Epoch 834, Train_Loss: 0.024153379681592924, Test Acc: 0.660958904109589\n",
      "Epoch 835, Train_Loss: 0.02121989172701433, Test Acc: 0.6643835616438356\n",
      "Epoch 836, Train_Loss: 0.019513288249981997, Test Acc: 0.6643835616438356\n",
      "Epoch 837, Train_Loss: 0.022289957932116522, Test Acc: 0.6643835616438356\n",
      "Epoch 838, Train_Loss: 0.02045511541655287, Test Acc: 0.6643835616438356\n",
      "Epoch 839, Train_Loss: 0.02166911528001947, Test Acc: 0.6643835616438356\n",
      "Epoch 840, Train_Loss: 0.02787375222942501, Test Acc: 0.6643835616438356\n",
      "Epoch 841, Train_Loss: 0.020757564254381577, Test Acc: 0.6643835616438356\n",
      "Epoch 842, Train_Loss: 0.01953900603848524, Test Acc: 0.660958904109589\n",
      "Epoch 843, Train_Loss: 0.022975388084887527, Test Acc: 0.6643835616438356\n",
      "Epoch 844, Train_Loss: 0.019874592640007904, Test Acc: 0.6643835616438356\n",
      "Epoch 845, Train_Loss: 0.020011861070088344, Test Acc: 0.666095890410959\n",
      "Epoch 846, Train_Loss: 0.021303548870491795, Test Acc: 0.666095890410959\n",
      "Epoch 847, Train_Loss: 0.02592307082932166, Test Acc: 0.6541095890410958\n",
      "Epoch 848, Train_Loss: 0.02275070917858102, Test Acc: 0.6592465753424658\n",
      "Epoch 849, Train_Loss: 0.02483192223826336, Test Acc: 0.6626712328767124\n",
      "Epoch 850, Train_Loss: 0.02230846915153961, Test Acc: 0.666095890410959\n",
      "Epoch 851, Train_Loss: 0.02091118217958865, Test Acc: 0.666095890410959\n",
      "Epoch 852, Train_Loss: 0.01932287656563858, Test Acc: 0.6643835616438356\n",
      "Epoch 853, Train_Loss: 0.023676638303186337, Test Acc: 0.6626712328767124\n",
      "Epoch 854, Train_Loss: 0.02388072519988782, Test Acc: 0.666095890410959\n",
      "Epoch 855, Train_Loss: 0.01979668852618488, Test Acc: 0.6643835616438356\n",
      "Epoch 856, Train_Loss: 0.021450312582601327, Test Acc: 0.6643835616438356\n",
      "Epoch 857, Train_Loss: 0.018936912753815704, Test Acc: 0.6643835616438356\n",
      "Epoch 858, Train_Loss: 0.023090447062713793, Test Acc: 0.6678082191780822\n",
      "Epoch 859, Train_Loss: 0.021590360995105584, Test Acc: 0.6626712328767124\n",
      "Epoch 860, Train_Loss: 0.025430073331335734, Test Acc: 0.6626712328767124\n",
      "Epoch 861, Train_Loss: 0.01874538818083238, Test Acc: 0.6643835616438356\n",
      "Epoch 862, Train_Loss: 0.018730130252151866, Test Acc: 0.660958904109589\n",
      "Epoch 863, Train_Loss: 0.019399157336920325, Test Acc: 0.6643835616438356\n",
      "Epoch 864, Train_Loss: 0.020413654896401567, Test Acc: 0.6643835616438356\n",
      "Epoch 865, Train_Loss: 0.01944598137470166, Test Acc: 0.6626712328767124\n",
      "Epoch 866, Train_Loss: 0.018589889597933507, Test Acc: 0.6626712328767124\n",
      "Epoch 867, Train_Loss: 0.027857140322339546, Test Acc: 0.6626712328767124\n",
      "Epoch 868, Train_Loss: 0.019384079828341783, Test Acc: 0.6626712328767124\n",
      "Epoch 869, Train_Loss: 0.021964618604215502, Test Acc: 0.660958904109589\n",
      "Epoch 870, Train_Loss: 0.019501675769788562, Test Acc: 0.6626712328767124\n",
      "Epoch 871, Train_Loss: 0.020399735415594478, Test Acc: 0.660958904109589\n",
      "Epoch 872, Train_Loss: 0.01785803733400826, Test Acc: 0.6626712328767124\n",
      "Epoch 873, Train_Loss: 0.020401433246661327, Test Acc: 0.6643835616438356\n",
      "Epoch 874, Train_Loss: 0.02382107469748007, Test Acc: 0.6626712328767124\n",
      "Epoch 875, Train_Loss: 0.019795866992353695, Test Acc: 0.6626712328767124\n",
      "Epoch 876, Train_Loss: 0.019042690489186498, Test Acc: 0.660958904109589\n",
      "Epoch 877, Train_Loss: 0.02119529157926081, Test Acc: 0.6626712328767124\n",
      "Epoch 878, Train_Loss: 0.01942299185338925, Test Acc: 0.660958904109589\n",
      "Epoch 879, Train_Loss: 0.022846696064334537, Test Acc: 0.660958904109589\n",
      "Epoch 880, Train_Loss: 0.0204605769868067, Test Acc: 0.6643835616438356\n",
      "Epoch 881, Train_Loss: 0.01686135355339502, Test Acc: 0.6643835616438356\n",
      "Epoch 882, Train_Loss: 0.014909360002548055, Test Acc: 0.6643835616438356\n",
      "Epoch 883, Train_Loss: 0.020813589185763703, Test Acc: 0.660958904109589\n",
      "Epoch 884, Train_Loss: 0.02033227197443921, Test Acc: 0.6575342465753424\n",
      "Epoch 885, Train_Loss: 0.017975795695747365, Test Acc: 0.6626712328767124\n",
      "Epoch 886, Train_Loss: 0.02069426272282726, Test Acc: 0.660958904109589\n",
      "Epoch 887, Train_Loss: 0.01794730152505508, Test Acc: 0.6575342465753424\n",
      "Epoch 888, Train_Loss: 0.02083054995000566, Test Acc: 0.6643835616438356\n",
      "Epoch 889, Train_Loss: 0.019367473967577098, Test Acc: 0.6643835616438356\n",
      "Epoch 890, Train_Loss: 0.02431866810456995, Test Acc: 0.6592465753424658\n",
      "Epoch 891, Train_Loss: 0.020257040126125503, Test Acc: 0.6643835616438356\n",
      "Epoch 892, Train_Loss: 0.02009346445902338, Test Acc: 0.6592465753424658\n",
      "Epoch 893, Train_Loss: 0.01890498966713494, Test Acc: 0.6643835616438356\n",
      "Epoch 894, Train_Loss: 0.019498282266795286, Test Acc: 0.6643835616438356\n",
      "Epoch 895, Train_Loss: 0.020218880809352413, Test Acc: 0.6626712328767124\n",
      "Epoch 896, Train_Loss: 0.017651362211836386, Test Acc: 0.660958904109589\n",
      "Epoch 897, Train_Loss: 0.019319345401527244, Test Acc: 0.6643835616438356\n",
      "Epoch 898, Train_Loss: 0.019957248386162973, Test Acc: 0.6626712328767124\n",
      "Epoch 899, Train_Loss: 0.019293642742468364, Test Acc: 0.6643835616438356\n",
      "Epoch 900, Train_Loss: 0.017116263039497426, Test Acc: 0.6643835616438356\n",
      "Epoch 901, Train_Loss: 0.018242015703890502, Test Acc: 0.6643835616438356\n",
      "Epoch 902, Train_Loss: 0.018995165680735226, Test Acc: 0.6626712328767124\n",
      "Epoch 903, Train_Loss: 0.019310523352942255, Test Acc: 0.6626712328767124\n",
      "Epoch 904, Train_Loss: 0.02335286834204453, Test Acc: 0.660958904109589\n",
      "Epoch 905, Train_Loss: 0.017058266215826734, Test Acc: 0.6626712328767124\n",
      "Epoch 906, Train_Loss: 0.017033679025189485, Test Acc: 0.6592465753424658\n",
      "Epoch 907, Train_Loss: 0.014501266937713808, Test Acc: 0.6643835616438356\n",
      "Epoch 908, Train_Loss: 0.01874752373623778, Test Acc: 0.6626712328767124\n",
      "Epoch 909, Train_Loss: 0.02031308034202084, Test Acc: 0.6626712328767124\n",
      "Epoch 910, Train_Loss: 0.018314053489120852, Test Acc: 0.6626712328767124\n",
      "Epoch 911, Train_Loss: 0.01849148016754043, Test Acc: 0.6643835616438356\n",
      "Epoch 912, Train_Loss: 0.01875008432489267, Test Acc: 0.660958904109589\n",
      "Epoch 913, Train_Loss: 0.016881826197277405, Test Acc: 0.660958904109589\n",
      "Epoch 914, Train_Loss: 0.019314123941512662, Test Acc: 0.6592465753424658\n",
      "Epoch 915, Train_Loss: 0.018487369710328494, Test Acc: 0.6626712328767124\n",
      "Epoch 916, Train_Loss: 0.02324883275650791, Test Acc: 0.6626712328767124\n",
      "Epoch 917, Train_Loss: 0.018247811967739835, Test Acc: 0.660958904109589\n",
      "Epoch 918, Train_Loss: 0.022557754022273002, Test Acc: 0.660958904109589\n",
      "Epoch 919, Train_Loss: 0.018194267995568225, Test Acc: 0.6626712328767124\n",
      "Epoch 920, Train_Loss: 0.019445802002337587, Test Acc: 0.6626712328767124\n",
      "Epoch 921, Train_Loss: 0.023487983648919908, Test Acc: 0.6626712328767124\n",
      "Epoch 922, Train_Loss: 0.021184817895118613, Test Acc: 0.6626712328767124\n",
      "Epoch 923, Train_Loss: 0.020910128030664055, Test Acc: 0.660958904109589\n",
      "Epoch 924, Train_Loss: 0.015347225036748569, Test Acc: 0.666095890410959\n",
      "Epoch 925, Train_Loss: 0.0203236963989184, Test Acc: 0.6626712328767124\n",
      "Epoch 926, Train_Loss: 0.01925406207828928, Test Acc: 0.666095890410959\n",
      "Epoch 927, Train_Loss: 0.016913763953198213, Test Acc: 0.6626712328767124\n",
      "Epoch 928, Train_Loss: 0.018557532628165063, Test Acc: 0.6643835616438356\n",
      "Epoch 929, Train_Loss: 0.01897669834511362, Test Acc: 0.6626712328767124\n",
      "Epoch 930, Train_Loss: 0.015637172440619906, Test Acc: 0.6626712328767124\n",
      "Epoch 931, Train_Loss: 0.023769764470671362, Test Acc: 0.6626712328767124\n",
      "Epoch 932, Train_Loss: 0.018633557871908124, Test Acc: 0.6626712328767124\n",
      "Epoch 933, Train_Loss: 0.016740661813400948, Test Acc: 0.6678082191780822\n",
      "Epoch 934, Train_Loss: 0.020740526786539704, Test Acc: 0.6626712328767124\n",
      "Epoch 935, Train_Loss: 0.017229364879312925, Test Acc: 0.6626712328767124\n",
      "Epoch 936, Train_Loss: 0.02007605480230268, Test Acc: 0.6575342465753424\n",
      "Epoch 937, Train_Loss: 0.018715287627401267, Test Acc: 0.6626712328767124\n",
      "Epoch 938, Train_Loss: 0.02043583764225332, Test Acc: 0.666095890410959\n",
      "Epoch 939, Train_Loss: 0.019759670556595665, Test Acc: 0.6626712328767124\n",
      "Epoch 940, Train_Loss: 0.017045143782524974, Test Acc: 0.660958904109589\n",
      "Epoch 941, Train_Loss: 0.01790349580369366, Test Acc: 0.666095890410959\n",
      "Epoch 942, Train_Loss: 0.017277652596931148, Test Acc: 0.6678082191780822\n",
      "Epoch 943, Train_Loss: 0.01608890574607358, Test Acc: 0.6643835616438356\n",
      "Epoch 944, Train_Loss: 0.01669346465223498, Test Acc: 0.6643835616438356\n",
      "Epoch 945, Train_Loss: 0.017066840462575783, Test Acc: 0.6592465753424658\n",
      "Epoch 946, Train_Loss: 0.01897018994895916, Test Acc: 0.6643835616438356\n",
      "Epoch 947, Train_Loss: 0.015566293789561314, Test Acc: 0.6643835616438356\n",
      "Epoch 948, Train_Loss: 0.01577678640569502, Test Acc: 0.6643835616438356\n",
      "Epoch 949, Train_Loss: 0.02705416508251801, Test Acc: 0.666095890410959\n",
      "Epoch 950, Train_Loss: 0.01710741402939675, Test Acc: 0.6592465753424658\n",
      "Epoch 951, Train_Loss: 0.01663349762384314, Test Acc: 0.666095890410959\n",
      "Epoch 952, Train_Loss: 0.018112971940809075, Test Acc: 0.6643835616438356\n",
      "Epoch 953, Train_Loss: 0.01684358660440921, Test Acc: 0.666095890410959\n",
      "Epoch 954, Train_Loss: 0.019563118457426754, Test Acc: 0.660958904109589\n",
      "Epoch 955, Train_Loss: 0.02028261363557249, Test Acc: 0.6643835616438356\n",
      "Epoch 956, Train_Loss: 0.01710678066865512, Test Acc: 0.6643835616438356\n",
      "Epoch 957, Train_Loss: 0.020146677196862584, Test Acc: 0.6643835616438356\n",
      "Epoch 958, Train_Loss: 0.017584145512046234, Test Acc: 0.660958904109589\n",
      "Epoch 959, Train_Loss: 0.019844915623252746, Test Acc: 0.648972602739726\n",
      "Epoch 960, Train_Loss: 0.01986049187826211, Test Acc: 0.666095890410959\n",
      "Epoch 961, Train_Loss: 0.01578345898178668, Test Acc: 0.6643835616438356\n",
      "Epoch 962, Train_Loss: 0.019131147708321805, Test Acc: 0.6643835616438356\n",
      "Epoch 963, Train_Loss: 0.017543426796237327, Test Acc: 0.6626712328767124\n",
      "Epoch 964, Train_Loss: 0.013416076194062043, Test Acc: 0.6643835616438356\n",
      "Epoch 965, Train_Loss: 0.018061955036955624, Test Acc: 0.6643835616438356\n",
      "Epoch 966, Train_Loss: 0.01945692606659577, Test Acc: 0.6643835616438356\n",
      "Epoch 967, Train_Loss: 0.021100353607380384, Test Acc: 0.6643835616438356\n",
      "Epoch 968, Train_Loss: 0.015592116019433888, Test Acc: 0.6626712328767124\n",
      "Epoch 969, Train_Loss: 0.01561017884228022, Test Acc: 0.6643835616438356\n",
      "Epoch 970, Train_Loss: 0.01619994420616422, Test Acc: 0.6678082191780822\n",
      "Epoch 971, Train_Loss: 0.017830511227657553, Test Acc: 0.6643835616438356\n",
      "Epoch 972, Train_Loss: 0.021013118669543474, Test Acc: 0.660958904109589\n",
      "Epoch 973, Train_Loss: 0.015378739300103916, Test Acc: 0.6643835616438356\n",
      "Epoch 974, Train_Loss: 0.015480728124657617, Test Acc: 0.6643835616438356\n",
      "Epoch 975, Train_Loss: 0.017734369017034624, Test Acc: 0.6643835616438356\n",
      "Epoch 976, Train_Loss: 0.013867274589756562, Test Acc: 0.666095890410959\n",
      "Epoch 977, Train_Loss: 0.020128778556681937, Test Acc: 0.6643835616438356\n",
      "Epoch 978, Train_Loss: 0.013206080138388643, Test Acc: 0.6643835616438356\n",
      "Epoch 979, Train_Loss: 0.01685727331732778, Test Acc: 0.6643835616438356\n",
      "Epoch 980, Train_Loss: 0.01573731143707846, Test Acc: 0.6643835616438356\n",
      "Epoch 981, Train_Loss: 0.01449811441671045, Test Acc: 0.6643835616438356\n",
      "Epoch 982, Train_Loss: 0.017113924204295472, Test Acc: 0.6643835616438356\n",
      "Epoch 983, Train_Loss: 0.017193114794281428, Test Acc: 0.6643835616438356\n",
      "Epoch 984, Train_Loss: 0.01487753444507689, Test Acc: 0.6643835616438356\n",
      "Epoch 985, Train_Loss: 0.017536147754071862, Test Acc: 0.666095890410959\n",
      "Epoch 986, Train_Loss: 0.015584728816975257, Test Acc: 0.6643835616438356\n",
      "Epoch 987, Train_Loss: 0.01999377896254373, Test Acc: 0.6643835616438356\n",
      "Epoch 988, Train_Loss: 0.017934522638825, Test Acc: 0.6643835616438356\n",
      "Epoch 989, Train_Loss: 0.017364473964789795, Test Acc: 0.6643835616438356\n",
      "Epoch 990, Train_Loss: 0.016452624739031307, Test Acc: 0.6643835616438356\n",
      "Epoch 991, Train_Loss: 0.016088575935555127, Test Acc: 0.6626712328767124\n",
      "Epoch 992, Train_Loss: 0.016679986660165014, Test Acc: 0.6592465753424658\n",
      "Epoch 993, Train_Loss: 0.015191621479971218, Test Acc: 0.6643835616438356\n",
      "Epoch 994, Train_Loss: 0.020171983764612378, Test Acc: 0.6643835616438356\n",
      "Epoch 995, Train_Loss: 0.016561188757350465, Test Acc: 0.6643835616438356\n",
      "Epoch 996, Train_Loss: 0.016867414923581237, Test Acc: 0.6626712328767124\n",
      "Epoch 997, Train_Loss: 0.016673588056619337, Test Acc: 0.6643835616438356\n",
      "Epoch 998, Train_Loss: 0.0175183869250759, Test Acc: 0.6643835616438356\n",
      "Epoch 999, Train_Loss: 0.018208710542239714, Test Acc: 0.6643835616438356\n",
      "Epoch 1000, Train_Loss: 0.017943137655947794, Test Acc: 0.6626712328767124\n",
      "Epoch 1001, Train_Loss: 0.015125483915198856, Test Acc: 0.6643835616438356\n",
      "Epoch 1002, Train_Loss: 0.017470544663410692, Test Acc: 0.660958904109589\n",
      "Epoch 1003, Train_Loss: 0.011989044374786317, Test Acc: 0.6643835616438356\n",
      "Epoch 1004, Train_Loss: 0.019063771653236472, Test Acc: 0.6643835616438356\n",
      "Epoch 1005, Train_Loss: 0.017148058550901624, Test Acc: 0.660958904109589\n",
      "Epoch 1006, Train_Loss: 0.016973952209809795, Test Acc: 0.6643835616438356\n",
      "Epoch 1007, Train_Loss: 0.01660123726378515, Test Acc: 0.666095890410959\n",
      "Epoch 1008, Train_Loss: 0.01656541983356874, Test Acc: 0.6643835616438356\n",
      "Epoch 1009, Train_Loss: 0.019463934920167958, Test Acc: 0.6643835616438356\n",
      "Epoch 1010, Train_Loss: 0.014832647508228547, Test Acc: 0.6643835616438356\n",
      "Epoch 1011, Train_Loss: 0.01405478267588478, Test Acc: 0.6643835616438356\n",
      "Epoch 1012, Train_Loss: 0.015705025060924527, Test Acc: 0.6643835616438356\n",
      "Epoch 1013, Train_Loss: 0.016605553828412667, Test Acc: 0.6643835616438356\n",
      "Epoch 1014, Train_Loss: 0.014683951751976565, Test Acc: 0.6643835616438356\n",
      "Epoch 1015, Train_Loss: 0.012605447833266226, Test Acc: 0.6643835616438356\n",
      "Epoch 1016, Train_Loss: 0.014095338881361386, Test Acc: 0.6643835616438356\n",
      "Epoch 1017, Train_Loss: 0.0167951186585924, Test Acc: 0.6643835616438356\n",
      "Epoch 1018, Train_Loss: 0.01739741013625462, Test Acc: 0.6643835616438356\n",
      "Epoch 1019, Train_Loss: 0.016866715433934587, Test Acc: 0.6626712328767124\n",
      "Epoch 1020, Train_Loss: 0.013559537216679018, Test Acc: 0.660958904109589\n",
      "Epoch 1021, Train_Loss: 0.016145014306857774, Test Acc: 0.660958904109589\n",
      "Epoch 1022, Train_Loss: 0.01813803240293055, Test Acc: 0.6643835616438356\n",
      "Epoch 1023, Train_Loss: 0.017083841942621802, Test Acc: 0.6643835616438356\n",
      "Epoch 1024, Train_Loss: 0.01470769201296207, Test Acc: 0.6643835616438356\n",
      "Epoch 1025, Train_Loss: 0.01586756643428089, Test Acc: 0.6643835616438356\n",
      "Epoch 1026, Train_Loss: 0.021190995572396787, Test Acc: 0.6575342465753424\n",
      "Epoch 1027, Train_Loss: 0.01761042069256291, Test Acc: 0.6626712328767124\n",
      "Epoch 1028, Train_Loss: 0.013835332089456642, Test Acc: 0.6643835616438356\n",
      "Epoch 1029, Train_Loss: 0.0175709627528704, Test Acc: 0.666095890410959\n",
      "Epoch 1030, Train_Loss: 0.018850343735721253, Test Acc: 0.6643835616438356\n",
      "Epoch 1031, Train_Loss: 0.015023022103378025, Test Acc: 0.6626712328767124\n",
      "Epoch 1032, Train_Loss: 0.0154047241032913, Test Acc: 0.6626712328767124\n",
      "Epoch 1033, Train_Loss: 0.015409820251989004, Test Acc: 0.660958904109589\n",
      "Epoch 1034, Train_Loss: 0.0159966844776136, Test Acc: 0.6643835616438356\n",
      "Epoch 1035, Train_Loss: 0.014434513602282095, Test Acc: 0.6643835616438356\n",
      "Epoch 1036, Train_Loss: 0.017181935908411106, Test Acc: 0.6643835616438356\n",
      "Epoch 1037, Train_Loss: 0.016818872234125593, Test Acc: 0.6558219178082192\n",
      "Epoch 1038, Train_Loss: 0.017912463056745764, Test Acc: 0.6626712328767124\n",
      "Epoch 1039, Train_Loss: 0.012552730383958988, Test Acc: 0.6643835616438356\n",
      "Epoch 1040, Train_Loss: 0.01384854697425908, Test Acc: 0.6626712328767124\n",
      "Epoch 1041, Train_Loss: 0.020125533571444976, Test Acc: 0.6643835616438356\n",
      "Epoch 1042, Train_Loss: 0.014089932772549218, Test Acc: 0.6643835616438356\n",
      "Epoch 1043, Train_Loss: 0.013396152867244382, Test Acc: 0.6643835616438356\n",
      "Epoch 1044, Train_Loss: 0.015371936677638587, Test Acc: 0.6643835616438356\n",
      "Epoch 1045, Train_Loss: 0.017017077349009924, Test Acc: 0.6626712328767124\n",
      "Epoch 1046, Train_Loss: 0.014242522210224706, Test Acc: 0.6643835616438356\n",
      "Epoch 1047, Train_Loss: 0.0206526858912639, Test Acc: 0.6643835616438356\n",
      "Epoch 1048, Train_Loss: 0.015106837540770357, Test Acc: 0.6643835616438356\n",
      "Epoch 1049, Train_Loss: 0.013861506860394002, Test Acc: 0.6643835616438356\n",
      "Epoch 1050, Train_Loss: 0.014425749120164255, Test Acc: 0.6643835616438356\n",
      "Epoch 1051, Train_Loss: 0.016416426817158936, Test Acc: 0.660958904109589\n",
      "Epoch 1052, Train_Loss: 0.013252131976514647, Test Acc: 0.666095890410959\n",
      "Epoch 1053, Train_Loss: 0.017111045754063525, Test Acc: 0.6643835616438356\n",
      "Epoch 1054, Train_Loss: 0.01427441729720158, Test Acc: 0.6643835616438356\n",
      "Epoch 1055, Train_Loss: 0.013281625213494408, Test Acc: 0.6643835616438356\n",
      "Epoch 1056, Train_Loss: 0.014300801839453925, Test Acc: 0.6643835616438356\n",
      "Epoch 1057, Train_Loss: 0.014777442514969152, Test Acc: 0.6643835616438356\n",
      "Epoch 1058, Train_Loss: 0.014932463645436655, Test Acc: 0.666095890410959\n",
      "Epoch 1059, Train_Loss: 0.01704622057332017, Test Acc: 0.6643835616438356\n",
      "Epoch 1060, Train_Loss: 0.013795705164739047, Test Acc: 0.6643835616438356\n",
      "Epoch 1061, Train_Loss: 0.018027510917363543, Test Acc: 0.660958904109589\n",
      "Epoch 1062, Train_Loss: 0.014517777004130039, Test Acc: 0.6643835616438356\n",
      "Epoch 1063, Train_Loss: 0.01854475212712714, Test Acc: 0.6643835616438356\n",
      "Epoch 1064, Train_Loss: 0.01932684044368216, Test Acc: 0.6643835616438356\n",
      "Epoch 1065, Train_Loss: 0.013973017538319255, Test Acc: 0.6643835616438356\n",
      "Epoch 1066, Train_Loss: 0.015864945326484303, Test Acc: 0.6643835616438356\n",
      "Epoch 1067, Train_Loss: 0.013430665401756414, Test Acc: 0.6643835616438356\n",
      "Epoch 1068, Train_Loss: 0.01345165993552655, Test Acc: 0.6626712328767124\n",
      "Epoch 1069, Train_Loss: 0.014713070178004273, Test Acc: 0.6626712328767124\n",
      "Epoch 1070, Train_Loss: 0.016800013139800285, Test Acc: 0.6575342465753424\n",
      "Epoch 1071, Train_Loss: 0.013894605242967373, Test Acc: 0.6626712328767124\n",
      "Epoch 1072, Train_Loss: 0.013447230991914694, Test Acc: 0.6643835616438356\n",
      "Epoch 1073, Train_Loss: 0.015424406748024921, Test Acc: 0.6626712328767124\n",
      "Epoch 1074, Train_Loss: 0.01547047356098119, Test Acc: 0.6643835616438356\n",
      "Epoch 1075, Train_Loss: 0.015705075515143108, Test Acc: 0.6643835616438356\n",
      "Epoch 1076, Train_Loss: 0.014760248452148517, Test Acc: 0.666095890410959\n",
      "Epoch 1077, Train_Loss: 0.01629719915581518, Test Acc: 0.6643835616438356\n",
      "Epoch 1078, Train_Loss: 0.01389331366181068, Test Acc: 0.6626712328767124\n",
      "Epoch 1079, Train_Loss: 0.015418833414059918, Test Acc: 0.6626712328767124\n",
      "Epoch 1080, Train_Loss: 0.012457672013624688, Test Acc: 0.6626712328767124\n",
      "Epoch 1081, Train_Loss: 0.01316803171448555, Test Acc: 0.6643835616438356\n",
      "Epoch 1082, Train_Loss: 0.014687984425563627, Test Acc: 0.6643835616438356\n",
      "Epoch 1083, Train_Loss: 0.01607914794658427, Test Acc: 0.6541095890410958\n",
      "Epoch 1084, Train_Loss: 0.01674586207172979, Test Acc: 0.6643835616438356\n",
      "Epoch 1085, Train_Loss: 0.016514244768131903, Test Acc: 0.6678082191780822\n",
      "Epoch 1086, Train_Loss: 0.014232522059501207, Test Acc: 0.6626712328767124\n",
      "Epoch 1087, Train_Loss: 0.016235030268944683, Test Acc: 0.6643835616438356\n",
      "Epoch 1088, Train_Loss: 0.01356973554857177, Test Acc: 0.6643835616438356\n",
      "Epoch 1089, Train_Loss: 0.015503886598253303, Test Acc: 0.6643835616438356\n",
      "Epoch 1090, Train_Loss: 0.013351099797091592, Test Acc: 0.6626712328767124\n",
      "Epoch 1091, Train_Loss: 0.01651728746128356, Test Acc: 0.6643835616438356\n",
      "Epoch 1092, Train_Loss: 0.013739763533976657, Test Acc: 0.6643835616438356\n",
      "Epoch 1093, Train_Loss: 0.013608894986646192, Test Acc: 0.6643835616438356\n",
      "Epoch 1094, Train_Loss: 0.0187311072677403, Test Acc: 0.6643835616438356\n",
      "Epoch 1095, Train_Loss: 0.01415590645683551, Test Acc: 0.6643835616438356\n",
      "Epoch 1096, Train_Loss: 0.015052983762416261, Test Acc: 0.6643835616438356\n",
      "Epoch 1097, Train_Loss: 0.015668392564293754, Test Acc: 0.666095890410959\n",
      "Epoch 1098, Train_Loss: 0.013724259019454621, Test Acc: 0.660958904109589\n",
      "Epoch 1099, Train_Loss: 0.01612437376206799, Test Acc: 0.6626712328767124\n",
      "Epoch 1100, Train_Loss: 0.016272860298158776, Test Acc: 0.6626712328767124\n",
      "Epoch 1101, Train_Loss: 0.014350565068980359, Test Acc: 0.6626712328767124\n",
      "Epoch 1102, Train_Loss: 0.01970783215119809, Test Acc: 0.6626712328767124\n",
      "Epoch 1103, Train_Loss: 0.012705156500487647, Test Acc: 0.6592465753424658\n",
      "Epoch 1104, Train_Loss: 0.011274121641690726, Test Acc: 0.6626712328767124\n",
      "Epoch 1105, Train_Loss: 0.015047095833779167, Test Acc: 0.6626712328767124\n",
      "Epoch 1106, Train_Loss: 0.019298471363981662, Test Acc: 0.6558219178082192\n",
      "Epoch 1107, Train_Loss: 0.013318303409505461, Test Acc: 0.6626712328767124\n",
      "Epoch 1108, Train_Loss: 0.013340117292955256, Test Acc: 0.6626712328767124\n",
      "Epoch 1109, Train_Loss: 0.015809211718533334, Test Acc: 0.6626712328767124\n",
      "Epoch 1110, Train_Loss: 0.014093668947680271, Test Acc: 0.660958904109589\n",
      "Epoch 1111, Train_Loss: 0.012657709827180952, Test Acc: 0.6626712328767124\n",
      "Epoch 1112, Train_Loss: 0.015926071311241685, Test Acc: 0.6626712328767124\n",
      "Epoch 1113, Train_Loss: 0.013962656221337966, Test Acc: 0.6626712328767124\n",
      "Epoch 1114, Train_Loss: 0.014091048207319545, Test Acc: 0.6626712328767124\n",
      "Epoch 1115, Train_Loss: 0.012851541427608026, Test Acc: 0.6626712328767124\n",
      "Epoch 1116, Train_Loss: 0.01614533554220543, Test Acc: 0.660958904109589\n",
      "Epoch 1117, Train_Loss: 0.011710416887126485, Test Acc: 0.6626712328767124\n",
      "Epoch 1118, Train_Loss: 0.016038734333051252, Test Acc: 0.6626712328767124\n",
      "Epoch 1119, Train_Loss: 0.01542747883058837, Test Acc: 0.6643835616438356\n",
      "Epoch 1120, Train_Loss: 0.015483908036003413, Test Acc: 0.6626712328767124\n",
      "Epoch 1121, Train_Loss: 0.01576163149320564, Test Acc: 0.6575342465753424\n",
      "Epoch 1122, Train_Loss: 0.01373960726414225, Test Acc: 0.6626712328767124\n",
      "Epoch 1123, Train_Loss: 0.01610663068095164, Test Acc: 0.6626712328767124\n",
      "Epoch 1124, Train_Loss: 0.01578235664010208, Test Acc: 0.6626712328767124\n",
      "Epoch 1125, Train_Loss: 0.014962532314484633, Test Acc: 0.6626712328767124\n",
      "Epoch 1126, Train_Loss: 0.0150442668937103, Test Acc: 0.6643835616438356\n",
      "Epoch 1127, Train_Loss: 0.01636735884767404, Test Acc: 0.6626712328767124\n",
      "Epoch 1128, Train_Loss: 0.019477178072065726, Test Acc: 0.6626712328767124\n",
      "Epoch 1129, Train_Loss: 0.013229740190581651, Test Acc: 0.6626712328767124\n",
      "Epoch 1130, Train_Loss: 0.013509876058151349, Test Acc: 0.6626712328767124\n",
      "Epoch 1131, Train_Loss: 0.013146620880888804, Test Acc: 0.6626712328767124\n",
      "Epoch 1132, Train_Loss: 0.014758156684365531, Test Acc: 0.6626712328767124\n",
      "Epoch 1133, Train_Loss: 0.014100681592026376, Test Acc: 0.6643835616438356\n",
      "Epoch 1134, Train_Loss: 0.014215286275884864, Test Acc: 0.6643835616438356\n",
      "Epoch 1135, Train_Loss: 0.014251215718104504, Test Acc: 0.6626712328767124\n",
      "Epoch 1136, Train_Loss: 0.014093108145516453, Test Acc: 0.6592465753424658\n",
      "Epoch 1137, Train_Loss: 0.012770293603807659, Test Acc: 0.6626712328767124\n",
      "Epoch 1138, Train_Loss: 0.015497630738991575, Test Acc: 0.6626712328767124\n",
      "Epoch 1139, Train_Loss: 0.011745426517791202, Test Acc: 0.6626712328767124\n",
      "Epoch 1140, Train_Loss: 0.016051844922913006, Test Acc: 0.6626712328767124\n",
      "Epoch 1141, Train_Loss: 0.01383002222974028, Test Acc: 0.6626712328767124\n",
      "Epoch 1142, Train_Loss: 0.016990800910662074, Test Acc: 0.6626712328767124\n",
      "Epoch 1143, Train_Loss: 0.011935770140098612, Test Acc: 0.660958904109589\n",
      "Epoch 1144, Train_Loss: 0.013662558640135103, Test Acc: 0.6626712328767124\n",
      "Epoch 1145, Train_Loss: 0.01587716198173439, Test Acc: 0.6643835616438356\n",
      "Epoch 1146, Train_Loss: 0.013048975002220686, Test Acc: 0.6643835616438356\n",
      "Epoch 1147, Train_Loss: 0.01382182751876826, Test Acc: 0.660958904109589\n",
      "Epoch 1148, Train_Loss: 0.01569642281447159, Test Acc: 0.6575342465753424\n",
      "Epoch 1149, Train_Loss: 0.016049432709223765, Test Acc: 0.6643835616438356\n",
      "Epoch 1150, Train_Loss: 0.01217462934573632, Test Acc: 0.6626712328767124\n",
      "Epoch 1151, Train_Loss: 0.013545541024996055, Test Acc: 0.6643835616438356\n",
      "Epoch 1152, Train_Loss: 0.011774090552535199, Test Acc: 0.6626712328767124\n",
      "Epoch 1153, Train_Loss: 0.015013268814755065, Test Acc: 0.6592465753424658\n",
      "Epoch 1154, Train_Loss: 0.016438910138276697, Test Acc: 0.6575342465753424\n",
      "Epoch 1155, Train_Loss: 0.014876977492804144, Test Acc: 0.6575342465753424\n",
      "Epoch 1156, Train_Loss: 0.013047207323324983, Test Acc: 0.660958904109589\n",
      "Epoch 1157, Train_Loss: 0.013133335265138157, Test Acc: 0.6575342465753424\n",
      "Epoch 1158, Train_Loss: 0.011954504384448228, Test Acc: 0.6626712328767124\n",
      "Epoch 1159, Train_Loss: 0.012735313358234635, Test Acc: 0.6626712328767124\n",
      "Epoch 1160, Train_Loss: 0.012730238194308185, Test Acc: 0.6626712328767124\n",
      "Epoch 1161, Train_Loss: 0.011877409164299024, Test Acc: 0.6626712328767124\n",
      "Epoch 1162, Train_Loss: 0.01292580213294059, Test Acc: 0.6626712328767124\n",
      "Epoch 1163, Train_Loss: 0.015117780737455178, Test Acc: 0.6575342465753424\n",
      "Epoch 1164, Train_Loss: 0.01712325616608723, Test Acc: 0.6626712328767124\n",
      "Epoch 1165, Train_Loss: 0.01485492175879699, Test Acc: 0.6592465753424658\n",
      "Epoch 1166, Train_Loss: 0.012060256516861045, Test Acc: 0.6626712328767124\n",
      "Epoch 1167, Train_Loss: 0.011937951409890957, Test Acc: 0.6592465753424658\n",
      "Epoch 1168, Train_Loss: 0.014521019389121648, Test Acc: 0.660958904109589\n",
      "Epoch 1169, Train_Loss: 0.013118610760102456, Test Acc: 0.660958904109589\n",
      "Epoch 1170, Train_Loss: 0.015239516781548446, Test Acc: 0.6592465753424658\n",
      "Epoch 1171, Train_Loss: 0.01209859113987477, Test Acc: 0.660958904109589\n",
      "Epoch 1172, Train_Loss: 0.017259814043882216, Test Acc: 0.660958904109589\n",
      "Epoch 1173, Train_Loss: 0.013977453749248525, Test Acc: 0.6626712328767124\n",
      "Epoch 1174, Train_Loss: 0.010798356738405346, Test Acc: 0.6626712328767124\n",
      "Epoch 1175, Train_Loss: 0.01177536847421834, Test Acc: 0.6626712328767124\n",
      "Epoch 1176, Train_Loss: 0.017867392298285267, Test Acc: 0.6626712328767124\n",
      "Epoch 1177, Train_Loss: 0.011666805113691225, Test Acc: 0.660958904109589\n",
      "Epoch 1178, Train_Loss: 0.012850511003762222, Test Acc: 0.6626712328767124\n",
      "Epoch 1179, Train_Loss: 0.012757171547491453, Test Acc: 0.660958904109589\n",
      "Epoch 1180, Train_Loss: 0.016240395908425853, Test Acc: 0.6592465753424658\n",
      "Epoch 1181, Train_Loss: 0.015690477151565574, Test Acc: 0.6558219178082192\n",
      "Epoch 1182, Train_Loss: 0.01272214725122467, Test Acc: 0.6592465753424658\n",
      "Epoch 1183, Train_Loss: 0.013037323442858906, Test Acc: 0.660958904109589\n",
      "Epoch 1184, Train_Loss: 0.013173119972407221, Test Acc: 0.660958904109589\n",
      "Epoch 1185, Train_Loss: 0.01249006880016168, Test Acc: 0.660958904109589\n",
      "Epoch 1186, Train_Loss: 0.013686750900888, Test Acc: 0.660958904109589\n",
      "Epoch 1187, Train_Loss: 0.013739616742896033, Test Acc: 0.660958904109589\n",
      "Epoch 1188, Train_Loss: 0.01163440395202997, Test Acc: 0.660958904109589\n",
      "Epoch 1189, Train_Loss: 0.010763645957922563, Test Acc: 0.6575342465753424\n",
      "Epoch 1190, Train_Loss: 0.012804703570964193, Test Acc: 0.660958904109589\n",
      "Epoch 1191, Train_Loss: 0.013690189858380108, Test Acc: 0.6626712328767124\n",
      "Epoch 1192, Train_Loss: 0.011193777625067014, Test Acc: 0.6575342465753424\n",
      "Epoch 1193, Train_Loss: 0.014259224337365595, Test Acc: 0.6643835616438356\n",
      "Epoch 1194, Train_Loss: 0.015509547087731335, Test Acc: 0.6643835616438356\n",
      "Epoch 1195, Train_Loss: 0.013179552081965085, Test Acc: 0.6643835616438356\n",
      "Epoch 1196, Train_Loss: 0.01305332990750685, Test Acc: 0.6626712328767124\n",
      "Epoch 1197, Train_Loss: 0.012355722923985013, Test Acc: 0.6626712328767124\n",
      "Epoch 1198, Train_Loss: 0.010969567047141027, Test Acc: 0.6626712328767124\n",
      "Epoch 1199, Train_Loss: 0.01247853870529525, Test Acc: 0.6626712328767124\n",
      "Epoch 1200, Train_Loss: 0.012125316533456498, Test Acc: 0.6626712328767124\n",
      "Epoch 1201, Train_Loss: 0.017986917724556406, Test Acc: 0.6626712328767124\n",
      "Epoch 1202, Train_Loss: 0.013918780666244857, Test Acc: 0.660958904109589\n",
      "Epoch 1203, Train_Loss: 0.01297605083527742, Test Acc: 0.660958904109589\n",
      "Epoch 1204, Train_Loss: 0.012957631856806984, Test Acc: 0.6626712328767124\n",
      "Epoch 1205, Train_Loss: 0.012460747494060342, Test Acc: 0.6626712328767124\n",
      "Epoch 1206, Train_Loss: 0.015662166333186178, Test Acc: 0.6626712328767124\n",
      "Epoch 1207, Train_Loss: 0.013357052212768394, Test Acc: 0.6626712328767124\n",
      "Epoch 1208, Train_Loss: 0.010932628363661934, Test Acc: 0.660958904109589\n",
      "Epoch 1209, Train_Loss: 0.013478095181199024, Test Acc: 0.6626712328767124\n",
      "Epoch 1210, Train_Loss: 0.012215775581125854, Test Acc: 0.6626712328767124\n",
      "Epoch 1211, Train_Loss: 0.011018437926395563, Test Acc: 0.660958904109589\n",
      "Epoch 1212, Train_Loss: 0.012392191276376252, Test Acc: 0.6626712328767124\n",
      "Epoch 1213, Train_Loss: 0.01283898694509844, Test Acc: 0.6626712328767124\n",
      "Epoch 1214, Train_Loss: 0.013225210585005698, Test Acc: 0.6626712328767124\n",
      "Epoch 1215, Train_Loss: 0.010555859883879748, Test Acc: 0.6626712328767124\n",
      "Epoch 1216, Train_Loss: 0.015438075513884542, Test Acc: 0.6626712328767124\n",
      "Epoch 1217, Train_Loss: 0.010186361023897916, Test Acc: 0.6626712328767124\n",
      "Epoch 1218, Train_Loss: 0.011662045084449346, Test Acc: 0.6626712328767124\n",
      "Epoch 1219, Train_Loss: 0.015425276524524634, Test Acc: 0.6592465753424658\n",
      "Epoch 1220, Train_Loss: 0.010905513549005263, Test Acc: 0.6626712328767124\n",
      "Epoch 1221, Train_Loss: 0.011594473147852113, Test Acc: 0.6626712328767124\n",
      "Epoch 1222, Train_Loss: 0.012301418913921225, Test Acc: 0.6626712328767124\n",
      "Epoch 1223, Train_Loss: 0.01286928704030288, Test Acc: 0.660958904109589\n",
      "Epoch 1224, Train_Loss: 0.014273735943334032, Test Acc: 0.660958904109589\n",
      "Epoch 1225, Train_Loss: 0.009471138039543803, Test Acc: 0.6626712328767124\n",
      "Epoch 1226, Train_Loss: 0.01091132634155656, Test Acc: 0.660958904109589\n",
      "Epoch 1227, Train_Loss: 0.013769324717941345, Test Acc: 0.6626712328767124\n",
      "Epoch 1228, Train_Loss: 0.011576656109809846, Test Acc: 0.6626712328767124\n",
      "Epoch 1229, Train_Loss: 0.00987360071690091, Test Acc: 0.6626712328767124\n",
      "Epoch 1230, Train_Loss: 0.012802976686543843, Test Acc: 0.6592465753424658\n",
      "Epoch 1231, Train_Loss: 0.016137561446157633, Test Acc: 0.6592465753424658\n",
      "Epoch 1232, Train_Loss: 0.012398692359511188, Test Acc: 0.6643835616438356\n",
      "Epoch 1233, Train_Loss: 0.010665846088159014, Test Acc: 0.6643835616438356\n",
      "Epoch 1234, Train_Loss: 0.013420649204817892, Test Acc: 0.6626712328767124\n",
      "Epoch 1235, Train_Loss: 0.012550441911344024, Test Acc: 0.6626712328767124\n",
      "Epoch 1236, Train_Loss: 0.013776659523159651, Test Acc: 0.666095890410959\n",
      "Epoch 1237, Train_Loss: 0.013555376017848175, Test Acc: 0.660958904109589\n",
      "Epoch 1238, Train_Loss: 0.011619315980169631, Test Acc: 0.6626712328767124\n",
      "Epoch 1239, Train_Loss: 0.010581754450413428, Test Acc: 0.6626712328767124\n",
      "Epoch 1240, Train_Loss: 0.014970664138672873, Test Acc: 0.660958904109589\n",
      "Epoch 1241, Train_Loss: 0.014864814158045192, Test Acc: 0.666095890410959\n",
      "Epoch 1242, Train_Loss: 0.01653664310424574, Test Acc: 0.6695205479452054\n",
      "Epoch 1243, Train_Loss: 0.013142715401500027, Test Acc: 0.6592465753424658\n",
      "Epoch 1244, Train_Loss: 0.015121074757644237, Test Acc: 0.6541095890410958\n",
      "Epoch 1245, Train_Loss: 0.011597145001815079, Test Acc: 0.660958904109589\n",
      "Epoch 1246, Train_Loss: 0.01282056298941825, Test Acc: 0.660958904109589\n",
      "Epoch 1247, Train_Loss: 0.01393078118462654, Test Acc: 0.660958904109589\n",
      "Epoch 1248, Train_Loss: 0.010450694348492107, Test Acc: 0.660958904109589\n",
      "Epoch 1249, Train_Loss: 0.012958144071262723, Test Acc: 0.660958904109589\n",
      "Epoch 1250, Train_Loss: 0.01038903486255549, Test Acc: 0.660958904109589\n",
      "Epoch 1251, Train_Loss: 0.012458029425033601, Test Acc: 0.6592465753424658\n",
      "Epoch 1252, Train_Loss: 0.014073274011252579, Test Acc: 0.6643835616438356\n",
      "Epoch 1253, Train_Loss: 0.013030108620114333, Test Acc: 0.6592465753424658\n",
      "Epoch 1254, Train_Loss: 0.011029094610421453, Test Acc: 0.6626712328767124\n",
      "Epoch 1255, Train_Loss: 0.01189538570656623, Test Acc: 0.6592465753424658\n",
      "Epoch 1256, Train_Loss: 0.010752898304417613, Test Acc: 0.660958904109589\n",
      "Epoch 1257, Train_Loss: 0.014621020762206172, Test Acc: 0.660958904109589\n",
      "Epoch 1258, Train_Loss: 0.012087320844329952, Test Acc: 0.660958904109589\n",
      "Epoch 1259, Train_Loss: 0.01157258236889902, Test Acc: 0.6592465753424658\n",
      "Epoch 1260, Train_Loss: 0.011713252627941984, Test Acc: 0.6558219178082192\n",
      "Epoch 1261, Train_Loss: 0.013872420233496996, Test Acc: 0.6626712328767124\n",
      "Epoch 1262, Train_Loss: 0.01095161810189893, Test Acc: 0.6592465753424658\n",
      "Epoch 1263, Train_Loss: 0.00880456139202579, Test Acc: 0.660958904109589\n",
      "Epoch 1264, Train_Loss: 0.01541376695740837, Test Acc: 0.6541095890410958\n",
      "Epoch 1265, Train_Loss: 0.01133422517773397, Test Acc: 0.660958904109589\n",
      "Epoch 1266, Train_Loss: 0.013586236950686725, Test Acc: 0.660958904109589\n",
      "Epoch 1267, Train_Loss: 0.010396302604931407, Test Acc: 0.660958904109589\n",
      "Epoch 1268, Train_Loss: 0.012903107787224144, Test Acc: 0.660958904109589\n",
      "Epoch 1269, Train_Loss: 0.01065022928446524, Test Acc: 0.660958904109589\n",
      "Epoch 1270, Train_Loss: 0.013729471996157372, Test Acc: 0.6575342465753424\n",
      "Epoch 1271, Train_Loss: 0.010575533526207437, Test Acc: 0.6575342465753424\n",
      "Epoch 1272, Train_Loss: 0.011427300918057881, Test Acc: 0.6626712328767124\n",
      "Epoch 1273, Train_Loss: 0.011838249891297892, Test Acc: 0.6626712328767124\n",
      "Epoch 1274, Train_Loss: 0.012358418582152808, Test Acc: 0.6592465753424658\n",
      "Epoch 1275, Train_Loss: 0.011091563187164866, Test Acc: 0.6592465753424658\n",
      "Epoch 1276, Train_Loss: 0.010440896103318664, Test Acc: 0.6643835616438356\n",
      "Epoch 1277, Train_Loss: 0.01307613523749751, Test Acc: 0.6626712328767124\n",
      "Epoch 1278, Train_Loss: 0.013087690580960043, Test Acc: 0.6575342465753424\n",
      "Epoch 1279, Train_Loss: 0.015181324433797272, Test Acc: 0.6575342465753424\n",
      "Epoch 1280, Train_Loss: 0.011459274829576316, Test Acc: 0.6592465753424658\n",
      "Epoch 1281, Train_Loss: 0.011423378997278633, Test Acc: 0.6592465753424658\n",
      "Epoch 1282, Train_Loss: 0.010806478060658264, Test Acc: 0.6592465753424658\n",
      "Epoch 1283, Train_Loss: 0.01112635339541157, Test Acc: 0.6592465753424658\n",
      "Epoch 1284, Train_Loss: 0.013807625194203865, Test Acc: 0.6592465753424658\n",
      "Epoch 1285, Train_Loss: 0.014328194614790846, Test Acc: 0.660958904109589\n",
      "Epoch 1286, Train_Loss: 0.01109169091887452, Test Acc: 0.660958904109589\n",
      "Epoch 1287, Train_Loss: 0.013496323309936997, Test Acc: 0.660958904109589\n",
      "Epoch 1288, Train_Loss: 0.014181410936544125, Test Acc: 0.6592465753424658\n",
      "Epoch 1289, Train_Loss: 0.010251121483634051, Test Acc: 0.6626712328767124\n",
      "Epoch 1290, Train_Loss: 0.013315425640485046, Test Acc: 0.660958904109589\n",
      "Epoch 1291, Train_Loss: 0.012082015516625688, Test Acc: 0.6592465753424658\n",
      "Epoch 1292, Train_Loss: 0.014712966515617154, Test Acc: 0.6575342465753424\n",
      "Epoch 1293, Train_Loss: 0.011627353000221774, Test Acc: 0.6592465753424658\n",
      "Epoch 1294, Train_Loss: 0.01208209382411951, Test Acc: 0.6626712328767124\n",
      "Epoch 1295, Train_Loss: 0.012826429274809925, Test Acc: 0.6592465753424658\n",
      "Epoch 1296, Train_Loss: 0.01124812594866853, Test Acc: 0.6626712328767124\n",
      "Epoch 1297, Train_Loss: 0.010715713708577823, Test Acc: 0.6626712328767124\n",
      "Epoch 1298, Train_Loss: 0.01058858630767645, Test Acc: 0.6626712328767124\n",
      "Epoch 1299, Train_Loss: 0.010740875746932943, Test Acc: 0.6626712328767124\n",
      "Epoch 1300, Train_Loss: 0.011994983576869345, Test Acc: 0.6592465753424658\n",
      "Epoch 1301, Train_Loss: 0.013455778890147485, Test Acc: 0.6626712328767124\n",
      "Epoch 1302, Train_Loss: 0.009244952433164144, Test Acc: 0.660958904109589\n",
      "Epoch 1303, Train_Loss: 0.01085662237028373, Test Acc: 0.6592465753424658\n",
      "Epoch 1304, Train_Loss: 0.010172327305099316, Test Acc: 0.660958904109589\n",
      "Epoch 1305, Train_Loss: 0.010460344541570521, Test Acc: 0.6626712328767124\n",
      "Epoch 1306, Train_Loss: 0.012274580382836575, Test Acc: 0.660958904109589\n",
      "Epoch 1307, Train_Loss: 0.010471230717143953, Test Acc: 0.660958904109589\n",
      "Epoch 1308, Train_Loss: 0.011617811109772447, Test Acc: 0.660958904109589\n",
      "Epoch 1309, Train_Loss: 0.012857016751695483, Test Acc: 0.6678082191780822\n",
      "Epoch 1310, Train_Loss: 0.01214716975164265, Test Acc: 0.660958904109589\n",
      "Epoch 1311, Train_Loss: 0.01137448873168978, Test Acc: 0.6592465753424658\n",
      "Epoch 1312, Train_Loss: 0.01260970491352964, Test Acc: 0.660958904109589\n",
      "Epoch 1313, Train_Loss: 0.012066424397744413, Test Acc: 0.6626712328767124\n",
      "Epoch 1314, Train_Loss: 0.01050342480539257, Test Acc: 0.6626712328767124\n",
      "Epoch 1315, Train_Loss: 0.012569772196002305, Test Acc: 0.6592465753424658\n",
      "Epoch 1316, Train_Loss: 0.013912222963881504, Test Acc: 0.6592465753424658\n",
      "Epoch 1317, Train_Loss: 0.012648788319438609, Test Acc: 0.660958904109589\n",
      "Epoch 1318, Train_Loss: 0.010432041374770051, Test Acc: 0.6592465753424658\n",
      "Epoch 1319, Train_Loss: 0.012117588412820623, Test Acc: 0.660958904109589\n",
      "Epoch 1320, Train_Loss: 0.012027323823531333, Test Acc: 0.6592465753424658\n",
      "Epoch 1321, Train_Loss: 0.008942433740685374, Test Acc: 0.6626712328767124\n",
      "Epoch 1322, Train_Loss: 0.01836997137183971, Test Acc: 0.6592465753424658\n",
      "Epoch 1323, Train_Loss: 0.011066074189329811, Test Acc: 0.6592465753424658\n",
      "Epoch 1324, Train_Loss: 0.013016598385092948, Test Acc: 0.6592465753424658\n",
      "Epoch 1325, Train_Loss: 0.013103976704769593, Test Acc: 0.6592465753424658\n",
      "Epoch 1326, Train_Loss: 0.012087434489330917, Test Acc: 0.6592465753424658\n",
      "Epoch 1327, Train_Loss: 0.011526964729910105, Test Acc: 0.6626712328767124\n",
      "Epoch 1328, Train_Loss: 0.013776385737401142, Test Acc: 0.6592465753424658\n",
      "Epoch 1329, Train_Loss: 0.011218179906336445, Test Acc: 0.660958904109589\n",
      "Epoch 1330, Train_Loss: 0.0112293653269262, Test Acc: 0.6592465753424658\n",
      "Epoch 1331, Train_Loss: 0.011813134817657556, Test Acc: 0.6592465753424658\n",
      "Epoch 1332, Train_Loss: 0.010276747534362585, Test Acc: 0.6592465753424658\n",
      "Epoch 1333, Train_Loss: 0.010983287128965458, Test Acc: 0.6592465753424658\n",
      "Epoch 1334, Train_Loss: 0.010764859687014905, Test Acc: 0.660958904109589\n",
      "Epoch 1335, Train_Loss: 0.012435227933110582, Test Acc: 0.660958904109589\n",
      "Epoch 1336, Train_Loss: 0.011485270708362805, Test Acc: 0.660958904109589\n",
      "Epoch 1337, Train_Loss: 0.0086617661299897, Test Acc: 0.660958904109589\n",
      "Epoch 1338, Train_Loss: 0.012794889898486872, Test Acc: 0.660958904109589\n",
      "Epoch 1339, Train_Loss: 0.01197380863163744, Test Acc: 0.660958904109589\n",
      "Epoch 1340, Train_Loss: 0.010733141201853869, Test Acc: 0.660958904109589\n",
      "Epoch 1341, Train_Loss: 0.01342493936590472, Test Acc: 0.660958904109589\n",
      "Epoch 1342, Train_Loss: 0.010996629046076123, Test Acc: 0.660958904109589\n",
      "Epoch 1343, Train_Loss: 0.013486903065313527, Test Acc: 0.660958904109589\n",
      "Epoch 1344, Train_Loss: 0.009547193358230288, Test Acc: 0.6575342465753424\n",
      "Epoch 1345, Train_Loss: 0.010708580270602397, Test Acc: 0.660958904109589\n",
      "Epoch 1346, Train_Loss: 0.009331739819117502, Test Acc: 0.660958904109589\n",
      "Epoch 1347, Train_Loss: 0.010855486717446183, Test Acc: 0.660958904109589\n",
      "Epoch 1348, Train_Loss: 0.0116906728419508, Test Acc: 0.6592465753424658\n",
      "Epoch 1349, Train_Loss: 0.013515935482701025, Test Acc: 0.660958904109589\n",
      "Epoch 1350, Train_Loss: 0.010457389265866368, Test Acc: 0.660958904109589\n",
      "Epoch 1351, Train_Loss: 0.01070622357758566, Test Acc: 0.660958904109589\n",
      "Epoch 1352, Train_Loss: 0.009152395510682254, Test Acc: 0.660958904109589\n",
      "Epoch 1353, Train_Loss: 0.01023502615680627, Test Acc: 0.6592465753424658\n",
      "Epoch 1354, Train_Loss: 0.013002442150195748, Test Acc: 0.6592465753424658\n",
      "Epoch 1355, Train_Loss: 0.009632674513341044, Test Acc: 0.660958904109589\n",
      "Epoch 1356, Train_Loss: 0.010086665595281374, Test Acc: 0.6575342465753424\n",
      "Epoch 1357, Train_Loss: 0.009388713926455239, Test Acc: 0.660958904109589\n",
      "Epoch 1358, Train_Loss: 0.012474629953658223, Test Acc: 0.660958904109589\n",
      "Epoch 1359, Train_Loss: 0.011986453347390125, Test Acc: 0.660958904109589\n",
      "Epoch 1360, Train_Loss: 0.01056905665927843, Test Acc: 0.660958904109589\n",
      "Epoch 1361, Train_Loss: 0.010202623773238884, Test Acc: 0.660958904109589\n",
      "Epoch 1362, Train_Loss: 0.012062022838108533, Test Acc: 0.660958904109589\n",
      "Epoch 1363, Train_Loss: 0.012597412852301204, Test Acc: 0.6626712328767124\n",
      "Epoch 1364, Train_Loss: 0.010267289989201345, Test Acc: 0.660958904109589\n",
      "Epoch 1365, Train_Loss: 0.013256716513751599, Test Acc: 0.660958904109589\n",
      "Epoch 1366, Train_Loss: 0.009620594124498894, Test Acc: 0.660958904109589\n",
      "Epoch 1367, Train_Loss: 0.010138637746422319, Test Acc: 0.660958904109589\n",
      "Epoch 1368, Train_Loss: 0.015071547090428794, Test Acc: 0.6575342465753424\n",
      "Epoch 1369, Train_Loss: 0.011893480156231817, Test Acc: 0.660958904109589\n",
      "Epoch 1370, Train_Loss: 0.013455927279665048, Test Acc: 0.6643835616438356\n",
      "Epoch 1371, Train_Loss: 0.00815354450992345, Test Acc: 0.660958904109589\n",
      "Epoch 1372, Train_Loss: 0.010099775135131495, Test Acc: 0.660958904109589\n",
      "Epoch 1373, Train_Loss: 0.009609545298644662, Test Acc: 0.660958904109589\n",
      "Epoch 1374, Train_Loss: 0.00865800523206417, Test Acc: 0.660958904109589\n",
      "Epoch 1375, Train_Loss: 0.008951776389039878, Test Acc: 0.6626712328767124\n",
      "Epoch 1376, Train_Loss: 0.011184824636529811, Test Acc: 0.6626712328767124\n",
      "Epoch 1377, Train_Loss: 0.011748386905310326, Test Acc: 0.6626712328767124\n",
      "Epoch 1378, Train_Loss: 0.009687795614809147, Test Acc: 0.6626712328767124\n",
      "Epoch 1379, Train_Loss: 0.008345317634393723, Test Acc: 0.6626712328767124\n",
      "Epoch 1380, Train_Loss: 0.012917094521867512, Test Acc: 0.660958904109589\n",
      "Epoch 1381, Train_Loss: 0.010658104872618424, Test Acc: 0.6626712328767124\n",
      "Epoch 1382, Train_Loss: 0.009302052364546398, Test Acc: 0.660958904109589\n",
      "Epoch 1383, Train_Loss: 0.007894593071796407, Test Acc: 0.660958904109589\n",
      "Epoch 1384, Train_Loss: 0.010145900451789203, Test Acc: 0.6626712328767124\n",
      "Epoch 1385, Train_Loss: 0.010683423927275726, Test Acc: 0.660958904109589\n",
      "Epoch 1386, Train_Loss: 0.01050407598449965, Test Acc: 0.6626712328767124\n",
      "Epoch 1387, Train_Loss: 0.010109209543998077, Test Acc: 0.660958904109589\n",
      "Epoch 1388, Train_Loss: 0.008660089122713543, Test Acc: 0.6626712328767124\n",
      "Epoch 1389, Train_Loss: 0.008830039882013807, Test Acc: 0.660958904109589\n",
      "Epoch 1390, Train_Loss: 0.009030516253233145, Test Acc: 0.6626712328767124\n",
      "Epoch 1391, Train_Loss: 0.010359942141008105, Test Acc: 0.6626712328767124\n",
      "Epoch 1392, Train_Loss: 0.011101450590103923, Test Acc: 0.660958904109589\n",
      "Epoch 1393, Train_Loss: 0.010018061502705677, Test Acc: 0.660958904109589\n",
      "Epoch 1394, Train_Loss: 0.011245940722801606, Test Acc: 0.660958904109589\n",
      "Epoch 1395, Train_Loss: 0.012046393884702411, Test Acc: 0.660958904109589\n",
      "Epoch 1396, Train_Loss: 0.009927798870648985, Test Acc: 0.6643835616438356\n",
      "Epoch 1397, Train_Loss: 0.0113209736191493, Test Acc: 0.660958904109589\n",
      "Epoch 1398, Train_Loss: 0.012330892199315713, Test Acc: 0.666095890410959\n",
      "Epoch 1399, Train_Loss: 0.012251696125531453, Test Acc: 0.660958904109589\n",
      "Epoch 1400, Train_Loss: 0.011052068383378355, Test Acc: 0.660958904109589\n",
      "Epoch 1401, Train_Loss: 0.012286867740385787, Test Acc: 0.6695205479452054\n",
      "Epoch 1402, Train_Loss: 0.01183320512973296, Test Acc: 0.6575342465753424\n",
      "Epoch 1403, Train_Loss: 0.011309025446735177, Test Acc: 0.660958904109589\n",
      "Epoch 1404, Train_Loss: 0.01069721982912597, Test Acc: 0.660958904109589\n",
      "Epoch 1405, Train_Loss: 0.008535559948313676, Test Acc: 0.660958904109589\n",
      "Epoch 1406, Train_Loss: 0.009092784318909253, Test Acc: 0.660958904109589\n",
      "Epoch 1407, Train_Loss: 0.009508148491477186, Test Acc: 0.660958904109589\n",
      "Epoch 1408, Train_Loss: 0.01025402786876839, Test Acc: 0.660958904109589\n",
      "Epoch 1409, Train_Loss: 0.01107090889945539, Test Acc: 0.660958904109589\n",
      "Epoch 1410, Train_Loss: 0.008508153337288604, Test Acc: 0.6575342465753424\n",
      "Epoch 1411, Train_Loss: 0.01105148781061871, Test Acc: 0.6626712328767124\n",
      "Epoch 1412, Train_Loss: 0.009909000700645265, Test Acc: 0.6626712328767124\n",
      "Epoch 1413, Train_Loss: 0.010688975376524468, Test Acc: 0.6678082191780822\n",
      "Epoch 1414, Train_Loss: 0.010558854550026808, Test Acc: 0.6575342465753424\n",
      "Epoch 1415, Train_Loss: 0.010848131896182167, Test Acc: 0.6626712328767124\n",
      "Epoch 1416, Train_Loss: 0.010954418191431614, Test Acc: 0.6643835616438356\n",
      "Epoch 1417, Train_Loss: 0.009367956031383073, Test Acc: 0.6643835616438356\n",
      "Epoch 1418, Train_Loss: 0.010710655237971878, Test Acc: 0.6678082191780822\n",
      "Epoch 1419, Train_Loss: 0.011515153739992456, Test Acc: 0.660958904109589\n",
      "Epoch 1420, Train_Loss: 0.009337487992070237, Test Acc: 0.660958904109589\n",
      "Epoch 1421, Train_Loss: 0.009505635352297759, Test Acc: 0.6592465753424658\n",
      "Epoch 1422, Train_Loss: 0.011945166925897865, Test Acc: 0.660958904109589\n",
      "Epoch 1423, Train_Loss: 0.011961067182255647, Test Acc: 0.660958904109589\n",
      "Epoch 1424, Train_Loss: 0.00972306162930181, Test Acc: 0.6626712328767124\n",
      "Epoch 1425, Train_Loss: 0.011643459349670593, Test Acc: 0.660958904109589\n",
      "Epoch 1426, Train_Loss: 0.009991238170641736, Test Acc: 0.6626712328767124\n",
      "Epoch 1427, Train_Loss: 0.008879065295559485, Test Acc: 0.6643835616438356\n",
      "Epoch 1428, Train_Loss: 0.010396058967444333, Test Acc: 0.6592465753424658\n",
      "Epoch 1429, Train_Loss: 0.0092387106442402, Test Acc: 0.6592465753424658\n",
      "Epoch 1430, Train_Loss: 0.010181179150094977, Test Acc: 0.6575342465753424\n",
      "Epoch 1431, Train_Loss: 0.010707360259402776, Test Acc: 0.660958904109589\n",
      "Epoch 1432, Train_Loss: 0.008363413008737552, Test Acc: 0.660958904109589\n",
      "Epoch 1433, Train_Loss: 0.011164465317051508, Test Acc: 0.6575342465753424\n",
      "Epoch 1434, Train_Loss: 0.011468150321434223, Test Acc: 0.660958904109589\n",
      "Epoch 1435, Train_Loss: 0.010640755601343699, Test Acc: 0.6592465753424658\n",
      "Epoch 1436, Train_Loss: 0.008254968179471689, Test Acc: 0.6643835616438356\n",
      "Epoch 1437, Train_Loss: 0.010070653059756296, Test Acc: 0.660958904109589\n",
      "Epoch 1438, Train_Loss: 0.015104794619446693, Test Acc: 0.6678082191780822\n",
      "Epoch 1439, Train_Loss: 0.011531496366842475, Test Acc: 0.6575342465753424\n",
      "Epoch 1440, Train_Loss: 0.009616783958563246, Test Acc: 0.6626712328767124\n",
      "Epoch 1441, Train_Loss: 0.009380144614169694, Test Acc: 0.660958904109589\n",
      "Epoch 1442, Train_Loss: 0.012075578395524644, Test Acc: 0.6626712328767124\n",
      "Epoch 1443, Train_Loss: 0.010012208563239255, Test Acc: 0.6626712328767124\n",
      "Epoch 1444, Train_Loss: 0.008822600636449351, Test Acc: 0.666095890410959\n",
      "Epoch 1445, Train_Loss: 0.011169515842084365, Test Acc: 0.6626712328767124\n",
      "Epoch 1446, Train_Loss: 0.009189198824969935, Test Acc: 0.6626712328767124\n",
      "Epoch 1447, Train_Loss: 0.010449181298099575, Test Acc: 0.660958904109589\n",
      "Epoch 1448, Train_Loss: 0.008134575308304193, Test Acc: 0.660958904109589\n",
      "Epoch 1449, Train_Loss: 0.011211163971211136, Test Acc: 0.660958904109589\n",
      "Epoch 1450, Train_Loss: 0.009624866737112825, Test Acc: 0.6626712328767124\n",
      "Epoch 1451, Train_Loss: 0.011200095407048138, Test Acc: 0.6678082191780822\n",
      "Epoch 1452, Train_Loss: 0.011246432690541042, Test Acc: 0.660958904109589\n",
      "Epoch 1453, Train_Loss: 0.011371772124221025, Test Acc: 0.6626712328767124\n",
      "Epoch 1454, Train_Loss: 0.011050936313040438, Test Acc: 0.660958904109589\n",
      "Epoch 1455, Train_Loss: 0.010868223957004375, Test Acc: 0.6626712328767124\n",
      "Epoch 1456, Train_Loss: 0.008718790006696508, Test Acc: 0.6626712328767124\n",
      "Epoch 1457, Train_Loss: 0.008524977245542686, Test Acc: 0.660958904109589\n",
      "Epoch 1458, Train_Loss: 0.010734875167599967, Test Acc: 0.6626712328767124\n",
      "Epoch 1459, Train_Loss: 0.012260370167496148, Test Acc: 0.6626712328767124\n",
      "Epoch 1460, Train_Loss: 0.011106371300229512, Test Acc: 0.6626712328767124\n",
      "Epoch 1461, Train_Loss: 0.012739204205900023, Test Acc: 0.660958904109589\n",
      "Epoch 1462, Train_Loss: 0.008704622851837485, Test Acc: 0.660958904109589\n",
      "Epoch 1463, Train_Loss: 0.009563975427340665, Test Acc: 0.6626712328767124\n",
      "Epoch 1464, Train_Loss: 0.012609200327005965, Test Acc: 0.660958904109589\n",
      "Epoch 1465, Train_Loss: 0.009625085332118033, Test Acc: 0.660958904109589\n",
      "Epoch 1466, Train_Loss: 0.009859761853022064, Test Acc: 0.660958904109589\n",
      "Epoch 1467, Train_Loss: 0.011711939656834147, Test Acc: 0.6626712328767124\n",
      "Epoch 1468, Train_Loss: 0.009438876566491672, Test Acc: 0.660958904109589\n",
      "Epoch 1469, Train_Loss: 0.012112178395000228, Test Acc: 0.6626712328767124\n",
      "Epoch 1470, Train_Loss: 0.009981097201944067, Test Acc: 0.660958904109589\n",
      "Epoch 1471, Train_Loss: 0.008448897706216485, Test Acc: 0.660958904109589\n",
      "Epoch 1472, Train_Loss: 0.010647310024978651, Test Acc: 0.660958904109589\n",
      "Epoch 1473, Train_Loss: 0.009745929208747839, Test Acc: 0.660958904109589\n",
      "Epoch 1474, Train_Loss: 0.010360660255400944, Test Acc: 0.660958904109589\n",
      "Epoch 1475, Train_Loss: 0.00839810909269545, Test Acc: 0.6626712328767124\n",
      "Epoch 1476, Train_Loss: 0.010375397228017391, Test Acc: 0.660958904109589\n",
      "Epoch 1477, Train_Loss: 0.01126397617076691, Test Acc: 0.6626712328767124\n",
      "Epoch 1478, Train_Loss: 0.010163200357965252, Test Acc: 0.660958904109589\n",
      "Epoch 1479, Train_Loss: 0.011901974218744726, Test Acc: 0.660958904109589\n",
      "Epoch 1480, Train_Loss: 0.009240950313937901, Test Acc: 0.6626712328767124\n",
      "Epoch 1481, Train_Loss: 0.008007121692799046, Test Acc: 0.660958904109589\n",
      "Epoch 1482, Train_Loss: 0.00836845577305212, Test Acc: 0.660958904109589\n",
      "Epoch 1483, Train_Loss: 0.009033103094225226, Test Acc: 0.6626712328767124\n",
      "Epoch 1484, Train_Loss: 0.00893753455238766, Test Acc: 0.660958904109589\n",
      "Epoch 1485, Train_Loss: 0.008962725751189282, Test Acc: 0.660958904109589\n",
      "Epoch 1486, Train_Loss: 0.012263894793250074, Test Acc: 0.6626712328767124\n",
      "Epoch 1487, Train_Loss: 0.00993247474070813, Test Acc: 0.660958904109589\n",
      "Epoch 1488, Train_Loss: 0.008440319542387442, Test Acc: 0.660958904109589\n",
      "Epoch 1489, Train_Loss: 0.009151498964229177, Test Acc: 0.660958904109589\n",
      "Epoch 1490, Train_Loss: 0.01082985050197749, Test Acc: 0.6626712328767124\n",
      "Epoch 1491, Train_Loss: 0.009315396795045672, Test Acc: 0.660958904109589\n",
      "Epoch 1492, Train_Loss: 0.008373596392630134, Test Acc: 0.660958904109589\n",
      "Epoch 1493, Train_Loss: 0.01164028319794852, Test Acc: 0.660958904109589\n",
      "Epoch 1494, Train_Loss: 0.011165244636686111, Test Acc: 0.6626712328767124\n",
      "Epoch 1495, Train_Loss: 0.00864982264374703, Test Acc: 0.660958904109589\n",
      "Epoch 1496, Train_Loss: 0.008265327762273955, Test Acc: 0.6695205479452054\n",
      "Epoch 1497, Train_Loss: 0.009754850225590417, Test Acc: 0.660958904109589\n",
      "Epoch 1498, Train_Loss: 0.010067709607483266, Test Acc: 0.6575342465753424\n",
      "Epoch 1499, Train_Loss: 0.01013521757113267, Test Acc: 0.660958904109589\n",
      "Epoch 1500, Train_Loss: 0.00971395163287525, Test Acc: 0.6626712328767124\n",
      "Epoch 1501, Train_Loss: 0.009946998516625172, Test Acc: 0.666095890410959\n",
      "Epoch 1502, Train_Loss: 0.011016665804390868, Test Acc: 0.6626712328767124\n",
      "Epoch 1503, Train_Loss: 0.008957278536399826, Test Acc: 0.6626712328767124\n",
      "Epoch 1504, Train_Loss: 0.010357964328932212, Test Acc: 0.6626712328767124\n",
      "Epoch 1505, Train_Loss: 0.008736029047668126, Test Acc: 0.660958904109589\n",
      "Epoch 1506, Train_Loss: 0.011982472273302847, Test Acc: 0.6626712328767124\n",
      "Epoch 1507, Train_Loss: 0.008738339708315834, Test Acc: 0.660958904109589\n",
      "Epoch 1508, Train_Loss: 0.008821562161074326, Test Acc: 0.6626712328767124\n",
      "Epoch 1509, Train_Loss: 0.009510725218206062, Test Acc: 0.6712328767123288\n",
      "Epoch 1510, Train_Loss: 0.011753356784538482, Test Acc: 0.660958904109589\n",
      "Epoch 1511, Train_Loss: 0.008784989707919522, Test Acc: 0.6626712328767124\n",
      "Epoch 1512, Train_Loss: 0.008894390063915125, Test Acc: 0.6592465753424658\n",
      "Epoch 1513, Train_Loss: 0.00900469440557572, Test Acc: 0.666095890410959\n",
      "Epoch 1514, Train_Loss: 0.00869067164990156, Test Acc: 0.660958904109589\n",
      "Epoch 1515, Train_Loss: 0.008833099018374924, Test Acc: 0.660958904109589\n",
      "Epoch 1516, Train_Loss: 0.009190254465920589, Test Acc: 0.660958904109589\n",
      "Epoch 1517, Train_Loss: 0.008149386196237174, Test Acc: 0.6626712328767124\n",
      "Epoch 1518, Train_Loss: 0.01001414361701336, Test Acc: 0.6592465753424658\n",
      "Epoch 1519, Train_Loss: 0.009356956212513978, Test Acc: 0.660958904109589\n",
      "Epoch 1520, Train_Loss: 0.008324523209921608, Test Acc: 0.660958904109589\n",
      "Epoch 1521, Train_Loss: 0.009897242404804274, Test Acc: 0.660958904109589\n",
      "Epoch 1522, Train_Loss: 0.007933233702260623, Test Acc: 0.660958904109589\n",
      "Epoch 1523, Train_Loss: 0.008685587097716052, Test Acc: 0.6626712328767124\n",
      "Epoch 1524, Train_Loss: 0.011469322575749175, Test Acc: 0.6626712328767124\n",
      "Epoch 1525, Train_Loss: 0.009736205271792642, Test Acc: 0.660958904109589\n",
      "Epoch 1526, Train_Loss: 0.011944865383611614, Test Acc: 0.6626712328767124\n",
      "Epoch 1527, Train_Loss: 0.009173412825930427, Test Acc: 0.666095890410959\n",
      "Epoch 1528, Train_Loss: 0.008428067578734044, Test Acc: 0.6643835616438356\n",
      "Epoch 1529, Train_Loss: 0.009011959566578298, Test Acc: 0.660958904109589\n",
      "Epoch 1530, Train_Loss: 0.010815605765174041, Test Acc: 0.6643835616438356\n",
      "Epoch 1531, Train_Loss: 0.008756025074944773, Test Acc: 0.660958904109589\n",
      "Epoch 1532, Train_Loss: 0.0070076564372811845, Test Acc: 0.660958904109589\n",
      "Epoch 1533, Train_Loss: 0.009621799637443473, Test Acc: 0.660958904109589\n",
      "Epoch 1534, Train_Loss: 0.00978422476077867, Test Acc: 0.660958904109589\n",
      "Epoch 1535, Train_Loss: 0.007770721425004012, Test Acc: 0.660958904109589\n",
      "Epoch 1536, Train_Loss: 0.007491883837474234, Test Acc: 0.660958904109589\n",
      "Epoch 1537, Train_Loss: 0.009162334276879847, Test Acc: 0.6592465753424658\n",
      "Epoch 1538, Train_Loss: 0.008549162879489813, Test Acc: 0.6626712328767124\n",
      "Epoch 1539, Train_Loss: 0.009678477264742469, Test Acc: 0.660958904109589\n",
      "Epoch 1540, Train_Loss: 0.009371503083912103, Test Acc: 0.660958904109589\n",
      "Epoch 1541, Train_Loss: 0.009707693320251565, Test Acc: 0.6626712328767124\n",
      "Epoch 1542, Train_Loss: 0.007362885080283377, Test Acc: 0.660958904109589\n",
      "Epoch 1543, Train_Loss: 0.00979598323101527, Test Acc: 0.660958904109589\n",
      "Epoch 1544, Train_Loss: 0.012230730257442701, Test Acc: 0.660958904109589\n",
      "Epoch 1545, Train_Loss: 0.008316201148545588, Test Acc: 0.660958904109589\n",
      "Epoch 1546, Train_Loss: 0.009221419119285201, Test Acc: 0.660958904109589\n",
      "Epoch 1547, Train_Loss: 0.009088439870538423, Test Acc: 0.666095890410959\n",
      "Epoch 1548, Train_Loss: 0.00819984194799872, Test Acc: 0.660958904109589\n",
      "Epoch 1549, Train_Loss: 0.007749980167091053, Test Acc: 0.6626712328767124\n",
      "Epoch 1550, Train_Loss: 0.008119987872760248, Test Acc: 0.6626712328767124\n",
      "Epoch 1551, Train_Loss: 0.007098249309365201, Test Acc: 0.660958904109589\n",
      "Epoch 1552, Train_Loss: 0.008328422342401609, Test Acc: 0.660958904109589\n",
      "Epoch 1553, Train_Loss: 0.011470338158574123, Test Acc: 0.6541095890410958\n",
      "Epoch 1554, Train_Loss: 0.008982776066659426, Test Acc: 0.660958904109589\n",
      "Epoch 1555, Train_Loss: 0.007150095435918047, Test Acc: 0.6592465753424658\n",
      "Epoch 1556, Train_Loss: 0.008585314053561888, Test Acc: 0.660958904109589\n",
      "Epoch 1557, Train_Loss: 0.0078011274315485934, Test Acc: 0.6592465753424658\n",
      "Epoch 1558, Train_Loss: 0.009637978846512851, Test Acc: 0.6626712328767124\n",
      "Epoch 1559, Train_Loss: 0.006997048622224611, Test Acc: 0.660958904109589\n",
      "Epoch 1560, Train_Loss: 0.007549079738964792, Test Acc: 0.6643835616438356\n",
      "Epoch 1561, Train_Loss: 0.007535274182600915, Test Acc: 0.6592465753424658\n",
      "Epoch 1562, Train_Loss: 0.011410982346660603, Test Acc: 0.6592465753424658\n",
      "Epoch 1563, Train_Loss: 0.009311165023518697, Test Acc: 0.660958904109589\n",
      "Epoch 1564, Train_Loss: 0.010527563507139348, Test Acc: 0.6575342465753424\n",
      "Epoch 1565, Train_Loss: 0.009967224506681305, Test Acc: 0.6626712328767124\n",
      "Epoch 1566, Train_Loss: 0.007540279319982801, Test Acc: 0.660958904109589\n",
      "Epoch 1567, Train_Loss: 0.011269193189718862, Test Acc: 0.660958904109589\n",
      "Epoch 1568, Train_Loss: 0.006754349890798039, Test Acc: 0.660958904109589\n",
      "Epoch 1569, Train_Loss: 0.007294249309325096, Test Acc: 0.660958904109589\n",
      "Epoch 1570, Train_Loss: 0.009563094210534473, Test Acc: 0.660958904109589\n",
      "Epoch 1571, Train_Loss: 0.009502086203156068, Test Acc: 0.666095890410959\n",
      "Epoch 1572, Train_Loss: 0.009100363253764954, Test Acc: 0.660958904109589\n",
      "Epoch 1573, Train_Loss: 0.007283263873659962, Test Acc: 0.660958904109589\n",
      "Epoch 1574, Train_Loss: 0.00813821888232269, Test Acc: 0.6626712328767124\n",
      "Epoch 1575, Train_Loss: 0.007683477786486037, Test Acc: 0.660958904109589\n",
      "Epoch 1576, Train_Loss: 0.007833110320007108, Test Acc: 0.660958904109589\n",
      "Epoch 1577, Train_Loss: 0.012235455959626051, Test Acc: 0.660958904109589\n",
      "Epoch 1578, Train_Loss: 0.007116594314084068, Test Acc: 0.660958904109589\n",
      "Epoch 1579, Train_Loss: 0.00899252801559669, Test Acc: 0.660958904109589\n",
      "Epoch 1580, Train_Loss: 0.008425766655363987, Test Acc: 0.660958904109589\n",
      "Epoch 1581, Train_Loss: 0.008919656662328634, Test Acc: 0.660958904109589\n",
      "Epoch 1582, Train_Loss: 0.009790724834374487, Test Acc: 0.660958904109589\n",
      "Epoch 1583, Train_Loss: 0.010493849526028498, Test Acc: 0.660958904109589\n",
      "Epoch 1584, Train_Loss: 0.008546927288534789, Test Acc: 0.660958904109589\n",
      "Epoch 1585, Train_Loss: 0.008390779485125677, Test Acc: 0.6643835616438356\n",
      "Epoch 1586, Train_Loss: 0.00778959901572307, Test Acc: 0.660958904109589\n",
      "Epoch 1587, Train_Loss: 0.009897527417479068, Test Acc: 0.6643835616438356\n",
      "Epoch 1588, Train_Loss: 0.008666831609616565, Test Acc: 0.660958904109589\n",
      "Epoch 1589, Train_Loss: 0.008468171903246002, Test Acc: 0.660958904109589\n",
      "Epoch 1590, Train_Loss: 0.0070672317756361736, Test Acc: 0.660958904109589\n",
      "Epoch 1591, Train_Loss: 0.00874617945942191, Test Acc: 0.660958904109589\n",
      "Epoch 1592, Train_Loss: 0.0093216654599928, Test Acc: 0.660958904109589\n",
      "Epoch 1593, Train_Loss: 0.011008355348849364, Test Acc: 0.660958904109589\n",
      "Epoch 1594, Train_Loss: 0.01063393193351203, Test Acc: 0.660958904109589\n",
      "Epoch 1595, Train_Loss: 0.009852910510289803, Test Acc: 0.660958904109589\n",
      "Epoch 1596, Train_Loss: 0.008945106085320731, Test Acc: 0.660958904109589\n",
      "Epoch 1597, Train_Loss: 0.009251745909068632, Test Acc: 0.6626712328767124\n",
      "Epoch 1598, Train_Loss: 0.00900995416236583, Test Acc: 0.6643835616438356\n",
      "Epoch 1599, Train_Loss: 0.0078102853947257245, Test Acc: 0.6626712328767124\n",
      "Epoch 1600, Train_Loss: 0.007506070071258364, Test Acc: 0.660958904109589\n",
      "Epoch 1601, Train_Loss: 0.01173986033768415, Test Acc: 0.660958904109589\n",
      "Epoch 1602, Train_Loss: 0.009480433622911733, Test Acc: 0.6626712328767124\n",
      "Epoch 1603, Train_Loss: 0.014235175796898147, Test Acc: 0.6523972602739726\n",
      "Epoch 1604, Train_Loss: 0.008080861612825174, Test Acc: 0.660958904109589\n",
      "Epoch 1605, Train_Loss: 0.009818167889079632, Test Acc: 0.6592465753424658\n",
      "Epoch 1606, Train_Loss: 0.010963520770019386, Test Acc: 0.6575342465753424\n",
      "Epoch 1607, Train_Loss: 0.007515011174746178, Test Acc: 0.6592465753424658\n",
      "Epoch 1608, Train_Loss: 0.011373617340723285, Test Acc: 0.6626712328767124\n",
      "Epoch 1609, Train_Loss: 0.007350622123340145, Test Acc: 0.660958904109589\n",
      "Epoch 1610, Train_Loss: 0.010644704442029251, Test Acc: 0.6523972602739726\n",
      "Epoch 1611, Train_Loss: 0.008031982331658583, Test Acc: 0.660958904109589\n",
      "Epoch 1612, Train_Loss: 0.0072567315047535885, Test Acc: 0.6592465753424658\n",
      "Epoch 1613, Train_Loss: 0.008864621904649539, Test Acc: 0.6592465753424658\n",
      "Epoch 1614, Train_Loss: 0.009175874961783848, Test Acc: 0.6592465753424658\n",
      "Epoch 1615, Train_Loss: 0.008037516879767281, Test Acc: 0.6592465753424658\n",
      "Epoch 1616, Train_Loss: 0.008190548103470974, Test Acc: 0.6643835616438356\n",
      "Epoch 1617, Train_Loss: 0.011382399604826787, Test Acc: 0.6592465753424658\n",
      "Epoch 1618, Train_Loss: 0.011575062810152303, Test Acc: 0.6592465753424658\n",
      "Epoch 1619, Train_Loss: 0.008735281466442757, Test Acc: 0.6643835616438356\n",
      "Epoch 1620, Train_Loss: 0.0074790819899135386, Test Acc: 0.660958904109589\n",
      "Epoch 1621, Train_Loss: 0.0073774116729055095, Test Acc: 0.660958904109589\n",
      "Epoch 1622, Train_Loss: 0.010305932510391358, Test Acc: 0.666095890410959\n",
      "Epoch 1623, Train_Loss: 0.008669470847053162, Test Acc: 0.660958904109589\n",
      "Epoch 1624, Train_Loss: 0.007816393004532074, Test Acc: 0.6643835616438356\n",
      "Epoch 1625, Train_Loss: 0.00842253702603557, Test Acc: 0.660958904109589\n",
      "Epoch 1626, Train_Loss: 0.007363992682712706, Test Acc: 0.660958904109589\n",
      "Epoch 1627, Train_Loss: 0.00961521715771596, Test Acc: 0.660958904109589\n",
      "Epoch 1628, Train_Loss: 0.009625013623463019, Test Acc: 0.6643835616438356\n",
      "Epoch 1629, Train_Loss: 0.007253792885421717, Test Acc: 0.660958904109589\n",
      "Epoch 1630, Train_Loss: 0.008719279986053152, Test Acc: 0.6592465753424658\n",
      "Epoch 1631, Train_Loss: 0.009868899081084237, Test Acc: 0.6643835616438356\n",
      "Epoch 1632, Train_Loss: 0.009958485066590583, Test Acc: 0.660958904109589\n",
      "Epoch 1633, Train_Loss: 0.00617325585949402, Test Acc: 0.6592465753424658\n",
      "Epoch 1634, Train_Loss: 0.005965734567780601, Test Acc: 0.660958904109589\n",
      "Epoch 1635, Train_Loss: 0.00996349976617239, Test Acc: 0.660958904109589\n",
      "Epoch 1636, Train_Loss: 0.009809148828935577, Test Acc: 0.6592465753424658\n",
      "Epoch 1637, Train_Loss: 0.009721709685209134, Test Acc: 0.6626712328767124\n",
      "Epoch 1638, Train_Loss: 0.008858212836457824, Test Acc: 0.6695205479452054\n",
      "Epoch 1639, Train_Loss: 0.008138189984038036, Test Acc: 0.6643835616438356\n",
      "Epoch 1640, Train_Loss: 0.008293307752410328, Test Acc: 0.660958904109589\n",
      "Epoch 1641, Train_Loss: 0.010066804567713916, Test Acc: 0.660958904109589\n",
      "Epoch 1642, Train_Loss: 0.009430532165424665, Test Acc: 0.6592465753424658\n",
      "Epoch 1643, Train_Loss: 0.011975602629718196, Test Acc: 0.660958904109589\n",
      "Epoch 1644, Train_Loss: 0.008816315944386588, Test Acc: 0.666095890410959\n",
      "Epoch 1645, Train_Loss: 0.009635655246938768, Test Acc: 0.6592465753424658\n",
      "Epoch 1646, Train_Loss: 0.011833010105760877, Test Acc: 0.6592465753424658\n",
      "Epoch 1647, Train_Loss: 0.00831679078032721, Test Acc: 0.660958904109589\n",
      "Epoch 1648, Train_Loss: 0.009854398503648554, Test Acc: 0.660958904109589\n",
      "Epoch 1649, Train_Loss: 0.009460623704853788, Test Acc: 0.6592465753424658\n",
      "Epoch 1650, Train_Loss: 0.007891639749914248, Test Acc: 0.6592465753424658\n",
      "Epoch 1651, Train_Loss: 0.007810456547076683, Test Acc: 0.6592465753424658\n",
      "Epoch 1652, Train_Loss: 0.008291446436032857, Test Acc: 0.6592465753424658\n",
      "Epoch 1653, Train_Loss: 0.00824666984135547, Test Acc: 0.6592465753424658\n",
      "Epoch 1654, Train_Loss: 0.010456819817363794, Test Acc: 0.660958904109589\n",
      "Epoch 1655, Train_Loss: 0.009383229012655647, Test Acc: 0.6592465753424658\n",
      "Epoch 1656, Train_Loss: 0.009586340061332521, Test Acc: 0.6592465753424658\n",
      "Epoch 1657, Train_Loss: 0.007828973130699524, Test Acc: 0.6592465753424658\n",
      "Epoch 1658, Train_Loss: 0.011309746588494818, Test Acc: 0.6592465753424658\n",
      "Epoch 1659, Train_Loss: 0.00849838952308346, Test Acc: 0.6592465753424658\n",
      "Epoch 1660, Train_Loss: 0.011011196576646398, Test Acc: 0.6592465753424658\n",
      "Epoch 1661, Train_Loss: 0.007053326454524722, Test Acc: 0.6592465753424658\n",
      "Epoch 1662, Train_Loss: 0.007413062978912421, Test Acc: 0.6592465753424658\n",
      "Epoch 1663, Train_Loss: 0.010384294562072682, Test Acc: 0.6592465753424658\n",
      "Epoch 1664, Train_Loss: 0.0074801663572543475, Test Acc: 0.6592465753424658\n",
      "Epoch 1665, Train_Loss: 0.006653851949977252, Test Acc: 0.6626712328767124\n",
      "Epoch 1666, Train_Loss: 0.015330746726704092, Test Acc: 0.6643835616438356\n",
      "Epoch 1667, Train_Loss: 0.0077666148358730425, Test Acc: 0.6592465753424658\n",
      "Epoch 1668, Train_Loss: 0.01127314391510481, Test Acc: 0.660958904109589\n",
      "Epoch 1669, Train_Loss: 0.012284111502594897, Test Acc: 0.6592465753424658\n",
      "Epoch 1670, Train_Loss: 0.008495793083056924, Test Acc: 0.6643835616438356\n",
      "Epoch 1671, Train_Loss: 0.00792471666181882, Test Acc: 0.660958904109589\n",
      "Epoch 1672, Train_Loss: 0.006707200638174982, Test Acc: 0.660958904109589\n",
      "Epoch 1673, Train_Loss: 0.006403905984825542, Test Acc: 0.6592465753424658\n",
      "Epoch 1674, Train_Loss: 0.00940422449957623, Test Acc: 0.666095890410959\n",
      "Epoch 1675, Train_Loss: 0.009382720804751443, Test Acc: 0.660958904109589\n",
      "Epoch 1676, Train_Loss: 0.011770581320547535, Test Acc: 0.6626712328767124\n",
      "Epoch 1677, Train_Loss: 0.010125641040758637, Test Acc: 0.6592465753424658\n",
      "Epoch 1678, Train_Loss: 0.006687360769319639, Test Acc: 0.6592465753424658\n",
      "Epoch 1679, Train_Loss: 0.008574525187896143, Test Acc: 0.6592465753424658\n",
      "Epoch 1680, Train_Loss: 0.006659873466560384, Test Acc: 0.6592465753424658\n",
      "Epoch 1681, Train_Loss: 0.010056469969185855, Test Acc: 0.6626712328767124\n",
      "Epoch 1682, Train_Loss: 0.006753868438181598, Test Acc: 0.660958904109589\n",
      "Epoch 1683, Train_Loss: 0.009428598039448843, Test Acc: 0.6592465753424658\n",
      "Epoch 1684, Train_Loss: 0.008206653617094162, Test Acc: 0.6643835616438356\n",
      "Epoch 1685, Train_Loss: 0.010783050835016184, Test Acc: 0.660958904109589\n",
      "Epoch 1686, Train_Loss: 0.007529917480496806, Test Acc: 0.6592465753424658\n",
      "Epoch 1687, Train_Loss: 0.007379634785138478, Test Acc: 0.660958904109589\n",
      "Epoch 1688, Train_Loss: 0.01068438247443737, Test Acc: 0.6592465753424658\n",
      "Epoch 1689, Train_Loss: 0.009168278156096221, Test Acc: 0.6592465753424658\n",
      "Epoch 1690, Train_Loss: 0.009472727417687565, Test Acc: 0.660958904109589\n",
      "Epoch 1691, Train_Loss: 0.008052465188484348, Test Acc: 0.660958904109589\n",
      "Epoch 1692, Train_Loss: 0.01022353329597081, Test Acc: 0.6592465753424658\n",
      "Epoch 1693, Train_Loss: 0.009698840062810632, Test Acc: 0.6592465753424658\n",
      "Epoch 1694, Train_Loss: 0.009011984375320026, Test Acc: 0.6558219178082192\n",
      "Epoch 1695, Train_Loss: 0.006928012116190985, Test Acc: 0.6592465753424658\n",
      "Epoch 1696, Train_Loss: 0.008260105362410286, Test Acc: 0.6592465753424658\n",
      "Epoch 1697, Train_Loss: 0.00944987647767448, Test Acc: 0.6592465753424658\n",
      "Epoch 1698, Train_Loss: 0.00859110850478828, Test Acc: 0.6592465753424658\n",
      "Epoch 1699, Train_Loss: 0.01092188570601138, Test Acc: 0.6592465753424658\n",
      "Epoch 1700, Train_Loss: 0.008715711782087965, Test Acc: 0.6592465753424658\n",
      "Epoch 1701, Train_Loss: 0.0073818016062432434, Test Acc: 0.6592465753424658\n",
      "Epoch 1702, Train_Loss: 0.007530063892772887, Test Acc: 0.6592465753424658\n",
      "Epoch 1703, Train_Loss: 0.005528164363340693, Test Acc: 0.6592465753424658\n",
      "Epoch 1704, Train_Loss: 0.0073071243930371566, Test Acc: 0.6592465753424658\n",
      "Epoch 1705, Train_Loss: 0.008183611225149434, Test Acc: 0.6592465753424658\n",
      "Epoch 1706, Train_Loss: 0.0070016034005675465, Test Acc: 0.6592465753424658\n",
      "Epoch 1707, Train_Loss: 0.008733438549143102, Test Acc: 0.6575342465753424\n",
      "Epoch 1708, Train_Loss: 0.009102256425421729, Test Acc: 0.6592465753424658\n",
      "Epoch 1709, Train_Loss: 0.0061619610154366455, Test Acc: 0.6592465753424658\n",
      "Epoch 1710, Train_Loss: 0.00902220267539633, Test Acc: 0.6592465753424658\n",
      "Epoch 1711, Train_Loss: 0.006733767731077478, Test Acc: 0.6575342465753424\n",
      "Epoch 1712, Train_Loss: 0.008223590056786634, Test Acc: 0.6592465753424658\n",
      "Epoch 1713, Train_Loss: 0.008537015229649114, Test Acc: 0.6643835616438356\n",
      "Epoch 1714, Train_Loss: 0.007534969778134837, Test Acc: 0.666095890410959\n",
      "Epoch 1715, Train_Loss: 0.00661684618489744, Test Acc: 0.6592465753424658\n",
      "Epoch 1716, Train_Loss: 0.009260595496471069, Test Acc: 0.6592465753424658\n",
      "Epoch 1717, Train_Loss: 0.009188161123176997, Test Acc: 0.6592465753424658\n",
      "Epoch 1718, Train_Loss: 0.011033890741600771, Test Acc: 0.6575342465753424\n",
      "Epoch 1719, Train_Loss: 0.0076896161792774365, Test Acc: 0.6626712328767124\n",
      "Epoch 1720, Train_Loss: 0.00797010120527375, Test Acc: 0.6643835616438356\n",
      "Epoch 1721, Train_Loss: 0.007408774312352762, Test Acc: 0.6592465753424658\n",
      "Epoch 1722, Train_Loss: 0.007196983227345299, Test Acc: 0.6592465753424658\n",
      "Epoch 1723, Train_Loss: 0.007908986724942224, Test Acc: 0.6592465753424658\n",
      "Epoch 1724, Train_Loss: 0.007963034560589222, Test Acc: 0.6643835616438356\n",
      "Epoch 1725, Train_Loss: 0.007060214274588361, Test Acc: 0.6592465753424658\n",
      "Epoch 1726, Train_Loss: 0.007920787953025865, Test Acc: 0.660958904109589\n",
      "Epoch 1727, Train_Loss: 0.006272616732985625, Test Acc: 0.660958904109589\n",
      "Epoch 1728, Train_Loss: 0.008430345784745441, Test Acc: 0.6592465753424658\n",
      "Epoch 1729, Train_Loss: 0.012410391227604123, Test Acc: 0.6558219178082192\n",
      "Epoch 1730, Train_Loss: 0.0069677571636930224, Test Acc: 0.660958904109589\n",
      "Epoch 1731, Train_Loss: 0.0077558850432524196, Test Acc: 0.6678082191780822\n",
      "Epoch 1732, Train_Loss: 0.006841173077418716, Test Acc: 0.6592465753424658\n",
      "Epoch 1733, Train_Loss: 0.006515872010822932, Test Acc: 0.6592465753424658\n",
      "Epoch 1734, Train_Loss: 0.00734331245962494, Test Acc: 0.6592465753424658\n",
      "Epoch 1735, Train_Loss: 0.007097232389014607, Test Acc: 0.666095890410959\n",
      "Epoch 1736, Train_Loss: 0.007906688099637904, Test Acc: 0.6592465753424658\n",
      "Epoch 1737, Train_Loss: 0.008211041870481495, Test Acc: 0.6592465753424658\n",
      "Epoch 1738, Train_Loss: 0.0086576438047814, Test Acc: 0.6592465753424658\n",
      "Epoch 1739, Train_Loss: 0.00871456631671208, Test Acc: 0.6592465753424658\n",
      "Epoch 1740, Train_Loss: 0.008416583189728044, Test Acc: 0.660958904109589\n",
      "Epoch 1741, Train_Loss: 0.010506642231121077, Test Acc: 0.6592465753424658\n",
      "Epoch 1742, Train_Loss: 0.007437522318241463, Test Acc: 0.6643835616438356\n",
      "Epoch 1743, Train_Loss: 0.01092125366358232, Test Acc: 0.6592465753424658\n",
      "Epoch 1744, Train_Loss: 0.009518475545064575, Test Acc: 0.6558219178082192\n",
      "Epoch 1745, Train_Loss: 0.0076970296311174025, Test Acc: 0.6592465753424658\n",
      "Epoch 1746, Train_Loss: 0.0077923437145273056, Test Acc: 0.6592465753424658\n",
      "Epoch 1747, Train_Loss: 0.007697740045159662, Test Acc: 0.6592465753424658\n",
      "Epoch 1748, Train_Loss: 0.008731458844522422, Test Acc: 0.6643835616438356\n",
      "Epoch 1749, Train_Loss: 0.010955210352449285, Test Acc: 0.6575342465753424\n",
      "Epoch 1750, Train_Loss: 0.007434288910644682, Test Acc: 0.6592465753424658\n",
      "Epoch 1751, Train_Loss: 0.006136019731457054, Test Acc: 0.660958904109589\n",
      "Epoch 1752, Train_Loss: 0.008660072670181762, Test Acc: 0.6592465753424658\n",
      "Epoch 1753, Train_Loss: 0.007976710368438944, Test Acc: 0.6592465753424658\n",
      "Epoch 1754, Train_Loss: 0.010259273778046918, Test Acc: 0.6592465753424658\n",
      "Epoch 1755, Train_Loss: 0.008088656133622862, Test Acc: 0.6592465753424658\n",
      "Epoch 1756, Train_Loss: 0.007330768021347467, Test Acc: 0.6592465753424658\n",
      "Epoch 1757, Train_Loss: 0.007455826651607822, Test Acc: 0.660958904109589\n",
      "Epoch 1758, Train_Loss: 0.010339657851773154, Test Acc: 0.6592465753424658\n",
      "Epoch 1759, Train_Loss: 0.008943342039401614, Test Acc: 0.6592465753424658\n",
      "Epoch 1760, Train_Loss: 0.009884454896109673, Test Acc: 0.6575342465753424\n",
      "Epoch 1761, Train_Loss: 0.007078454592374328, Test Acc: 0.660958904109589\n",
      "Epoch 1762, Train_Loss: 0.009826733397858334, Test Acc: 0.6626712328767124\n",
      "Epoch 1763, Train_Loss: 0.006148172973553301, Test Acc: 0.660958904109589\n",
      "Epoch 1764, Train_Loss: 0.012399371707260798, Test Acc: 0.6592465753424658\n",
      "Epoch 1765, Train_Loss: 0.008425780682500772, Test Acc: 0.6592465753424658\n",
      "Epoch 1766, Train_Loss: 0.0059289016251113935, Test Acc: 0.660958904109589\n",
      "Epoch 1767, Train_Loss: 0.007972297081323632, Test Acc: 0.660958904109589\n",
      "Epoch 1768, Train_Loss: 0.006338597206081431, Test Acc: 0.660958904109589\n",
      "Epoch 1769, Train_Loss: 0.006869958834954559, Test Acc: 0.6626712328767124\n",
      "Epoch 1770, Train_Loss: 0.007136949280265981, Test Acc: 0.6592465753424658\n",
      "Epoch 1771, Train_Loss: 0.011231238061782278, Test Acc: 0.6592465753424658\n",
      "Epoch 1772, Train_Loss: 0.008115780170101061, Test Acc: 0.6626712328767124\n",
      "Epoch 1773, Train_Loss: 0.006820456359946547, Test Acc: 0.6626712328767124\n",
      "Epoch 1774, Train_Loss: 0.006918717484950321, Test Acc: 0.6592465753424658\n",
      "Epoch 1775, Train_Loss: 0.008958890232634076, Test Acc: 0.6626712328767124\n",
      "Epoch 1776, Train_Loss: 0.006640500392450122, Test Acc: 0.660958904109589\n",
      "Epoch 1777, Train_Loss: 0.007974921035838634, Test Acc: 0.6626712328767124\n",
      "Epoch 1778, Train_Loss: 0.007962041665223296, Test Acc: 0.660958904109589\n",
      "Epoch 1779, Train_Loss: 0.008840064871264985, Test Acc: 0.660958904109589\n",
      "Epoch 1780, Train_Loss: 0.006802791215022808, Test Acc: 0.660958904109589\n",
      "Epoch 1781, Train_Loss: 0.007253680827489006, Test Acc: 0.660958904109589\n",
      "Epoch 1782, Train_Loss: 0.006302367870830494, Test Acc: 0.660958904109589\n",
      "Epoch 1783, Train_Loss: 0.008073787827242995, Test Acc: 0.660958904109589\n",
      "Epoch 1784, Train_Loss: 0.006888305663096617, Test Acc: 0.6643835616438356\n",
      "Epoch 1785, Train_Loss: 0.00680591017999177, Test Acc: 0.660958904109589\n",
      "Epoch 1786, Train_Loss: 0.006294412827628548, Test Acc: 0.6643835616438356\n",
      "Epoch 1787, Train_Loss: 0.007924118298660687, Test Acc: 0.660958904109589\n",
      "Epoch 1788, Train_Loss: 0.010986949501443632, Test Acc: 0.6558219178082192\n",
      "Epoch 1789, Train_Loss: 0.008975480680419423, Test Acc: 0.6575342465753424\n",
      "Epoch 1790, Train_Loss: 0.00994138210512574, Test Acc: 0.6575342465753424\n",
      "Epoch 1791, Train_Loss: 0.007676882483451664, Test Acc: 0.6575342465753424\n",
      "Epoch 1792, Train_Loss: 0.007157146790632396, Test Acc: 0.6575342465753424\n",
      "Epoch 1793, Train_Loss: 0.007526983905222551, Test Acc: 0.6575342465753424\n",
      "Epoch 1794, Train_Loss: 0.007540181140257118, Test Acc: 0.6575342465753424\n",
      "Epoch 1795, Train_Loss: 0.008952005587161693, Test Acc: 0.6643835616438356\n",
      "Epoch 1796, Train_Loss: 0.006521823603634402, Test Acc: 0.660958904109589\n",
      "Epoch 1797, Train_Loss: 0.00763539513491196, Test Acc: 0.6626712328767124\n",
      "Epoch 1798, Train_Loss: 0.009507628127039425, Test Acc: 0.6575342465753424\n",
      "Epoch 1799, Train_Loss: 0.007854687099097646, Test Acc: 0.660958904109589\n",
      "Epoch 1800, Train_Loss: 0.006181938016197819, Test Acc: 0.6626712328767124\n",
      "Epoch 1801, Train_Loss: 0.00784725095070371, Test Acc: 0.6626712328767124\n",
      "Epoch 1802, Train_Loss: 0.008077872833723632, Test Acc: 0.6592465753424658\n",
      "Epoch 1803, Train_Loss: 0.007183077171703189, Test Acc: 0.6575342465753424\n",
      "Epoch 1804, Train_Loss: 0.006696977329738729, Test Acc: 0.6626712328767124\n",
      "Epoch 1805, Train_Loss: 0.006738940943250782, Test Acc: 0.6643835616438356\n",
      "Epoch 1806, Train_Loss: 0.007585508248212136, Test Acc: 0.6575342465753424\n",
      "Epoch 1807, Train_Loss: 0.007575503515909077, Test Acc: 0.6592465753424658\n",
      "Epoch 1808, Train_Loss: 0.008404814869891197, Test Acc: 0.6626712328767124\n",
      "Epoch 1809, Train_Loss: 0.008548137040861548, Test Acc: 0.660958904109589\n",
      "Epoch 1810, Train_Loss: 0.007045690338372879, Test Acc: 0.6643835616438356\n",
      "Epoch 1811, Train_Loss: 0.006937472494428221, Test Acc: 0.660958904109589\n",
      "Epoch 1812, Train_Loss: 0.007645042357580678, Test Acc: 0.660958904109589\n",
      "Epoch 1813, Train_Loss: 0.0084326282285474, Test Acc: 0.660958904109589\n",
      "Epoch 1814, Train_Loss: 0.00820271020279506, Test Acc: 0.6592465753424658\n",
      "Epoch 1815, Train_Loss: 0.009838186450906505, Test Acc: 0.6575342465753424\n",
      "Epoch 1816, Train_Loss: 0.007050866719055193, Test Acc: 0.6575342465753424\n",
      "Epoch 1817, Train_Loss: 0.009557991401607069, Test Acc: 0.6592465753424658\n",
      "Epoch 1818, Train_Loss: 0.006766942708509305, Test Acc: 0.6592465753424658\n",
      "Epoch 1819, Train_Loss: 0.007204690131629832, Test Acc: 0.6575342465753424\n",
      "Epoch 1820, Train_Loss: 0.008290783593110973, Test Acc: 0.6575342465753424\n",
      "Epoch 1821, Train_Loss: 0.006483530551804506, Test Acc: 0.660958904109589\n",
      "Epoch 1822, Train_Loss: 0.008871621764683368, Test Acc: 0.666095890410959\n",
      "Epoch 1823, Train_Loss: 0.008305878725650473, Test Acc: 0.660958904109589\n",
      "Epoch 1824, Train_Loss: 0.00774548568051614, Test Acc: 0.660958904109589\n",
      "Epoch 1825, Train_Loss: 0.007014104021436651, Test Acc: 0.6575342465753424\n",
      "Epoch 1826, Train_Loss: 0.008688925579690476, Test Acc: 0.6575342465753424\n",
      "Epoch 1827, Train_Loss: 0.008401086887943165, Test Acc: 0.6592465753424658\n",
      "Epoch 1828, Train_Loss: 0.006666907779845133, Test Acc: 0.660958904109589\n",
      "Epoch 1829, Train_Loss: 0.009087208022492632, Test Acc: 0.6575342465753424\n",
      "Epoch 1830, Train_Loss: 0.00873259716809116, Test Acc: 0.6558219178082192\n",
      "Epoch 1831, Train_Loss: 0.006937626989724777, Test Acc: 0.6575342465753424\n",
      "Epoch 1832, Train_Loss: 0.008557601034681284, Test Acc: 0.6541095890410958\n",
      "Epoch 1833, Train_Loss: 0.006176436102350635, Test Acc: 0.6592465753424658\n",
      "Epoch 1834, Train_Loss: 0.008963758595768923, Test Acc: 0.6643835616438356\n",
      "Epoch 1835, Train_Loss: 0.007399772367307378, Test Acc: 0.6592465753424658\n",
      "Epoch 1836, Train_Loss: 0.00736721368389226, Test Acc: 0.6575342465753424\n",
      "Epoch 1837, Train_Loss: 0.010315088122524685, Test Acc: 0.6523972602739726\n",
      "Epoch 1838, Train_Loss: 0.010293462650906804, Test Acc: 0.6592465753424658\n",
      "Epoch 1839, Train_Loss: 0.008448894236508409, Test Acc: 0.6592465753424658\n",
      "Epoch 1840, Train_Loss: 0.0054619059737888165, Test Acc: 0.660958904109589\n",
      "Epoch 1841, Train_Loss: 0.0065964206767148426, Test Acc: 0.660958904109589\n",
      "Epoch 1842, Train_Loss: 0.007059306366272722, Test Acc: 0.660958904109589\n",
      "Epoch 1843, Train_Loss: 0.006534449828222932, Test Acc: 0.660958904109589\n",
      "Epoch 1844, Train_Loss: 0.00592105311557134, Test Acc: 0.6643835616438356\n",
      "Epoch 1845, Train_Loss: 0.008863548533099674, Test Acc: 0.6575342465753424\n",
      "Epoch 1846, Train_Loss: 0.00618193623586194, Test Acc: 0.6575342465753424\n",
      "Epoch 1847, Train_Loss: 0.006164059468119376, Test Acc: 0.6592465753424658\n",
      "Epoch 1848, Train_Loss: 0.007105215436922663, Test Acc: 0.6575342465753424\n",
      "Epoch 1849, Train_Loss: 0.005958088539955497, Test Acc: 0.6626712328767124\n",
      "Epoch 1850, Train_Loss: 0.0075190697360767444, Test Acc: 0.660958904109589\n",
      "Epoch 1851, Train_Loss: 0.006380500495311026, Test Acc: 0.6575342465753424\n",
      "Epoch 1852, Train_Loss: 0.007887835993869885, Test Acc: 0.6575342465753424\n",
      "Epoch 1853, Train_Loss: 0.006452498863836809, Test Acc: 0.6575342465753424\n",
      "Epoch 1854, Train_Loss: 0.007778526612355563, Test Acc: 0.660958904109589\n",
      "Epoch 1855, Train_Loss: 0.006272552850305146, Test Acc: 0.6626712328767124\n",
      "Epoch 1856, Train_Loss: 0.006889596964356315, Test Acc: 0.6643835616438356\n",
      "Epoch 1857, Train_Loss: 0.005629685735812018, Test Acc: 0.660958904109589\n",
      "Epoch 1858, Train_Loss: 0.007163101920582449, Test Acc: 0.6592465753424658\n",
      "Epoch 1859, Train_Loss: 0.007167361952497231, Test Acc: 0.666095890410959\n",
      "Epoch 1860, Train_Loss: 0.006967673323060808, Test Acc: 0.6643835616438356\n",
      "Epoch 1861, Train_Loss: 0.008248555202044372, Test Acc: 0.6592465753424658\n",
      "Epoch 1862, Train_Loss: 0.005975905903369494, Test Acc: 0.6592465753424658\n",
      "Epoch 1863, Train_Loss: 0.006344541051930719, Test Acc: 0.666095890410959\n",
      "Epoch 1864, Train_Loss: 0.0055138599034307845, Test Acc: 0.6643835616438356\n",
      "Epoch 1865, Train_Loss: 0.006509362507813421, Test Acc: 0.6626712328767124\n",
      "Epoch 1866, Train_Loss: 0.007021531612053877, Test Acc: 0.6575342465753424\n",
      "Epoch 1867, Train_Loss: 0.009234467892383691, Test Acc: 0.6575342465753424\n",
      "Epoch 1868, Train_Loss: 0.0066396849531429325, Test Acc: 0.6626712328767124\n",
      "Epoch 1869, Train_Loss: 0.009357471091107072, Test Acc: 0.6541095890410958\n",
      "Epoch 1870, Train_Loss: 0.007897370926229996, Test Acc: 0.6558219178082192\n",
      "Epoch 1871, Train_Loss: 0.006073992483266011, Test Acc: 0.6575342465753424\n",
      "Epoch 1872, Train_Loss: 0.007499337810372708, Test Acc: 0.660958904109589\n",
      "Epoch 1873, Train_Loss: 0.005492906725038438, Test Acc: 0.666095890410959\n",
      "Epoch 1874, Train_Loss: 0.006650643376701737, Test Acc: 0.6626712328767124\n",
      "Epoch 1875, Train_Loss: 0.006988660767319743, Test Acc: 0.6592465753424658\n",
      "Epoch 1876, Train_Loss: 0.006243552819114484, Test Acc: 0.6626712328767124\n",
      "Epoch 1877, Train_Loss: 0.008825326179021431, Test Acc: 0.660958904109589\n",
      "Epoch 1878, Train_Loss: 0.005707134049771412, Test Acc: 0.660958904109589\n",
      "Epoch 1879, Train_Loss: 0.0051497134414830725, Test Acc: 0.660958904109589\n",
      "Epoch 1880, Train_Loss: 0.006733874344263313, Test Acc: 0.660958904109589\n",
      "Epoch 1881, Train_Loss: 0.007081282666717925, Test Acc: 0.6592465753424658\n",
      "Epoch 1882, Train_Loss: 0.008151279875619366, Test Acc: 0.6592465753424658\n",
      "Epoch 1883, Train_Loss: 0.006696583157349778, Test Acc: 0.660958904109589\n",
      "Epoch 1884, Train_Loss: 0.008075981666365806, Test Acc: 0.6592465753424658\n",
      "Epoch 1885, Train_Loss: 0.006828590183886263, Test Acc: 0.660958904109589\n",
      "Epoch 1886, Train_Loss: 0.00650828048310359, Test Acc: 0.6592465753424658\n",
      "Epoch 1887, Train_Loss: 0.010395255320418073, Test Acc: 0.660958904109589\n",
      "Epoch 1888, Train_Loss: 0.008203971058378556, Test Acc: 0.6643835616438356\n",
      "Epoch 1889, Train_Loss: 0.007687959366649011, Test Acc: 0.666095890410959\n",
      "Epoch 1890, Train_Loss: 0.0075249629840072885, Test Acc: 0.6592465753424658\n",
      "Epoch 1891, Train_Loss: 0.008304727441441173, Test Acc: 0.660958904109589\n",
      "Epoch 1892, Train_Loss: 0.00787244889579597, Test Acc: 0.660958904109589\n",
      "Epoch 1893, Train_Loss: 0.006980048329808142, Test Acc: 0.660958904109589\n",
      "Epoch 1894, Train_Loss: 0.00589793554672724, Test Acc: 0.660958904109589\n",
      "Epoch 1895, Train_Loss: 0.006498049576975973, Test Acc: 0.6643835616438356\n",
      "Epoch 1896, Train_Loss: 0.006564061511198815, Test Acc: 0.660958904109589\n",
      "Epoch 1897, Train_Loss: 0.007528884553948956, Test Acc: 0.6592465753424658\n",
      "Epoch 1898, Train_Loss: 0.0076364946423836955, Test Acc: 0.6592465753424658\n",
      "Epoch 1899, Train_Loss: 0.006289564573080497, Test Acc: 0.660958904109589\n",
      "Epoch 1900, Train_Loss: 0.007472695138858398, Test Acc: 0.660958904109589\n",
      "Epoch 1901, Train_Loss: 0.008275235995142793, Test Acc: 0.6626712328767124\n",
      "Epoch 1902, Train_Loss: 0.0065865342768347546, Test Acc: 0.660958904109589\n",
      "Epoch 1903, Train_Loss: 0.007814180167315499, Test Acc: 0.666095890410959\n",
      "Epoch 1904, Train_Loss: 0.007217280227905576, Test Acc: 0.660958904109589\n",
      "Epoch 1905, Train_Loss: 0.00688132395998764, Test Acc: 0.660958904109589\n",
      "Epoch 1906, Train_Loss: 0.006714755829591468, Test Acc: 0.660958904109589\n",
      "Epoch 1907, Train_Loss: 0.00712325783729284, Test Acc: 0.6575342465753424\n",
      "Epoch 1908, Train_Loss: 0.005941726111132084, Test Acc: 0.660958904109589\n",
      "Epoch 1909, Train_Loss: 0.005480647388139914, Test Acc: 0.660958904109589\n",
      "Epoch 1910, Train_Loss: 0.006376971271492948, Test Acc: 0.660958904109589\n",
      "Epoch 1911, Train_Loss: 0.006808389516777424, Test Acc: 0.6592465753424658\n",
      "Epoch 1912, Train_Loss: 0.0078127636637646, Test Acc: 0.660958904109589\n",
      "Epoch 1913, Train_Loss: 0.007741544482996687, Test Acc: 0.660958904109589\n",
      "Epoch 1914, Train_Loss: 0.005801914083917836, Test Acc: 0.660958904109589\n",
      "Epoch 1915, Train_Loss: 0.0076382918236959085, Test Acc: 0.660958904109589\n",
      "Epoch 1916, Train_Loss: 0.00637753301111843, Test Acc: 0.660958904109589\n",
      "Epoch 1917, Train_Loss: 0.010857762499199453, Test Acc: 0.6592465753424658\n",
      "Epoch 1918, Train_Loss: 0.005715859932251988, Test Acc: 0.660958904109589\n",
      "Epoch 1919, Train_Loss: 0.00779977767751916, Test Acc: 0.666095890410959\n",
      "Epoch 1920, Train_Loss: 0.006771081769556986, Test Acc: 0.6575342465753424\n",
      "Epoch 1921, Train_Loss: 0.008349463544846003, Test Acc: 0.660958904109589\n",
      "Epoch 1922, Train_Loss: 0.008609685894953145, Test Acc: 0.6592465753424658\n",
      "Epoch 1923, Train_Loss: 0.007153495264446974, Test Acc: 0.6575342465753424\n",
      "Epoch 1924, Train_Loss: 0.007887155241860455, Test Acc: 0.6592465753424658\n",
      "Epoch 1925, Train_Loss: 0.00696352668273903, Test Acc: 0.6626712328767124\n",
      "Epoch 1926, Train_Loss: 0.0071758633539502625, Test Acc: 0.6643835616438356\n",
      "Epoch 1927, Train_Loss: 0.006415377844064096, Test Acc: 0.660958904109589\n",
      "Epoch 1928, Train_Loss: 0.00741260722224979, Test Acc: 0.6643835616438356\n",
      "Epoch 1929, Train_Loss: 0.008026851963904846, Test Acc: 0.6592465753424658\n",
      "Epoch 1930, Train_Loss: 0.007503432927933318, Test Acc: 0.6643835616438356\n",
      "Epoch 1931, Train_Loss: 0.008015529677777522, Test Acc: 0.6558219178082192\n",
      "Epoch 1932, Train_Loss: 0.006847705208656407, Test Acc: 0.6643835616438356\n",
      "Epoch 1933, Train_Loss: 0.006194244434027496, Test Acc: 0.6626712328767124\n",
      "Epoch 1934, Train_Loss: 0.005661346915530885, Test Acc: 0.660958904109589\n",
      "Epoch 1935, Train_Loss: 0.006221080694444936, Test Acc: 0.660958904109589\n",
      "Epoch 1936, Train_Loss: 0.006235436881752321, Test Acc: 0.660958904109589\n",
      "Epoch 1937, Train_Loss: 0.005885969016048875, Test Acc: 0.6643835616438356\n",
      "Epoch 1938, Train_Loss: 0.005551238890120658, Test Acc: 0.6643835616438356\n",
      "Epoch 1939, Train_Loss: 0.008955526904173894, Test Acc: 0.6678082191780822\n",
      "Epoch 1940, Train_Loss: 0.006560414687442062, Test Acc: 0.6643835616438356\n",
      "Epoch 1941, Train_Loss: 0.007274417694588919, Test Acc: 0.6643835616438356\n",
      "Epoch 1942, Train_Loss: 0.007940336075535015, Test Acc: 0.6626712328767124\n",
      "Epoch 1943, Train_Loss: 0.00840713977663654, Test Acc: 0.660958904109589\n",
      "Epoch 1944, Train_Loss: 0.007164542678765429, Test Acc: 0.660958904109589\n",
      "Epoch 1945, Train_Loss: 0.00802461824105194, Test Acc: 0.660958904109589\n",
      "Epoch 1946, Train_Loss: 0.008415291710662132, Test Acc: 0.660958904109589\n",
      "Epoch 1947, Train_Loss: 0.006243917918709485, Test Acc: 0.660958904109589\n",
      "Epoch 1948, Train_Loss: 0.006533378662879841, Test Acc: 0.660958904109589\n",
      "Epoch 1949, Train_Loss: 0.007225112636206177, Test Acc: 0.660958904109589\n",
      "Epoch 1950, Train_Loss: 0.007477866683245793, Test Acc: 0.6643835616438356\n",
      "Epoch 1951, Train_Loss: 0.005658497387457828, Test Acc: 0.6592465753424658\n",
      "Epoch 1952, Train_Loss: 0.012571957993714022, Test Acc: 0.6575342465753424\n",
      "Epoch 1953, Train_Loss: 0.00867948938275731, Test Acc: 0.660958904109589\n",
      "Epoch 1954, Train_Loss: 0.005415761417680187, Test Acc: 0.6643835616438356\n",
      "Epoch 1955, Train_Loss: 0.006926365284698477, Test Acc: 0.660958904109589\n",
      "Epoch 1956, Train_Loss: 0.007439417625050737, Test Acc: 0.660958904109589\n",
      "Epoch 1957, Train_Loss: 0.006357381446377985, Test Acc: 0.660958904109589\n",
      "Epoch 1958, Train_Loss: 0.007055017221318849, Test Acc: 0.6626712328767124\n",
      "Epoch 1959, Train_Loss: 0.006230108550539626, Test Acc: 0.6626712328767124\n",
      "Epoch 1960, Train_Loss: 0.007494788820963549, Test Acc: 0.6626712328767124\n",
      "Epoch 1961, Train_Loss: 0.005134542747782689, Test Acc: 0.6643835616438356\n",
      "Epoch 1962, Train_Loss: 0.007674206181036425, Test Acc: 0.660958904109589\n",
      "Epoch 1963, Train_Loss: 0.006883574218477406, Test Acc: 0.660958904109589\n",
      "Epoch 1964, Train_Loss: 0.006675491290252467, Test Acc: 0.6626712328767124\n",
      "Epoch 1965, Train_Loss: 0.00909946028923514, Test Acc: 0.6592465753424658\n",
      "Epoch 1966, Train_Loss: 0.0067470156666331604, Test Acc: 0.660958904109589\n",
      "Epoch 1967, Train_Loss: 0.005801310239689883, Test Acc: 0.6643835616438356\n",
      "Epoch 1968, Train_Loss: 0.006449969126606447, Test Acc: 0.660958904109589\n",
      "Epoch 1969, Train_Loss: 0.009867742700407689, Test Acc: 0.660958904109589\n",
      "Epoch 1970, Train_Loss: 0.006320329370055333, Test Acc: 0.6592465753424658\n",
      "Epoch 1971, Train_Loss: 0.006910931309221269, Test Acc: 0.660958904109589\n",
      "Epoch 1972, Train_Loss: 0.006606660958709654, Test Acc: 0.660958904109589\n",
      "Epoch 1973, Train_Loss: 0.007476977345845626, Test Acc: 0.660958904109589\n",
      "Epoch 1974, Train_Loss: 0.005883268594061519, Test Acc: 0.6643835616438356\n",
      "Epoch 1975, Train_Loss: 0.005390982324570359, Test Acc: 0.6592465753424658\n",
      "Epoch 1976, Train_Loss: 0.007032024860336605, Test Acc: 0.6592465753424658\n",
      "Epoch 1977, Train_Loss: 0.006167402476421557, Test Acc: 0.666095890410959\n",
      "Epoch 1978, Train_Loss: 0.006787701788994127, Test Acc: 0.660958904109589\n",
      "Epoch 1979, Train_Loss: 0.0074632833296846, Test Acc: 0.660958904109589\n",
      "Epoch 1980, Train_Loss: 0.0056402392422114644, Test Acc: 0.660958904109589\n",
      "Epoch 1981, Train_Loss: 0.0062007739272758045, Test Acc: 0.660958904109589\n",
      "Epoch 1982, Train_Loss: 0.00689733807189441, Test Acc: 0.660958904109589\n",
      "Epoch 1983, Train_Loss: 0.006246166474738857, Test Acc: 0.660958904109589\n",
      "Epoch 1984, Train_Loss: 0.007202600080177035, Test Acc: 0.6592465753424658\n",
      "Epoch 1985, Train_Loss: 0.0059313407771242055, Test Acc: 0.660958904109589\n",
      "Epoch 1986, Train_Loss: 0.007403955069207768, Test Acc: 0.6592465753424658\n",
      "Epoch 1987, Train_Loss: 0.006333728213121503, Test Acc: 0.6592465753424658\n",
      "Epoch 1988, Train_Loss: 0.007746742101062409, Test Acc: 0.660958904109589\n",
      "Epoch 1989, Train_Loss: 0.005370823362682131, Test Acc: 0.660958904109589\n",
      "Epoch 1990, Train_Loss: 0.006358346495744627, Test Acc: 0.660958904109589\n",
      "Epoch 1991, Train_Loss: 0.006639627701133577, Test Acc: 0.6592465753424658\n",
      "Epoch 1992, Train_Loss: 0.005758700868000233, Test Acc: 0.660958904109589\n",
      "Epoch 1993, Train_Loss: 0.006400169018434099, Test Acc: 0.6592465753424658\n",
      "Epoch 1994, Train_Loss: 0.006624670416158551, Test Acc: 0.660958904109589\n",
      "Epoch 1995, Train_Loss: 0.009083124647077057, Test Acc: 0.6592465753424658\n",
      "Epoch 1996, Train_Loss: 0.005478363204929337, Test Acc: 0.6592465753424658\n",
      "Epoch 1997, Train_Loss: 0.005654362703580773, Test Acc: 0.660958904109589\n",
      "Epoch 1998, Train_Loss: 0.005998012857617141, Test Acc: 0.6592465753424658\n",
      "Epoch 1999, Train_Loss: 0.006554335742293915, Test Acc: 0.6592465753424658\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000 \n",
    "for epoch in range(epochs):\n",
    "  loss = train()\n",
    "  test_acc = test(test_loader)\n",
    "  print(f\"Epoch {epoch}, Train_Loss: {loss}, Test Acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX_FGbp4tV0Z"
   },
   "source": [
    "# Ex13 (optional)\n",
    "\n",
    "\n",
    "\n",
    "1. If we use torch.nn.BCEWithLogitsLoss(), what does we need to change to the definition of the model ?\n",
    "\n",
    "\n",
    "\n",
    "2. The same question for torch.nn.CrossEntropyLoss() loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l-GFUSOmOIr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
